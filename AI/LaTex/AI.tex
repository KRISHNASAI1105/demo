\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{mathtools}  
\usepackage{diffcoeff}
\usepackage{pgfplots}
\usepackage{comment}
\usepackage{hyperref}
% \usepackage{babel}
 \usepackage[useregional]{datetime2}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage{subcaption}
\usepackage[export]{adjustbox}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage{blindtext}
\usepackage[most]{tcolorbox}
\usepackage{lipsum}
\usepackage{array}
\usepackage{booktabs}
\usepackage[svgnames, table]{xcolor}
\usepackage{tabularx, makecell, linegoal}
\usepackage{diagbox}
\usepackage{slashbox}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}

\newlength{\Lnote}
\newcommand{\notte}[1]
     {\addtolength{\leftmargini}{4em}
        \settowidth{\Lnote}{\textbf{Note:~}}
        \begin{quote}
            \rule{\dimexpr\textwidth-2\leftmargini}{1pt}\\
                        \mbox{}\hspace{-\Lnote}\textbf{Note:~}%
                                            #1\\[-0.5ex] 
            \rule{\dimexpr\textwidth-2\leftmargini}{1pt}
        \end{quote}
        \addtolength{\leftmargini}{-4em}}

\newcommand{\mynote}[1]{\medskip\par\textbf{\small Note}\quad\setlength{\extrarowheight}{2pt}\begin{tabularx}{\linegoal}{X}
\Xhline{1pt}
\Xhline{1pt}
\end{tabularx}}

\title{\textbf{Machine Learning}}
\author{\textbf{Nagubandi Krishna Sai} \\ \textbf{MS20BTECH11014}}

\graphicspath{{images/}}

\begin{document}

\maketitle

% \section{Contents}

\tableofcontents{}

\chapter{Fundamentals of Machine Learning}
\section{Supervised Learning}
Supervised Learning gives "correct answers", the output values are same as real life values. \\
In Supervised Learning, we are given a set of data and we know what our correct output should look like, having an idea that there is relationship between the input and the output. \\
Supervised Learning problems has two types of problems, 
\begin{enumerate}
    \item Regression. 
    \item Classification. 
\end{enumerate} \\

\subsection{Linear Regression}
In regression type of problems, we are trying to predict results within a continuous output, means that we are trying to map input variables to some continuous function. \\
Linear regression has real-valued output, but the output will be same or near valued to the actual output. 
\begin{enumerate}
    \item m = Number of training examples. 
    \item x's = "input" variable (or) feature.
    \item y's = "output (or) target" variable.
    \item (x,y) = one training example.
    \item ($x^{(i)}$,$y^{(i)}$) = $i^{th}$ training example.
\end{enumerate} 
Example : \newline
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} \hline
        Size of feet^2 (x) & Price(\$) in 1000's (y)  \\ \hline
        2104 & 460  \\ \hline
        1416 & 232  \\ \hline
        1534 & 315  \\ \hline
        852 & 178  \\ \hline
        \vdots & \vdots  \\ \hline
    \end{tabular}
    \caption{Training set of housing prices.}
    \label{tab:my_label}
\end{table}
\begin{enumerate}
    \item m = 47.
    \item $x^{(1)}$ = 2104 and $y^{(1)}$ = 460. 
    \item $x^{(2)}$ = 1416 and $y^{(2)}$ = 232.
\end{enumerate}
\begin{center}
\begin{tikzpicture}
\node[draw] at (2.5,2.5) {Training set};
\node[draw] at (2.5,0.5) {Learning algorithm};
\node[draw] at (2.5,-2.5) {h};
\node (A) at (2.5,2.3) {};
\node (B) at (2.5,0.7) {};
\draw[->, to path={-\ (\tikztotarget)}]
  (A) edge (B) ;
\node (A) at (2.5,0.3) {};
\node (B) at (2.5,-2.3) {};
\draw[->, to path={-\ (\tikztotarget)}]
  (A) edge (B) ;
\node[draw] at (6.5,1.5) {Training set is feeded to learning algorithm};
\node[draw] at (9,-2.5) {(hypothesis, This is a function that takes input x and estimate value of y.)}
\end{tikzpicture}
\end{center} \\
\subsection{Hypothesis for Linear Regression}
\begin{equation}
    h_\theta(x) = \theta_0 + \theta_1.x 
\end{equation} 
\begin{center}
    $\theta_i$'s = parameters.
\end{center} \\
This type of hypothesis mode is called "Linear regression with one variable" (or) "Univariate linear regression."  \\
\subsection{Cost function for Linear Regression}
Cost function helps us know that how well to fit the best possible straight line over the given data. \\ \newline
Q\rangle\; How\ to\ choose\ $\theta_i$'s\ ? 
\begin{enumerate}
    \item Choose\ $\theta_i$ 's so that $h_\theta(x)$ is close to y for our training example (x,y). 
    \item minimise $\theta_0$,$\theta_1$ so, that [$h_\theta(x)$ - y] is small.
\end{enumerate}
\begin{equation}
    J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^{m}[h_\theta(x^{(i)}) - y^{(i)}]^2
\end{equation}
\begin{center}
    J($\theta_0$,$\theta_1$) = Cost function (or) Squared error function.
\end{center} \\
\subsection{Gradient descent for Linear Regression}
Gradient descent is used to minimise cost function(J) in linear regression. \\
Gradient descent is used in many areas to minimise many functions in ML/AI. \\
\textbf{Gradient descent algorithm,} \\
\begin{equation}
    \text{Repeat\ until\ convergence\ (minimum)} \Bigg\{ \theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1),\ for\ j=0, j=1.
\end{equation}
\begin{enumerate}
    \item := is Assignment operator.
    \item $\alpha$ is learning rate.
    \item $\frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1)$ is derivative. 
\end{enumerate} \\
Gradient descent is nothing but the derivative of the Cost function. \\
\begin{equation}
    Slope\ of\ cost\ function\ curve = \frac{\partial J(\theta_1)}{\partial \theta_1},\ when\ \theta_0 = 0.
\end{equation} \\
\textbf{Learning rate,} \\
\begin{enumerate}
    \item If $\alpha$ is too small, gradient descent can be slow. After many such operations(can be infinite times), the '$\theta_1$' could reach "global minimum".
    \item If $\alpha$ is too large, gradient descent can overshoot the minimum. It may "fail to converge (or) even diverge". 
    \item If $\theta_1$ is at the local optima itself when we started or taken $\theta_1$, then there is no use of "$\alpha$ (or) gradient descent". 
\end{enumerate}
\subsection{Linear Regression for multivariables}
\textbf{Hypothesis,} \\
\begin{equation}
    h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n.
\end{equation} \\
In the total context of Supervised learning, hypothesis is just predicting the output. \\
For convenience of notation, declare x_0 = 1\ (x^{(i)}_0 = 1). \\
\begin{math} 
x = \left[\begin{array}{c}
     x_0 \\
     x_1 \\
     x_2 \\
     \vdots \\
     x_n
\end{array}\right] \epsilon\ \Re^{n+1}  \;\;\;\;\;\;\;\;\;  
\theta = \left[\begin{array}{c}
     \theta_0 \\
     \theta_1 \\
     \theta_2 \\
     \vdots \\
     \theta_n
\end{array}\right] \epsilon\ \Re^{n+1}
\end{math} \\
The above matrix is 0 - indexed. \\
\begin{equation}
    h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n. \\
    h_\theta(x) = \theta^{\top} x.
\end{equation} \\
\textbf{Cost function,} \\
\begin{equation}
    J(\theta) = J(\theta_0,\theta_1,\theta_2,...,\theta_n) = \frac{1}{2m}\sum_{i=1}^{m}[h_\theta(x^{(i)}) - y^{(i)}]^2
\end{equation} \\
\textbf{Gradient descent,} 
\begin{center}
\begin{align}
    \theta_j &:= \theta_j - \alpha\frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1,\theta_2,...,\theta_n)\\
    \theta_j &:= \theta_j - \alpha\frac{\partial}{\partial \theta_j} J(\theta) \\
    \theta_j &:= \theta_j - \alpha\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^{(i)}) - y^{(i)}]x^{(i)}_j
\end{align} 
\end{center} \\
\textbf{Feature scaling,} \\
Get every feature into approximately -1\le x_i \le 1 \ range. \\
\textbf{Mean normalization,} \\
Replace x_i \ with\ x_i - \mu_i\ to\ make\ features\ have\ approximately\ zero\ mean. \\
\subsection{Polynomial regression}
\textbf{Hypothesis,} \\
\begin{equation}
    h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2 x_2 + \theta_3 x_3.
\end{equation}\\
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|} \hline
         Housing price prediction  \\ \hline 
         x_1 = size \\ \hline
         x_2 = (size)^2 \\ \hline
         x_3 = (size)^3 \\  \hline
    \end{tabular}
    \caption{Features be-like in Polynomial regression.}
    \label{tab:my_label}
\end{table}
\subsection{Normal Equation}
\textbf{Intuition,} \\
\begin{center}
    \begin{math}
    \diff{}{\theta}J(\theta) = 0. 
    \end{math}
\end{center} \\
\textbf{Cost function,} 
\begin{center} \vspace{-9mm}
    \begin{align} 
    \theta\ \epsilon\ \Re^{n+1},\ J(\theta_0,\theta_1,\theta_2,...,\theta_n) = \frac{1}{2m} \sum_{i=1}^{m} [h_\theta(x^{(i)}) - y{(i)}]^2 \\
    \frac{\partial }{\partial \theta_j} J(\theta) = 0,\ (solve\ for\ \theta_0\ ,\theta_1\ ,...\ ,\theta_n)
    \end{align} \\
\end{center}
\textbf{Example,} \\
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|} \hline
         & Size (feet^2) & No.of Bed rooms & No.of floors & Age of home (years) & Price(\$1000) \\ \hline
        x_0 & x_1 & x_2 & x_3 & x_4 & y \\ \hline 
        1 & 2104 & 5 & 1 & 45 & 460 \\ \hline
        1 & 1416 & 3 & 2 & 40 & 232 \\ \hline
        1 & 1534 & 3 & 2 & 30 & 315 \\ \hline
        1 & 1852 & 2 & 1 & 36 & 178 \\ \hline
    \end{tabular}
    \caption{Sample Training set for Multi-Variate Linear regression.}
    \label{tab:my_label}
\end{table} 
\begin{enumerate}
    \item n = number of Features. 
    \item $x_j^{(i)}$ = value of j in the $i^{th}$ training example.
    \item $x_{(i)}$ = the input(features) of the $i^{th}$ training example.
\end{enumerate} \\
\begin{math}
X = \left[\begin{array}{ccccc}
    1 & 2104 & 5 & 1 & 45 \\
    1 & 1416 & 3 & 2 & 40 \\
    1 & 1534 & 3 & 2 & 30 \\
    1 & 852 & 2 & 1 & 36
\end{array} \right] &\;\;\;\;\;\;\;\;\; Y = \left[\begin{array}{c}
    460  \\
    232 \\
    315 \\
    178 
\end{array} \right]
\end{math} \\
X is m$\times$(n+1)-dimensional\ matrix\ and\ Y\ is\ a\ m-dimensional\ vector. \\
\begin{align}
\theta &= {(X^{\top}X)}^{-1} X^{\top} Y
\end{align} \\
The above $\theta$\ value\ is\ optimal\ $\theta$\ value. \\
For Normal equation method, then no need to use \textbf{feature scaling.} \\
We use 'Gradient descent' and 'Normal equation' methods to minimise cost function. \\
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} \hline
        Gradient descent & Normal equation  \\ \hline
        1$\rangle$ Need to choose '$\alpha$'. & 1$\rangle$ No need to choose '$\alpha$'. \\ \hline
        2$\rangle$ Need many iterations. & 2$\rangle$ Don't need to iterate. \\ \hline
        3$\rangle$ Works well even, when 'n' is large. (n>10000) & 3$\rangle$ Need to compute n$\times$ n matrix inverse {(X^{$\top$}X)}^{-1} \\ \hline
         & 4$\rangle$ Works Now if n is very large. \\ \hline
    \end{tabular}
    \caption{Why should we use the particular method? Advantages and Disadvantages of two methods.}
    \label{tab:my_label}
\end{table} 
\begin{align}
\theta &= {(X^{\top}X)}^{-1} X^{\top} Y
\end{align} \\
Q$\rangle$ What is $X^{\top}X$ is non-invertible(singular/degenerate) ? \\
Reasons, 
\begin{enumerate}
    \item Redundant features (linearly dependent)
    \begin{itemize}
        \item $x_1$ = Size in $feet^2$ 
        \item $x_2$ = Size in $m^2$
        \item $x_2$ = ${(3.28)}^2$ $x_1$ , 1m = 3.28feet. 
    \end{itemize}
    \item Too many features. (m$\le$n)
    \begin{itemize}
        \item m = 10
        \item n = 100, $\theta\ \epsilon\ {\Re}^{101}$, Delete some features (or) Regularization.
    \end{itemize}
\end{enumerate} \\
\subsection{Classification} 
The output value 'y' is \textbf{discrete value.} \\
The algorithm used is \textbf{logistic regression.} \\
\subsection{Logistic Regression Model}
$\therefore$ We want $0\le h_\theta(x) \le 1$. \\
\begin{align}
    h_\theta(x) &= g(\theta^{\top} x) \\
    g(z) &= \frac{1}{1+e^{-z}} \\
    h_\theta(x) &= g(\theta^{\top} x) \\
                &= \frac{1}{1+e^{-\theta^{\top} x}}
\end{align} \\
The above g(z) is called sigmoid function (or) logistic function. \\
\textbf{Graph,} \\
\begin{center}
\begin{tikzpicture}
\begin{axis}
\addplot[color=red]{1/(1+exp(-x))};
\end{axis}
\end{tikzpicture}
\end{center}
\begin{align}
    g(z) &\ge 0.5,\ when\ z \ge 0. \\
    h_\theta(x) = g(\theta^{\top} x) &\ge 0.5,\ when\ \theta^{\top} x \ge 0.
\end{align} 
\subsection{Interpretation of hypothesis output for Logistic Regression}
\begin{align}
    h_\theta(x) &= P(y=1|x;\theta) \\
    P(y=0|x;\theta) &+ P(y=1|x;\theta) = 1 \\
    P(y=0|x;\theta) &= 1-P(y=1|x;\theta) \\
                    &= 1-h_\theta(x) \\
\end{align} 
\therefore\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; y = 0 (or) 1. \\
\textbf{Decision boundary,} \\
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    enlargelimits=false,
]
\addplot[
    only marks,
    scatter,
    mark=x,
    mark size=2.9pt]
table[meta=ma]
{scattered_example.dat};
\end{axis}
\end{tikzpicture}
\end{center} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.45\textwidth]{Screenshot (79).png}
\caption{A simple decision boundary looks like.}
\label{fig:fig2}
\end{figure} \\
For, the above diagram decision boundary will be a line separating the two output values of y(y=0 (or) y=1). \\
\begin{align}
    h_\theta(x) &\ge 0.5 \rightarrow y=1. \\
    h_\theta(x) &< 0.5 \rightarrow y=0. \\
    h_\theta(x) &= g(\theta^{\top} x) \\
                &= g(\theta_0 + \theta_1x_1 + \theta_2x_2)
\end{align} 
\begin{math}
\theta = \left[\begin{array}{c}
    \theta_0  \\
    \theta_1 \\
    \theta_2
\end{array} \right] & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; x = \left[\begin{array}{c}
    1  \\
    x_1 \\
    x_2
\end{array} \right]
\end{math} \\
\textbf{Example,} \\
Let, \\
\begin{center}
\begin{math}
\theta = \left[\begin{array}{c}
    -3  \\
    1 \\
    1
\end{array} \right] 
\end{math}
\end{center}
\begin{align}
    y = 1, if\ \theta^{\top} x &\ge 0 \\
               -3+x_1+x_2 &\ge 0 \\
               x_1+x_2 &\ge 3,\ y=1 \\
               x_1+x_2 &<3,\ y=0.
\end{align} 
\textbf{Non-Linear Decision Boundary,} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{Screenshot (83).png}
\end{figure} \\
So, in the above non-linear classification, the \textbf{decision boundary is a circle} of radius of 1unit. \\
\begin{align}
    Inside\ circle,\ y &= 0 \\
    Outside\ circle,\ y &= 1.
\end{align} \\
\subsection{Cost function for Logistic Regression}
\textbf{Training set,} \\
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} \hline
        x^{(1)} & y^{(1)} \\ \hline
        x^{(2)} & y^{(2)} \\ \hline
        x^{(3)} & y^{(3)} \\ \hline
        \vdots & \vdots \\ \hline
        x^{(m)} & y^{(m)} \\ \hline
    \end{tabular}
    \caption{Training set of m-examples for Logistic Regression}
    \label{tab:my_label}
\end{table} \\
\begin{math}
x = \left[\begin{array}{c}
     x_0 \\
     x_1 \\
     x_2 \\
     \vdots \\
     x_n
\end{array}\right] \epsilon\ \Re^{n+1}\ -\ n\ features.\ x_0 = 1,\ y\ \epsilon\ {0,1}. 
\end{math} \\
For linear regression, \\
\begin{align}
    J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} [h_\theta(x^{(i)} - y^{(i)}]^2 \\
    J(\theta) = \frac{1}{m} \sum_{i=1}^{m} Cost(h_\theta(x^{(i)}, y^{(i)}) \\
    Cost(h_\theta(x^{(i)}), y^{(i)}) = \frac{1}{2} [h_\theta(x^{(i)} - y^{(i)}]^2
\end{align} \\
\begin{center}
\begin{tikzpicture}
\begin{axis}[title={Convex function},
    xlabel={x-axis},
    ylabel={y-axis}]
\addplot[color=red]{x^2};
\end{axis}
\end{tikzpicture}
\end{center}
The above graph is a convex. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.45\textwidth]{Non-convex(10.png}
\caption{Graph of non-convex function.}
\label{fig:fig2}
\end{figure} \\
The below graph is a non-convex.\\
If we use the cost function of linear regression in logistic regression, the the we would get non-convex cost function, because the \textbf{hypothesis is} $\frac{1}{1+e^{-\theta^{\top}x}}$. \\
For Logistic regression, 
\begin{align}
    Cost(h_\theta(x^{(i)}), y^{(i)}) =
    \begin{cases}
    -\log(1-h_\theta(x)), & if\ y = 0\\
    -\log(h_\theta(x)), & if\ y=1.
    \end{cases}
\end{align} \\
If y=1, \\
% \begin{center}
% \begin{tikzpicture}
% \begin{axis}[title={Convex function},
%     xlabel={h_\theta(x)}
%     ylabel={Cost function}]
% \addplot[color=red]{-ln(x)};
% \end{axis}
% \end{tikzpicture}
% \end{center}
\begin{center}
\setlength{\unitlength}{0.8cm}
\begin{picture}(6,6)
\thicklines
\put(1,1){\line(1,0){5}}
\put(1,1){\line(0,1){5}}
\qbezier(1.3,6)(1.5,1.693147)(6,1)
\put(3,0.5){$h_\theta(x)$}
\put(0.5,2.5){\begin{turn}{90}Cost function\end{turn}}
\put(5.95,0.9){$|$}
\put(5.95,0.3){1}
\put(0.95,0.9){$|$}
\put(1,0.3){0}
\end{picture}
\end{center} \\
If y=0, \\
\begin{center}
\setlength{\unitlength}{0.8cm}
\begin{picture}(6,6)
\thicklines
\put(1,1){\line(1,0){5}}
\put(1,1){\line(0,1){5}}
\qbezier(1,1)(5,1.3)(6,6)
\put(3,0.5){$h_\theta(x)$}
\put(0.5,2.5){\begin{turn}{90}Cost function\end{turn}}
\put(5.95,0.9){$|$}
\put(5.95,0.3){1}
\put(0.95,0.9){$|$}
\put(1,0.3){0}
\end{picture}
\end{center} \\
\textbf{Simplified Cost function,} \\
\begin{align}
    Cost(h_\theta(x^{(i)}), y^{(i)}) &= -(1-y)\log(1-h_\theta(x)) -y\log(h_\theta(x)).\ \forall\ y\ \epsilon\ \{0,1\}. \\
    If\ y &= 1: Cost(h_\theta(x), y) = -\log(h_\theta(x)). \\
    If\ y &= 0: Cost(h_\theta(x), y) = -\log(1-h_\theta(x)). 
\end{align}
\begin{align}
    Cost function = J(\theta) &= \frac{1}{m} \sum_{i=1}^{m} Cost(h_\theta(x^{(i)}, y^{(i)}) \\
    &= \frac{1}{m} [-\sum_{i=1}^{m} (1-y^{(i)})\log(1-h_\theta(x^{(i)})) +y^{(i)} \log(h_\theta(x^{(i)}))] 
\end{align}
\subsection{Gradient Descent for Logistic Regression}
\begin{align}
    \text{Repeat\ until\ convergence\ (minimum)}& \Bigg\{ \theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j} J(\theta) \\
    \frac{\partial}{\partial \theta_j} J(\theta) &= \frac{1}{m} \sum_{i=1}^{m} [h_\theta(x^{(i)}) - y^{(i)}]x_j^{(i)}
\end{align}
\subsection{Optimization algorithm}
\begin{enumerate}
    \item Gradient descent. 
    \item Conjugate gradient.
    \item BFGS.
    \item L - BFGS.
\end{enumerate}
These are the 4 algorithms to minimise \textbf{cost function.} \\
Advantages of the \textbf{last three advanced optimization algorithm.} 
\begin{itemize}
    \item No need to manually pick $\alpha$.
    \item Often faster than Gradient descent.
    \item They themselves choose $\alpha$, for faster convergence.
\end{itemize} 
\subsection{Multiclass Classification : One-vs-All}
\begin{align*}
y &\in \lbrace0, 1 ... n\rbrace \\
h_\theta^{(0)}(x) &= P(y = 0 | x ; \theta) \\
h_\theta^{(1)}(x) &= P(y = 1 | x ; \theta) \\
&\vdots \\
h_\theta^{(n)}(x) &= P(y = n | x ; \theta) \\
\mathrm{prediction} &= \max_i( h_\theta ^{(i)}(x) ) 
\end{align*}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{multi class.png}
\caption{Multiclass being classified into n-classifiers for n-types of output.}
\label{fig:fig2}
\end{figure} 
To summarize, 
\begin{enumerate}
    \item Train a logistic regression classifier $h_\theta(x)$ for each class to predict the probability that y=i.
    \item To make a prediction on a new x, pick the class that maximizes  $h_\theta(x)$
\end{enumerate}
\subsection{Problem of Overfitting,} 
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{Screenshot (89).png}
\caption{Example for underfit and overfit hypothesis.}
\label{fig:fig2}
\end{figure} \\
Similar for Logistic Regression. \\
\textbf{Addressing Overfitting,} 
\begin{enumerate}
    \item Reduce number of features.
    \begin{itemize}
        \item Manually select which features to keep.
        \item Model selection algorithm.
    \end{itemize}
    \item Regularization
    \begin{itemize}
        \item Keep all features, but reduce magnitude/values of parameters $\thicklines_j$.
        \item Works well when we have a lot of features, each of which contributes a bit to predict $y^{(i)}$.
    \end{itemize}
\end{enumerate}
\subsection{Regularization}
Small values for parameters $\theta_0,\theta_1,\theta_2,...,\theta_n$.
\begin{enumerate}
    \item Simpler hypothesis.
    \item Less prone to overfitting.
\end{enumerate}
\textbf{Regularized Cost function,}
\begin{align}
    J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \bigg[[h_\theta(x^{(i)} - y^{(i)}]^2 + \lambda\sum_{j=1}^{n} \theta_j^2\bigg] 
\end{align} \\
If '$\lambda$' is extremely large, then the cost function will become underfitting (doesn't fit to our training data). \\
\begin{align*} 
\text{Repeat}\ \lbrace & \\
\theta_0 &:= \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\
\theta_j &:= \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\\
\rbrace
\end{align*}
The term $\frac{\lambda}{m} \theta_j$ performs our regularization. With some manipulation our update rule can also be represented as:
\begin{align}
\theta_j := \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
\end{align}
The first term in the above equation, $1 - \alpha\frac{\lambda}{m}$ will always be less than 1. Intuitively you can see it as reducing the value of $\theta_j$ by some amount on every update. Notice that the second term is now exactly the same as it was before. \\
\textbf{Normal equation after regularization,}
\begin{align}
\theta &= {(X^{\top}X) + \lambda.L}^{-1} X^{\top} Y
\end{align} 
\begin{math}
where\ L = \left[\begin{array}{ccccc}
    0 &  &  &  &   \\
     & 1 &  &  &   \\
     &  & 1 &  & \\
     &  &  & \ddots &  \\
     &  &  &  & 1 \\
\end{array} \right]
\end{math} 
\subsection{Regularized Logistic Regression}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{regular logis 1.png}
\caption{Regularization of a overfit data.}
\label{fig:fig2}
\end{figure}
\textbf{Cost function,} 
\begin{align}
J(\theta) = \frac{1}{m} \bigg[-\sum_{i=1}^{m} (1-y^{(i)})\log(1-h_\theta(x^{(i)})) +y^{(i)} \log(h_\theta(x^{(i)}))\bigg] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
\end{align} 
\newpage The second sum, $\sum_{j=1}^n \theta_j^2$ means to explicitly exclude the bias term, $\theta_0$. I.e. the θ vector is indexed from 0 to n (holding n+1 values, $\theta_0$ through $\theta_n$), and this sum explicitly skips $\theta_0$, by running from 1 to n, skipping 0. Thus, when computing the equation, we should continuously update the two following equations: 
\begin{align*} 
\text{Repeat}\ \lbrace & \\
\theta_0 &:= \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\
\theta_j &:= \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\\
\rbrace
\end{align*}
\section{Neural Networks}
\subsection{Model representation I}
Let's examine how we will represent a hypothesis function using neural networks. At a very simple level, neurons are basically computational units that take inputs (dendrites) as electrical inputs (called \textbf{"spikes"}) that are channeled to outputs (axons). In our model, our dendrites are like the input features $x_1\cdots x_n$, and the output is the result of our hypothesis function. In this model our $x_0$ input node is sometimes called the \textbf{"bias unit".} It is always equal to 1. In neural networks, we use the same logistic function as in classification, $\frac{1}{1 + e^{-\theta^Tx}}$, yet we sometimes call it a sigmoid (logistic) activation function. In this situation, our \textbf{"theta" parameters} are sometimes called \textbf{"weights".} \\
A simple representation looks like : 
\begin{center}
\begin{math}
\left[ \begin{array}{c}
    x_0x_1x_2  \\
\end{array}\right] \rightarrow \left[\; \right] \rightarrow h_\theta(x)
\end{math}
\end{center} \\
Our input nodes (layer 1), also known as the \textbf{"input layer"}, go into another node (layer 2), which finally outputs the hypothesis function, known as the \textbf{"output layer".} \\
We can have intermediate layers of nodes between the input and output layers called the \textbf{"hidden layers."} \\
In this example, we label these intermediate or \textbf{"hidden"} layer nodes $a^2_0, \cdots, a^2_na$ and call them \textbf{"activation units."} 
\begin{center}
    \begin{enumerate}
        \item $a_i^{(j)}$ = "activation" of unit i in layer j
        \item $\Theta^{(j)}$ = matrix of weights controlling function mapping from layer j to layer j+1
    \end{enumerate}
\end{center}
If we had one hidden layer, it would look like : \\
\begin{center}
\begin{math}
\left[ \begin{array}{c}
    x_0x_1x_2x_3  \\
\end{array}\right] \rightarrow \left[a_1^{(2)}a_2^{(2)}a_3^{(2)} \right] \rightarrow h_\theta(x)
\end{math}
\end{center}
The values for each of the \textbf{"activation"} nodes is obtained as follows : \\
\begin{align}
    a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \\
    a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \\
    a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \\
    h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})
\end{align}
This is saying that we compute our activation nodes by using a 3$\times$4 matrix of parameters. We apply each row of the parameters to our inputs to obtain the value for one activation node. Our hypothesis output is the logistic function applied to the sum of the values of our activation nodes, which have been multiplied by yet another parameter matrix $\Theta^{(2)}$ containing the weights for our second layer of nodes. \\
Each layer gets its own matrix of weights, $\Theta^{(j)}$. \\
The dimensions of these matrices of weights is determined as follows : \\ 
If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1} \times (s_j + 1)$. \\
The +1 comes from the addition in $\Theta^{(j)}$ of the \textbf{"bias nodes,"} $x_0$ and $\Theta_0^{(j)}$. In other words the output nodes will not include the bias nodes while the inputs will. \\ 
\subsection{Model representation II}
we'll do a vectorized implementation of the above functions. We're going to define a new variable $z_k^{(j)}$ that encompasses the parameters inside our g function. \\
\begin{align}
    a_1^{(2)} &= g(z_1^{(2)}) \\
    a_2^{(2)} &= g(z_2^{(2)}) \\
    a_3^{(2)} &= g(z_3^{(2)}) \\
    x &= a^{(1)} \\
    a^{(2)} &= g(\theta^{(1)} x) \\
    &= g(\theta^{(1)} a^{(1)}) = g(z^{(2)}) \\
    h_\theta(x) &= g(\theta^{(2)} a^{(2)}) \\
    &= g(z^{(3)})
\end{align}
In other words, for layer j=2 and node k, the variable z will be : \\
\begin{align}
    z_k^{(2)} = \Theta_{k,0}^{(1)} x_0 + \Theta_{k,1}^{(1)} x_1 + \cdots + \Theta_{k,n}^{(1)} x_n
\end{align}
The vector representation of x and $z^{j}$ is : \\
\begin{math}
x = \left[\begin{array}{c}
x_0 \\
x_1 \\
\vdots \\
x_n
\end{array} \right] \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
z^{(j)} = \left[\begin{array}{c}
z_1^{(j)} \\
z_2^{(j)} \\
\vdots \\
z_n^{(j)}
\end{array} \right]
\end{math} \\
Setting x = $a^{(1)}$, we can rewrite the equation as :
\begin{align}
    z^{(j)} = \theta^{(j-1)} a^{(j-1)}
\end{align} 
We are multiplying our matrix $\Theta^{(j-1)}$ with dimensions $s_j\times$ (n+1)(where $s_j$ is the number of our activation nodes) by our vector $a^{(j-1)}$ with height (n+1). This gives us our vector $z^{(j)}$ with height $s_j$. Now we can get a vector of our activation nodes for layer j as follows : \\
\begin{align} 
a^{(j)} = g(z^{(j)})a 
\end{align}
Where our function g can be applied element-wise to our vector $z^{(j)}$ \\
We can then add a bias unit (equal to 1) to layer j after we have computed $a^{(j)}$. This will be element $a_0^{(j)}$ and will be equal to 1. To compute our final hypothesis, let's first compute another z vector : \\
\begin{align}
z^{(j+1)} = \Theta^{(j)}a^{(j)}
\end{align}
We get this final z vector by multiplying the next theta matrix after $\Theta^{(j-1)}$ with the values of all the activation nodes we just got. This last theta matrix $\Theta^{(j)}$ will have only one row which is multiplied by one column $a^{(j)}$ so that our result is a single number. We then get our final result with : \\
\begin{align}
h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)})
\end{align}
Notice that in this last step, between layer j and layer j+1, we are doing exactly the same thing as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypothesis. \\
\textbf{Examples,} \\
A simple example of applying neural networks is by predicting $x_1$ AND $x_2$, which is the logical 'and' operator and is only true if both $x_1$ and $x_2$ are 1.
\begin{align*}
h_\Theta(x) &= g(-30 + 20x_1 + 20x_2) \\
x_1 &= 0 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-30) \approx 0 \\
x_1 &= 0 \ \ and \ \ x_2 = 1 \ \ then \ \ g(-10) \approx 0 \\
x_1 &= 1 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-10) \approx 0 \\
x_1 &= 1 \ \ and \ \ x_2 = 1 \ \ then \ \ g(10) \approx 1
\end{align*}
So we have constructed one of the fundamental operations in computers by using a small neural network rather than using an actual AND gate. Neural networks can also be used to simulate all the other logical gates. The following is an example of the logical operator 'OR', meaning either $x_1$ is true or $x_2$ is true, or both :
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{OR 1.png}
\caption{Hypothesis of neural networks is represented through logical gates.}
\label{fig:fig2}
\end{figure} \\
The $Θ^{(1)}$ matrices for AND, NOR, and OR are : 
\begin{align*}
AND:\\
\Theta^{(1)} &=\begin{bmatrix}-30 & 20 & 20\end{bmatrix} \\
NOR:\\
\Theta^{(1)} &= \begin{bmatrix}10 & -20 & -20\end{bmatrix} \\
OR:\\
\Theta^{(1)} &= \begin{bmatrix}-10 & 20 & 20\end{bmatrix} \\
\end{align*}
We can combine these to get the XNOR logical operator (which gives 1 if $x_1$ and $x_2$ are both 0 or both 1). 
\begin{align*}
\begin{bmatrix}
x_0 \\
x_1 \\
x_2\end{bmatrix} 
\rightarrow \begin{bmatrix}
a_1^{(2)} \\
a_2^{(2)} \end{bmatrix}
\rightarrow\begin{bmatrix}
a^{(3)}\end{bmatrix} 
\rightarrow h_\Theta(x)
\end{align*}
Let's write out the values for all our nodes :
\begin{align*}
a^{(2)} = g(\Theta^{(1)} \cdot x) \\
a^{(3)} = g(\Theta^{(2)} \cdot a^{(2)}) \\
h_\Theta(x) = a^{(3)}
\end{align*}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{XNOR.png}
\caption{Hypothesis in XNOR logical gate.}
\label{fig:fig2}
\end{figure} 
\subsection{Multiclass Classification}
To classify data into multiple classes, we let our hypothesis function return a vector of values. Say we wanted to classify our data into one of four categories. We will use the following example to see how this classification is done. This algorithm takes as input an image and classifies it accordingly : \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{neural multi.png}
\caption{Multiclass Classification for different outputs for different things.}
\label{fig:fig2}
\end{figure} \\
\notte{\textbf{As a general rule, the when the number of classes are greater than 2, then the classes should be one hot encoded to ease the process of classification. It can be seen an defining individual logistic units for determining whether or not it belongs to that class.}} \\ \newpage
We can define our set of resulting classes as y : \\
\begin{figure}[h]
\centering
\includegraphics[width = 1.2\textwidth]{y.png}
\caption{The output 'y' of neural networks.}
\label{fig:fig2}
\end{figure} \\
Each $y^{(i)}$ represents a different image corresponding to either a car, pedestrian, truck, or motorcycle. The inner layers, each provide us with some new information which leads to our final hypothesis function. The setup looks like : \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{h.png}
\caption{Hypothesis calculation of neural networks.}
\label{fig:fig2}
\end{figure} \\ \newpage
Our resulting hypothesis for one set of inputs may look like : 
\begin{center}
    \begin{math}
    h_\theta(x) = \left[\begin{array}{c}
        0\ 0\ 1\ 0  \\
    \end{array} \right]
    \end{math}
\end{center} \\
In which case our resulting class is the third one down, or $h_\Theta(x)_3$, which represents the motorcycle. \\
\subsection{Cost function for Neural networks}
Let's declare some variables. 
\begin{enumerate}
    \item L = total number of layers in the network.
    \item $s_l$ = number of units (not counting bias unit) in layer l.
    \item K = number of output units/classes.
\end{enumerate}
\textbf{The cost function for regularized logistic regression,}
\begin{align}
    J(\theta) = \frac{1}{m} \bigg[-\sum_{i=1}^{m} (1-y^{(i)})\log(1-h_\theta(x^{(i)})) +y^{(i)} \log(h_\theta(x^{(i)}))\bigg] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
\end{align} \\
\textbf{The cost function for Neural networks,}
\begin{align}
    J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}\sum_{k=1}^{K} \bigg[(1-y^{(i)}_k)\log(1-(h_\theta(x^{(i)}))_k) +y^{(i)}_k \log((h_\theta(x^{(i)}))_k)\bigg] + \frac{\lambda}{2m} \sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}} (\Theta_{j,i}^{(l)})^2
\end{align} \\
In the regularization part, after the square brackets, we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term. \\
\begin{enumerate}
    \item The double sum simply adds up the logistic regression costs calculated for each cell in the output layer.
    \item The triple sum simply adds up the squares of all the individual Θs in the entire network.
    \item The i in the triple sum does not refer to training example i.
\end{enumerate}
\subsection{Backpropagation Algorithm}
\textbf{"Backpropagation"} is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression. Our goal is to compute : 
\begin{align}
    \min_\Theta J(\Theta)
\end{align}
That is, we want to minimize our cost function J using an optimal set of parameters in theta. In this section we'll look at the equations we use to compute the partial derivative of J(Θ) :
\begin{align}
    \frac{\partial}{\partial \Theta_{j,i}^{(l)}} J(\Theta)
\end{align}
\textbf{Backpropagation algorithm,}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{Back propagation.png}
\caption{Backpropagation algorithm.}
\label{fig:fig2}
\end{figure} 
\begin{enumerate}
    \item Given training set $\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace$.
    \begin{itemize}
        \item Set $\Delta^{(l)}_{i,j} :=$ 0 for all (l,i,j), (hence you end up having a matrix full of zeros)
    \end{itemize}
    \item For training example t = 1 to m :
    \begin{itemize}
        \item Set $a^{(1)} := x^{(t)}$.
        \item Perform forward propagation to compute $a^{(l)}$ for l=2,3,…,L.
        \item Using $y^{(t)}$, compute $\delta^{(L)} = a^{(L)} - y^{(t)}$.
        Where L is our total number of layers and $a^{(L)}$ is the vector of outputs of the activation units for the last layer. So our \textbf{"error values"} for the last layer are simply the differences of our actual results in the last layer and the correct outputs in y. To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left :
        \begin{figure}[!htb]
        \centering
        \includegraphics[width = 1.1\textwidth]{Back gradient.png}
        \caption{Calculation of values of activation and $\Theta$.}
        \label{fig:fig2}
        \end{figure} 
        \item Compute $\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}$ using $\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})$. \\
        The delta values of layer l are calculated by multiplying the delta values in the next layer with the theta matrix of layer l. We then element-wise multiply that with a function called $g^\prime$, or g-prime, which is the derivative of the activation function g evaluated with the input values given by $z^{(l)}$. \\
        The g-prime derivative terms can also be written out as :
        \begin{align}
            g^\prime(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})
        \end{align}
        \item  $\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}$ (or) with vectorization, $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$. \\
        Hence we update our new $\Delta$ matrix.
        \begin{enumerate}
            \item $D_{i,j}^{(l)} := \frac{1}{m} (\Delta_{i,j}^{(l)} + \lambda\Theta_{i,j}^{(l)})$, if j\neq 0. \\
            \item $D_{i,j}^{(l)} := \frac{1}{m} (\Delta_{i,j}^{(l)}$, if j = 0.
        \end{enumerate}
        The capital-delta matrix D is used as an "accumulator" to add up our values as we go along and eventually compute our partial derivative. Thus we get $\frac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta) = D_{ij}^{(l)}$.
    \end{itemize}
\end{enumerate}
If we consider simple non-multiclass classification (k = 1) and disregard regularization, the cost is computed with :
\begin{align}
    Cost(t) = (1-y^{(t)})\log(1-h_\theta(x^{(t)})) +y^{(t)} \log(h_\theta(x^{(t)}))
\end{align}
Intuitively, $\delta_j^{(l)}$ is the \textbf{"error"} for $a^{(l)}_j$ (unit j in layer l). More formally, the delta values are actually the derivative of the cost function :
\begin{align}
    \delta_{j}^{(l)} = \frac{\partial}{\partial z_j^{(l)}} cost(t)
\end{align}
Our derivative is the slope of a line tangent to the cost function, so the steeper the slope the more incorrect we are. Let us consider the following neural network below and see how we could calculate some $\delta_j^{(l)}$ :
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{Forward.png}
\caption{The forward propagation in backward propagation for $\delta_j^{(l)}$.}
\label{fig:fig2}
\end{figure} \newpage
In the image above, to calculate $\delta_2^{(2)}$, we multiply the weights $\Theta_{12}^{(2)}$ and $\Theta_{22}^{(2)}$ by their respective $\delta$ values found to the right of each edge. So we get $\delta_2^{(2)} = \Theta_{12}^{(2)}*\delta_1^{(3)}+\Theta_{22}^{(2)}*\delta_2^{(3)}$. To calculate every single possible $\delta_j^{(l)}$, we could start from the right of our diagram. We can think of our edges as our $\Theta_{ij}$. Going from right to left, to calculate the value of $\delta_j^{(l)}$, you can just take the over all sum of each weight times the $\delta$ it is coming from. Hence, another example would be $\delta_2^{(3)} =\Theta_{12}^{(3)}*\delta_1^{(4)}$.
\subsection{Gradient Checking}
Gradient checking will assure that our backpropagation works as intended. We can approximate the derivative of our cost function with :
\begin{align}
    \frac{\partial}{\partial \theta} J(\Theta) \approx \frac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}
\end{align}
With multiple theta matrices, we can approximate the derivative with respect to $\Theta_j$ as follows :
\begin{align}
    \frac{\partial}{\partial \theta_j} J(\Theta) \approx \frac{J(\Theta_1,...,\Theta_j + \epsilon,..., \Theta_n) - J(\Theta_1,...,\Theta_j - \epsilon,..., \Theta_n)}{2\epsilon}
\end{align}
A small value for ${\epsilon}$ (epsilon) such as ${\epsilon = 10^{-4}}$, guarantees that the math works out properly. If the value for $\epsilon$ is too small, we can end up with numerical problems. \\
We previously saw how to calculate the deltaVector. So once we compute our gradApprox vector, we can check that gradApprox $\approx$ deltaVector. \\
Once you have verified once that your backpropagation algorithm is correct, you don't need to compute gradApprox again. The code to compute gradApprox can be very slow.
\subsection{Random Intialization}
Initializing all theta weights to zero does not work with neural networks. When we backpropagate, all nodes will update to the same value repeatedly. Instead we can randomly initialize our weights for our $\Theta$ matrices using the following method :
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{random.png}
\caption{Random intialization to begin and get into the problem.}
\label{fig:fig2}
\end{figure} 
\newpage Hence, we initialize each $\Theta^{(l)}_{ij}$ to a random value between $[-\epsilon,\epsilon]$. Using the above formula guarantees that we get the desired bound. \\
\boxed{\textbf{The epsilon used above is unrelated to the epsilon from Gradient Checking.}}
\subsection{Choosing Neural network}
\begin{enumerate}
    \item Pick a network architecture.
    \item Choose the layout of your neural network.
    \item Including how many hidden units in each layer and how many layers in total you want to have.
    \begin{itemize}
        \item Number of input units = dimension of features $x^{(i)}$.
        \item Number of output units = number of classes.
        \item Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units).
        \item \textbf{Defaults} : 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.
    \end{itemize}
\end{enumerate}
\subsection{Training a Neural Network}
\begin{enumerate}
    \item Randomly initialize the weights.
    \item Implement forward propagation to get $h_\Theta(x^{(i)})$. \\
    \item Implement the cost function.
    \item Implement backpropagation to compute partial derivatives. ($\frac {\partial} {\partial \Theta_{jk}^{(l)}} J(\Theta)$)
    \item Use gradient checking to compare ($\frac {\partial} {\partial \Theta_{jk}^{(l)}} J(\Theta)$) given by backpropagation vs the numerical estimate of gradient. S0, to confirm that your backpropagation works. Then disable gradient checking.
    \item Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.
\end{enumerate}
\notte{\textbf{Ideally, you want $h_\Theta(x^{(i)}) \approx≈ y^{(i)}$. This will minimize our cost function. However, keep in mind that $J(\Theta)$ is not convex and thus we can end up in a local minimum instead. }}
\subsection{Neural network Architecture}
This usually means to pick the connectivity pattern between the neurons. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{network.png}
\caption{Different architectures of neural networks.}
\label{fig:fig2}
\end{figure} \\
Above figure shows a few examples of network architectures. It can be seen that the three networks have the same number of input and output units. This is so because the input units equals input features in a given training example while the output units equals the number of target classes. \\
\notte{\textbf{For neural networks the cost function, $J(\Theta)$ is non-convex and hence susceptible to local minima. But in practice it does not present a serious problem in the implementation.}}
\chapter{Deciding whether the algorithm is perfect or not}
\section{Evaluating the algorithm}
\subsection{Evaluating a Hypothesis}
\textbf{Fails to generalize to new examples not in training set.} \\
\textbf{Once we have done some trouble shooting for errors in our predictions by : } 
\begin{enumerate}
    \item Getting more training examples.
    \item Trying smaller sets of features.
    \item Trying additional features.
    \item Trying polynomial features.
    \item Increasing or decreasing $\lambda$.
\end{enumerate}
We can move on to evaluate our new hypothesis. \\
A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a training set and a test set. Typically, the training set consists of 70$\%$ of your data and the test set is the remaining 30$\%$. \\
The new procedure using these two sets is then :  \newpage
\textbf{Dataset,} \\
\begin{table}[h!]
    \centering
    \begin{tabular}{c|ccc}
        Size & Price  \\ \hline
        2104 & 400  &  & (x^{(1)},y^{(1)})\\
        1600 & 330  &  & (x^{(2)},y^{(1)})\\
        2400 & 369  &  & (x^{(3)},y^{(1)})\\
        1416 & 232  & $\rightarrow$ &  \vdots\\
        3000 & 540  &  &  \vdots\\
        1985 & 300  &  &  \vdots\\
        1534 & 315 &  & (x^{(m)},y^{(m)})\\ \hline
        1427 & 199  &  & (x^{(1)}_{test},y^{(1)}_{test})\\
        1380 & 212  & $\longmapsto$ & (x^{(2)}_{test},y^{(2)}_{test})\\
        1494 & 243  &  & (x^{(m_{test})}_{test},y^{(m_{test})}_{test})\\ \hline
    \end{tabular}
    \caption{Evaluating your hypothesis}
    \label{tab:my_label}
\end{table}
\textbf{Model selection,}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{model.png}
\caption{Model selectio process.}
\label{fig:fig2}
\end{figure}
\begin{enumerate}
    \item $m_{test}$ = number of test example. 
    \item Learn $\Theta$ and minimize $J_{train}(\Theta)$ using the training set.
    \item Compute the test set error $J_{test}(\Theta)$.
\end{enumerate}
\subsection{The test set error}
\begin{enumerate}
    \item For linear regression: $J_{test}(\Theta) = \dfrac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_\Theta(x^{(i)}_{test}) - y^{(i)}_{test})^2$.
    \item For classification Misclassification error (aka 0/1 misclassification error):
    \begin{align}
        err(h_\Theta(x),y) = \begin{cases}
        1 & if\ h_\Theta(x) \geq 0.5\ and\ y = 0\ or\ h_\Theta(x) < 0.5\ and\ y = 1 \\
        0 & otherwise
        \end{cases}
    \end{align}
\end{enumerate}
This gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is :
\begin{align}
\text{Test Error} = \dfrac{1}{m_{test}} \sum^{m_{test}}_{i=1} err(h_\Theta(x^{(i)}_{test}), y^{(i)}_{test})
\end{align}
This gives us the proportion of the test data that was misclassified.
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{train test.png}
\caption{Cost function for training, cross-validation and test error.}
\label{fig:fig2}
\end{figure}
\subsection{Model Selection and Train/ Validation/ Test Sets}
Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could over fit and as a result your predictions on the test set would be poor. The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than the error on any other data set. \\
Given many models with different polynomial degrees, we can use a systematic approach to identify the 'best' function. In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result. \\
One way to break down our dataset into the three sets is :
\begin{enumerate}
    \item Training set: 60$\%$.
    \item Cross validation set: 20$\%$.
    \item Test set: 20$\%$.
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{dataset.png}
\caption{Separation of dataset into trsin, cross-validation and test sets.}
\label{fig:fig2}
\end{figure}
We can now calculate three separate error values for the three different sets using the following method:
\begin{enumerate}
    \item Optimize the parameters in Θ using the training set for each polynomial degree.
    \item Find the polynomial degree d with the least error using the cross validation set.
    \item Estimate the generalization error using the test set with $J_{test}(\Theta^{(d)})$, (d = theta from polynomial with lower error).
\end{enumerate}
\boxed{\textbf{This way, the degree of the polynomial d has not been trained using the test set.}}
\subsection{Diagnosing Bias vs Variance}
In this section we examine the relationship between the degree of the polynomial (d) and the underfitting (or) overfitting of our hypothesis.
\begin{itemize}
    \item We need to distinguish whether \textbf{bias} or \textbf{variance} is the problem contributing to bad predictions.
    \item High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two.
\end{itemize}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{d bais variance.png}
\caption{Diagnosing bias and variance.}
\label{fig:fig2}
\end{figure}
The training error will tend to \textbf{decrease} as we increase the degree d of the polynomial. \\
At the same time, the cross validation error will tend to \textbf{decrease} as we increase d up to a point, and then it will \textbf{increase} as \textbf{d} is increased, forming a convex curve. \\
\textbf{High\ bias\ (underfitting)}\ :\ both\ $J_{train}(\Theta)$\ and\ $J_{CV}(\Theta)$\ will\ be\ high.\ Also,\ $J_{CV}(\Theta)\ \approx$\ $J_{train}(\Theta)$. \newline
\textbf{High\ variance\ (overfitting)}\ :\ $J_{train}(\Theta)$\ will\ be\ low\ and\ $J_{CV}(\Theta)$\ will\ be\ much\ greater\ than\ $J_{train}(\Theta)$.
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{bias variance.png}
\caption{graph for optimal value of 'd'.}
\label{fig:fig2}
\end{figure}
\subsection{Regularization}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{regular bias.png}
\caption{Regularization for Linear regression.}
\label{fig:fig2}
\end{figure} 
In the figure below, we see that as $\lambda$ increases, our fit becomes more rigid. On the other hand, as $\lambda$ approaches 0, we tend to over overfit the data. So how do we choose our parameter $\lambda$ to get it 'just right' ? In order to choose the model and the regularization term $\lambda$, we need to:
\newpage
\begin{enumerate}
    \item Create a list of lambdas (i.e. $\lambda$ ∈ {0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24}).
    \item Create a set of models with different degrees or any other variants.
    \item Iterate through the $\lambda$s and for each $\lambda$ go through all the models to learn some $\Theta$. 
    \item Compute the cross validation error using the learned $\Theta$ (computed with $\lambda$) on the $J_{CV}(\Theta)$ without regularization or $\lambda$ = 0.
    \item Select the best combo that produces the lowest error on the cross validation set.
    \item Using the best combo Θ and λ, apply it on $J_{test}(\Theta)$ to see if it has a good generalization of the problem.
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{regular lam.png}
\caption{Regularization parameter.}
\label{fig:fig2}
\end{figure}
\textbf{Machine Learning Diagnostics are tests that help gain insight about what would or would not work with a learning algorithm, and hence give guidance about how to improve the performance. These can take time to implement, but are still worth venturing into during the time of uncertainties.}
\subsection{Learning Curves}
Training an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence :
\begin{enumerate}
    \item As the training set gets larger, the error for a quadratic function increases.
    \item The error value will plateau out after a certain m, or training set size.
\end{enumerate}
\textbf{Experiencing high bias :} \\
\textbf{Low training set size:} causes $J_{train}(\Theta)$ to be low and $J_{CV}(\Theta)$ to be high. \\
\textbf{Large training set size:} causes both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ to be high with $J_{train}(\Theta) \approx J_{CV}(\Theta)$. \\
If a learning algorithm is suffering from \textbf{high bias}, getting more training data will not (by itself) help much.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.8\textwidth]{overfit.png}
\caption{High variance.}
\label{fig:fig2}
\end{figure}
\newline \textbf{Experiencing high variance:} \\
\textbf{Low training set size:} $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be high. \\
\textbf{Large training set size:} $J_{train}(\Theta)$ increases with training set size and $J_{CV}(\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\Theta) < J_{CV}(\Theta)$ but the difference between them remains significant. \\
If a learning algorithm is suffering from \textbf{high variance}, getting more training data is likely to help.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.8\textwidth]{underfit 1.png}
\caption{High bais.}
\label{fig:fig2}
\end{figure}
\subsection{Deciding What to Do Next Revisited}
Our decision process can be broken down as follows:
\begin{enumerate}
    \item \textbf{Getting more training examples:} Fixes high variance.
    \item \textbf{Trying smaller sets of features:} Fixes high variance.
    \item \textbf{Adding features:} Fixes high bias.
    \item \textbf{Adding polynomial features:} Fixes high bias.
    \item \textbf{Decreasing $\lambda$:} Fixes high bias.
    \item \textbf{Increasing $\lambda$:} Fixes high variance.
\end{enumerate}
\subsubsection{Diagnosing Neural Networks}
\begin{enumerate}
    \item A neural network with fewer parameters is \textbf{prone to underfitting}. It is also \textbf{computationally cheaper.}
    \item A large neural network with more parameters is \textbf{prone to overfitting.} It is also \textbf{computationally expensive.} In this case you can use regularization (increase $\lambda$) to address the overfitting.
\end{enumerate}
Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best. 
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{neural.png}
\caption{Regularization for neural networks with overfitting hypothesis.}
\label{fig:fig2}
\end{figure}
\subsubsection{Machine learning diagnostic}
\begin{definition}
\textbf{Diagnostic:} A test that you can run to gain insight what is/ isn’t working with a learning algorithm, and gain guidance as to how best to improve its performance. \\
Diagnostic can take time to implement, but doing so can be a very good use of your time.
\end{definition}
\subsubsection{Model Complexity Effects:}
\begin{enumerate}
    \item Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.
    \item Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.
    \item In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.
\end{enumerate}
\subsection{Example on Building a spam classifier}
\textbf{Spam classifier is a problem of classification under Supervised learning.} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{Screenshot (113).png}
\caption{Classifier of mails based spam/non-spam..}
\label{fig:fig2}
\end{figure}
\textbf{System Design Example:} \\
Given a data set of emails, we could construct a vector for each email. Each entry in this vector represents a word. The vector normally contains 10,000 to 50,000 entries gathered by finding the most frequently used words in our data set.  If a word is to be found in the email, we would assign its respective entry a 1, else if it is not found, that entry would be a 0. Once we have all our x vectors ready, we train our algorithm and finally, we could use it to classify if an email is a spam (or) not. \\
As we will train our algorithm by giving a large set of data, and it will classify on what terms the emails are being classified into spam and non-spam, by seeing and learning from that our algorithm classifies the given set of emails given by us into spam and non-spam. \\
\textbf{So how could you spend your time to improve the accuracy of this classifier?}
\begin{enumerate}
    \item Collect lots of data (for example \textbf{"honeypot"} project but doesn't always work).
    \item Develop sophisticated features (for example: using email header data in spam emails).
    \item Develop algorithms to process your input in different ways (recognizing misspellings in spam).
\end{enumerate}
\boxed{\textbf{It is difficult to tell which of the options will be most helpful.}}
\subsection{Error Analysis}
The recommended approach to solving machine learning problems is to:
\begin{enumerate}
    \item Start with a simple algorithm, implement it quickly, and test it early on your cross validation data.
    \item Plot learning curves to decide if more data, more features, etc. are likely to help.
    \item Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made.
\end{enumerate}
For example, assume that we have 500 emails and our algorithm misclassifies a 100 of them. We could manually analyze the 100 emails and categorize them based on what type of emails they are. We could then try to come up with new cues and features that would help us classify these 100 emails correctly. Hence, if most of our misclassified emails are those which try to steal passwords, then we could find some features that are particular to those emails and add them to our model. We could also see how classifying each word according to its root changes our error rate: \\
We do \textbf{error analysis} to know that whether our algorithm is working fine (or) not. As we take certain cases(even though we are taking large set of data, there could be a corner case that our algorithm might not encountered in the given data set) and it is must that we take 20$\%$ each of cross-validation set and test set. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{Screenshot (116).png}
\caption{Error analysis.}
\label{fig:fig2}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{imp.png}
\caption{Examples of Numerical evaluation.}
\label{fig:fig2}
\end{figure} \newpage
It is very important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm's performance. For example if we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a 3$\%$ error rate instead of 5$\%$, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a 3.2$\%$ error rate instead of 3$\%$, then we should avoid using this new feature.  Hence, we should try new things, get a numerical value for our error rate, and based on our result decide whether we want to keep the new feature or not. 
\subsection{Error metrics for skewed classes}
\subsubsection{Skewed classes}
\begin{definition}
\textbf{Skewed classes} : Skewed classes basically refer to a dataset, wherein the number of training example belonging to one class out-numbers heavily the number of training examples beloning to the other. \\
Consider a binary classification, where a cancerous patient is to be detected based on some features. And say only 1 of the data provided has cancer positive. In a setting where having cancer is labelled 1 and not cancer labelled 0, if a system naively gives the prediction as all 0’s, still the prediction accuracy will be 99$\%$. \\
Therefore, it can be said with conviction that the accuracy metrics or mean-squared error for skewed classes, is not a proper indicator of model performance. Hence, there is a need for a different error metric for skewed classes.
\end{definition}
Skewed class distributions present a challenge in many different domains. Specifically, most supervised machine learning algorithms exhibit poor performance when faced with skewed class distributions. This is referred to as the class imbalance problem. \\
Class imbalance often occurs in real-life datasets involving rare events such as detecting certain medical conditions, fraudulent transactions. \\
When the majority of data items in your dataset represents items belonging to one class, we say the dataset is skewed or imbalanced. For better understanding, lets consider a binary classification problem, cancer detection. Say we’ve five thousand instances in our dataset but only five hundred positive instances, i.e., instances were cancer was actually present. Then we’ve an imbalanced dataset. This happens more often with datasets in real life as the chances of finding cancer among all the checks that happen or a fraudulent transaction among all the transactions that occur daily is comparatively low.\\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{class.png}
\caption{Sample output of a Classification problem.}
\label{fig:fig2}
\end{figure}
\newpage\textbf{Why does it matter if the dataset is skewed?} \\
When your dataset do not represent all classes of data equally, the model might overfit to the class that’s represented more in your dataset and become oblivious to the existence of the minority class. It might even give you a good accuracy but fail miserably in real life. In our example, a model that keeps predicting that there’s no cancer every single time will also have a good accuracy as the occurrence of cancer itself will be rare among the inputs. But it will fail when an actual case of cancer is subjected to classification, failing its original purpose. \\
\newline\textbf{Different ways to deal with an imbalanced dataset} \\
A widely adopted technique for dealing with highly unbalanced datasets is called resampling. Resampling is done after the data is split into training, test and validation sets. Resampling is done only on the training set or the performance measures could get skewed. Resampling can be of two types:
\begin{enumerate}
    \item Over-sampling.
    \item Under-sampling. 
\end{enumerate} 
Under sampling involves removing samples from the majority class and over-sampling involves adding more examples from the minority class . The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{under over.png}
\caption{Undersampling and Oversampling.}
\label{fig:fig2}
\end{figure}
\textbf{Another technique similar to upsampling is to create synthetic samples. Adding synthetic samples is also only done after the train-test split, into the training data.}
\subsubsection{Precision/Recall}
\boxed{\textbf{Note: y = 1 is the rarer class among the two.}} \\
In a binary classification, one of the following four scenarios may occur,
\begin{itemize}
    \item \textbf{True Positive (TP):} the model predicts 1 and the actual class is 1.
    \item \textbf{True Negative (TN):} the model predicts 0 and the actual class is 0.
    \item \textbf{False Positive (FP):} the model predicts 1 but the actual class is 0.
    \item \textbf{False Negative (FN):} the model predicts 0 but the actual class is 1.
\end{itemize}
\subsubsection{Confusion matrix}
\textbf{Confusion\ matrix} is a table that tells us how well our model has performed after it has been trained. \\
A confusion matrix is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. \\
\begin{table}[h!]
\begin{tabular}{ll|l|l|}
\cline{3-4}
          &  &   \multicolumn{2}{l|}{\quad\;\;\; Predicted} \\ \cline{3-4} 
          &  &   Negative       & Positive                  \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Actual}} & Negative & a & b \\ \cline{2-4} 
\multicolumn{1}{|c|}{}                        & Positive & c & d \\ \hline
\end{tabular}
\end{table}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{matrix.png}
\caption{Basic binary confusion matrix.}
\label{fig:fig2}
\end{figure}
\begin{align} 
    \text{Accuracy} &= \dfrac{TP + TN}{TP + TN + FP + FN} \\
    \text{Specificity} &= \dfrac{TN}{FP + TN} \\
    \textbf{Sensitivity} &= \dfrac{TN}{TN + FP}
\end{align}
Then, precision and recall can be defined as : \\
\begin{definition}
\textbf{Precision :} Of all patients where we predicted y = 1, what fraction actually has cancer?
\end{definition}
\begin{align}
    \text{Precision} &= \dfrac{TP}{TP + FP} 
\end{align}
\begin{definition}
\textbf{Recall :} Of all patients that actually have cancer, what fraction did we correctly detect as having cancer?
\end{definition}
\begin{align}
    \text{Recall} &= \dfrac{TP}{TP + FN}
\end{align}
\textbf{Recall defines of all the actual y = 1, which ones did the model predict correctly.} \\
\textbf{Now, if we evaluate a scenario where the classifier predicts all 0’s then the recall of the model will be 0, which then points out the inability of the system.} \\
\subsection{Trading of precision and recall}
\textbf{By changing the threshold value for the classifier confidence, one can adjust the precision and recall for the model.} \\
For example, in a logistic regression the threshold is generally at 0.5. If one increases it, we can be sure that of all the predictions made more will be correct, hence, high precision. But there are also higher chances of missing the positive cases, hence, the lower recall. \\
Similarly, if one decreases the threshold, then the chances of false positives increases, hence low precision. Also, there is lesser probability of missing the actual cases, hence high recall. \\
A precision-recall trade - off curve may look like one among the following,
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{Screenshot (132).png}
\caption{Trading of precision and recall.}
\label{fig:fig2}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{Screenshot (137).png}
\caption{Precision/Recall for different values of threshold hypothesis.}
\label{fig:fig2}
\end{figure} \newpage
\subsection{F Score}
Given two pairs of precision and recall, how to choose the better pair. One of the options would be to choose the one which higher average. That is not the ideal solution as the pair with (precision=0.02 and recall=1) has a better mean than the pair (precision= 0.5 and recall=0.4).
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{Screenshot (133).png}
\caption{Calculation of F score through precision/recall.}
\label{fig:fig2}
\end{figure}
Enter F Score or $F_1$ Score, which is the harmonic mean of precision and recall, defined as,
\begin{align}
    F_1 = 2\frac{P\times R}{P+R}
\end{align}
The above formula has advantage over the average method because, if either precision or recall is small, the the numerator product P∗R will weigh the F-Score low and consequently lead to choosing the better pair of precision and recall. So, \newpage
\begin{enumerate}
    \item If P=0 (or) R=0, then $F_1$=0.
    \item If P=1 and R=1, then $F_1$=1.
\end{enumerate}
\notte{\textbf{One reasonable way of automatically choosing threshold for classifier is to try a range of them on the cross-validation set and pick the one that gives the highest F-Score.}}
\subsubsection{Sensitivity/Specifivity}
\textbf{Apart from precision and recall, sensitivity and specifivity are among the most used error metrics in classfication.}
\begin{enumerate}
    \item \textbf{Sensitivity or True Positive Rate (TPR)} is another name for recall and is also called \textbf{hit rate.}
    \item \textbf{Specificity (SPC) or True Negative Rate.}
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{Screenshot (134).png}
\caption{Designing a high accuracy learning system.}
\label{fig:fig2}
\end{figure} \newpage
\subsubsection{Using Large Datasets}
For a high bias problem in the model, gathering more and more data will not help the model improve. \\
But under certain conditions, getting a lot of data and training on a certain type of training algorithm can be an effective way to improve the learning algorithm’s performance. \\
The following are the conditions that should be met for the above statement to hold true,
\begin{enumerate}
    \item The features, x, must have sufficient information to predict y
    accurately. One way to test this would be to check if human expert can make a confident predition using the features.
    \item Using a learning algorithm with a large number of parameters to learn (e.g. logistic regression, linear regression, neural network with many hidden units etc.). What this truly accomplishes is that these algorithms are \textbf{low bias algorithm} due to the large number of learnable parameters.
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{Screenshot (135).png}
\caption{Data Rationale}
\label{fig:fig2}
\end{figure} \newpage
In such settings, where the problem of high bias is removed by the virtue of highly parametrized learning algorithms, a large dataset ensures the \textbf{low variance.} Hence, under the listed settings a large number of dataset is almost always going to help improve the model performance.
\chapter{Support Vector Machine (SVM)}
\textbf{A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they're able to categorize new text.} \\
\newline\textbf{Compared to newer algorithms like neural networks, they have too main advantages: higher speed and better performance with a limited number of samples (in the thousands). This makes the algorithm very suitable for text classification problems,where it's common to have access to a dataset of at most a couple of thousands of tagged samples.}
A SVM is a discriminative classifier formally defined by a separating hyperplane. Given labeled training data, the algorithm outputs an optimal hyperplane which categorizes new examples. \\
\subsection{How does SVM work?}
\textbf{The linear SVM classifier works by drawing a straight line between two classes. All the data points that fall on one side of the line will be labeled as one class and all the points that fall on the other side will be labeled as the second. Sounds simple enough, but there’s an infinite amount of lines to choose from. How do we know which line will do the best job of classifying the data? This is where the LSVM algorithm comes in to play. The LSVM algorithm will select a line that not only separates the two classes but stays as far away from the closest samples as possible. In fact, the “support vector” in “support vector machine” refers to two position vectors drawn from the origin to the points which dictate the decision boundary.} \\
The basics of support vector machine and how it works are best understood with a simple example. Let's imagine we have two tags: red and blue, and our data has two features: x and y. We want a classifier that, given a pair of (x,y) coordinates, outputs if it’s either red or blue. We plot our already labeled training data on a plane: \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{data.png}
\caption{Our labeled data}
\label{fig:fig2}
\end{figure}
A support vector machine takes these data points and outputs the hyperplane (which in two dimensions it’s simply a line) that best separates the tags. This line is the \textbf{decision boundary}: anything that falls to one side of it we will classify as blue, and anything that falls to the other as red. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{plot_hyperplanes_2.png}
\caption{The best hyperplane is simply a line}
\label{fig:fig2}
\end{figure}
But, what exactly is the best hyperplane? For SVM, it’s the one that maximizes the margins from both tags. In other words: the hyperplane (remember it's a line in this case) whose distance to the nearest element of each tag is the largest. \\
\textbf{According to the SVM algorithm we find the points closest to the line from both the classes.These points are called support vectors. Now, we compute the distance between the line and the support vectors. This distance is called the margin. Our goal is to maximize the margin. The hyperplane for which the margin is maximum is the optimal hyperplane.} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{imp 1.png}
\caption{Not all hyperplanes are created equal}
\label{fig:fig2}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{imp 2.png}
\caption{More information regarding SVM and hyperplane}
\label{fig:fig2}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{hyper.png}
\end{figure} \newpage
\textbf{It is a very powerful classification algorithm to maximize the margin among class variables. This margin (support vector) represents the distance between the separating hyperplanes (decision boundary). The reason to have decision boundaries with large margin is to separate positive and negative hyperplanes with adjustable bias-variance proportion. The goal is to separate so that negative samples would fall under negative hyperplane and positive samples would fall under positive hyperplane. SVM is not as prone to outliers as it only cares about the points closest to the decision boundary. It changes its decision boundary depending on the placement of the new positive or negative events.} \\
\textbf{The decision boundary is much more important for Linear SVM’s – the whole goal is to place a linear boundary in a smart way. There isn’t a probabilistic interpretation of individual classifications, at least not in the original formulation.} \\
\textbf{Hence, key points are:}
\begin{enumerate}
    \item SVM try to maximize the margin between the closest support vectors whereas logistic regression maximize the posterior class probability.
    \item SVM is deterministic (but we can use Platts model for probability score) while LR is probabilistic.
    \item For the kernel space, SVM is faster.
\end{enumerate}
\begin{tabular}{|c|c|} \hline
Logistic Regression & Support Vector Machine \\ \hline
1. It is an algorithm used for solving & 1. It is a model used for both \\
 classification problems. & classification and linear regression. \\ \hline
2. It is not used to find the best margin, & 2. It tries to find the "best" margin (distance \\
 instead, it can have different decision & between the line and the support vectors) \\
  boundaries with different weights that are & that separates the classes and thus reduces \\
   near the optimal point. & the risk of error on the data. \\ \hline
3. It works with already identified identified & 3. It works well with unstructured and semi-  \\
  independent variable. &  structured data like text and images. \\ \hline
4. It is based on statistical approach. & 4. It is based on geometrical properties \\
 &  of the data.  \\ \hline
5. It is vulnerable to overfitting. &  5. The risk of overfitting is less in SVM. \\ \hline
\end{tabular}
\newpage 
\subsection{SVM for Non-linear data}
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{small and large.png}
\caption{Difference between small and large margin.}
\label{fig:fig2}
\end{figure}
It’s pretty clear that there’s not a linear decision boundary (a single straight line that separates both tags). However, the vectors are very clearly segregated and it looks as though it should be easy to separate them.\\
This data is clearly not linearly separable. We cannot draw a straight line that can classify this data. But, this data can be converted to linearly separable data in higher dimension. Lets add one more dimension and call it z-axis. Let the co-ordinates on z-axis be governed by the constraint, \\
So here’s what we’ll do: we will add a third dimension. Up until now we had two dimensions: x and y. We create a new z dimension, and we rule that it be calculated a certain way that is convenient for us: z = $x^2 + y^2$ (you’ll notice that’s the equation for a circle). \\
This will give us a three-dimensional space. Taking a slice of that space, it looks like this: \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{z.png}
\caption{From a different perspective, the data is now in two linearly separated groups.}
\label{fig:fig2}
\end{figure}
Now, the SVM produces best hyperplane to the linearly separated data, \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{bh.png}
\caption{Best hyperplane is produced.}
\label{fig:fig2}
\end{figure} \newpage
Now the data is clearly linearly separable. Let the purple line separating the data in higher dimension be z=k, where k is a constant. Since, $z=x^2+y^2$ we get $x^2 + y^2 = k$; which is an equation of a circle. So, we can project this linear separator in higher dimension back in original dimensions using this transformation. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{non b h.png}
\caption{Decision boundary in original dimensions.}
\label{fig:fig2}
\end{figure} \\
And there we go! Our decision boundary is a circumference of radius 1, which separates both tags using SVM.\\
Thus we can classify data by adding an extra dimension to it so that it becomes linearly separable and then projecting the decision boundary back to original dimensions using mathematical transformation. But finding the correct transformation for any given dataset isn’t that easy. Thankfully, we can use kernels in sklearn’s SVM implementation to do this job.
\subsection{Hyperplane}
Now that we understand the SVM logic lets formally define the hyperplane. \\
\notte{A hyperplane in an n-dimensional Euclidean space is a flat, n-1 dimensional subset of that space that divides the space into two disconnected parts.} \\
\textbf{For example let’s assume a line to be our one dimensional Euclidean space(i.e. let’s say our datasets lie on a line). Now pick a point on the line, this point divides the line into two parts. The line has 1 dimension, while the point has 0 dimensions. So a point is a hyperplane of the line.} \\
\textbf{For two dimensions we saw that the separating line was the hyperplane. Similarly, for three dimensions a plane with two dimensions divides the 3d space into two parts and thus act as a hyperplane. Thus for a space of n dimensions we have a hyperplane of n-1 dimensions separating it into two parts.} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.35\textwidth]{p h.png}
\caption{Possible hyperplanes.}
\label{fig:fig2}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{h 3d.png}
\caption{Hyperplanes in 2D and 3D feature space.}
\label{fig:fig2}
\end{figure}
\subsection{Tuning parameters}
Parameters are arguments that you pass when you create your classifier. Following are the important parameters for SVM-
\begin{enumerate}
    \item \textbf{C($\frac{1}{\lambda}$):} \\
    It controls the trade off between smooth decision boundary and classifying training points correctly. A large value of c means you will get more training points correctly. \\
    \begin{figure}[!htb]
    \centering
    \includegraphics[width = 0.35\textwidth]{c.png}
    \caption{Smooth decision boundary vs classifying all points correctly.}
    \label{fig:fig2}
    \end{figure}  \\ 
    Consider an example as shown in the figure \ref{fig:fig2}. There are a number of decision boundaries that we can draw for this dataset. Consider a straight (green colored) decision boundary which is quite simple but it comes at the cost of a few points being misclassified. These misclassified points are called outliers. We can also make something that is considerably more wiggly(sky blue colored decision boundary) but where we get potentially all of the training points correct. Of course the trade off having something that is very intricate, very complicated like this is that chances are it is not going to generalize quite as well to our test set. So something that is simple, more straight maybe actually the better choice if you look at the accuracy. Large value of c means you will get more intricate decision curves trying to fit in all the points. Figuring out how much you want to have a smooth decision boundary vs one that gets things correct is part of artistry of machine learning. So try different values of c for your dataset to get the perfectly balanced curve and avoid over fitting.
    \item \textbf{Gamma:} \\
    It defines how far the influence of a single training example reaches. If it has a low value it means that every point has a far reach and conversely high value of gamma means that every point has close reach. \\
    If gamma has a very high value, then the decision boundary is just going to be dependent upon the points that are very close to the line which effectively results in ignoring some of the points that are very far from the decision boundary. This is because the closer points get more weight and it results in a wiggly curve as shown in previous graph.On the other hand, if the gamma value is low even the far away points get considerable weight and we get a more linear curve.
\end{enumerate}
\subsection{Algorithm}
Suppose, we had a vector w which is always normal to the hyperplane (perpendicular to the line in 2 dimensions). We can determine how far away a sample is from our decision boundary by projecting the position vector of the sample on to the vector w. As a quick refresher, the dot product of two vectors is proportional to the projection of the first vector on to the second.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.35\textwidth]{dot.png}
\caption{Projection vector.}
\label{fig:fig2}
\end{figure}  \\
If it’s a positive sample, we’re going to insist that the proceeding decision function (the dot product of w and the position vector of a given sample plus some constant) returns a value greater than or equal to 1.
\begin{align}
    \Vec{w}.\Vec{x}_+ + b \geq 1.
\end{align} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.35\textwidth]{dot plane.png}
\caption{Dot product representation for '+' samples.}
\label{fig:fig2}
\end{figure}  \\ \newpage
Similarly, if it’s a negative sample, we’re going to insist that the proceeding decision function returns a value smaller than or equal to -1.
\begin{align}
    \Vec{w}.\Vec{x}_- + b \leq -1.
\end{align} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.35\textwidth]{dot plane 1.png}
\caption{Dot product representation for '-' samples.}
\label{fig:fig2}
\end{figure}  \\
In other words, we won’t consider any samples located between the decision boundary and support vectors. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.35\textwidth]{dot plane 2.png}
\label{fig:fig2}
\end{figure} 
we introduce an additional variable stickily for convenience. The variable y will be equal to positive one for all positive samples and negative one for all negative samples. \\
\begin{center}
$y_i$\begin{cases}
     +1\ for\ +\ samples. \\
     -1\ for\ -\ samples.
\end{cases}
\end{center}
After multiplying by y, the equations for the positive and negative samples are equal to one another. \\
\begin{align*}
    y_i(\Vec{w}.\Vec{x}_+ + b) &\geq 1. \\
    y_i(\Vec{w}.\Vec{x}_- + b) &\leq 1.
\end{align*}
Meaning, we can simplify the constraints down to a single equation. \\
\begin{align*}
    y_i(\Vec{w}.\Vec{x}_i + b) - 1 = 0.
\end{align*} \\
Next, we need to address the process by which we go about maximizing the margin. To get an equation for the width of the margin, we subtract the first support vector from the one below it and the multiply the result by the unit vector of w which is always perpendicular to the decision boundary.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.35\textwidth]{width.png}
\caption{Width of the lines produced by samples.}
\label{fig:fig2}
\end{figure}
\begin{align*}
    width = (\Vec{x_+} - \Vec{x_-}) . \frac{\Vec{w}}{\| w\| }
\end{align*}
Using the constraints from above and a bit of algebra, we get the following.
\begin{align*}
    y_i(\Vec{w}.\Vec{x_i} + b) - 1 &= 0. \\
    (1)(\Vec{w}.\Vec{x}_+ + b) - 1 &= 0. \\
    \Vec{w}.\Vec{x}_+ = 1 - b \tag{1} \\
    (-1)(\Vec{w}.\Vec{x}_- + b) - 1 &= 0. \\
    \Vec{w}.\Vec{x}_- = -1 - b \tag{2} \\
    width = ((\Vec{x_+}.\Vec{w}) - (\Vec{x_-}.\Vec{w}))&\frac{1}{\| w\|} \\
    width = ((1-b) - (-1-b))&\frac{1}{\| w\|} \\
    width &= \frac{2}{\| w\|}
\end{align*} \\
Therefore, in order to select the optimal decision boundary, we must maximize the equation we just computed. We apply a few more tricks before proceeding.
(refer to the $\underline{\href{https://www.youtube.com/watch?v=_PwhiWxHK8o}{MIT Lecture}}$). \\
\begin{center}
    \begin{align*}
        \max&\frac{2}{\| w\|} \\
        \textbf{\text{which is &proportional to:}} \\
        \max&\frac{1}{\| w\|} \\
        \textbf{\text{which is &equivalent to:}} \\
        \min&\| w\| \\
        \textbf{\text{we're going to cheat a little and use:}} \\
        \min\frac{1}{2}{\| w\|}^2\;\;\;\;\;\; &\textbf{\text{since}} \;\;\;\;\;\; \diff{1}{x}\frac{1}{2}{x}^2 = x
    \end{align*}
\end{center} \\
Now, in most machine learning algorithms, we’d use something like gradient descent to minimize said function, however, for support vector machines, we use the Lagrangian.In essence, using Lagrangian, we can solve for the global minimum like we’d do in high school level calculus (i.e. take the derivative of the function and make it equal to zero). The Lagrange tells us to subtract the cost function by the summation over all the constraints where each of those constraints will be multiplied by some constant alpha (normally written as lambda for the Lagrangian). \\
\begin{align*}
    L = \frac{1}{2}{\| w\|}^2 - &\sum_{i}^{n} \alpha_i [y_i(\Vec{w}.\Vec{x} + b) - 1] \\
    \frac{\partial L}{\partial w} = \Vec{w} - &\sum_{i}^{n} \alpha_i y_i x_i = 0\\
    \boxed{\Vec{w} = \sum_{i}^{n} \alpha_i y_i x_i} \\
    \frac{\partial L}{\partial b} = &-\sum_{i}^{n} \alpha_i y_i = 0 \\
    \boxed{\sum_{i}^{n} \alpha_i y_i = 0}
\end{align*}
Then, we perform some more algebra, plugging the equations we found in the previous step back into the original equation.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{equations.png}
\label{fig:fig2}
\end{figure} \\
Before we can proceed any further, we need to express the equation in terms of matrices instead of summations. The reason being, the \boxed{qp} function from the \underline{\href{https://cvxopt.org/userguide/coneprog.html#quadratic-programming}{CVXOPT}} library, which we’ll use to solve the Lagrangian, accepts very specific arguments. Thus, we need to go from: \\
\textbf{As this is still beyond your capability. Let us stop it here. But, I will provide link for futher understandings.} \\
\begin{center}
    \underline{\href{https://towardsdatascience.com/support-vector-machine-python-example-d67d9b63f1c8}{Machine Learning with python.}}
\end{center}
\subsection{The Kernel}
In our example we found a way to classify nonlinear data by cleverly mapping our space to a higher dimension. However, it turns out that calculating this transformation can get pretty computationally expensive: there can be a lot of new dimensions, each one of them possibly involving a complicated calculation. Doing this for every vector in the dataset can be a lot of work, so it’d be great if we could find a cheaper solution. \\
And we’re in luck! Here’s a trick: SVM doesn’t need the actual vectors to work its magic, it actually can get by only with the dot products between them. This means that we can sidestep the expensive calculations of the new dimensions.\\
This is what we do instead:
\begin{enumerate}
    \item Imagine the new space we want: \\
    $z = x^2 + y^2.$
    \item Figure out what the dot product in that space looks like: \\
    a·b = xa·xb + ya·yb + za·zb \\
    a·b = xa·xb + ya·yb + (xa² + ya²)·(xb² + yb²)
    \item Tell SVM to do its thing, but using the new dot product — we call this a \textbf{kernel function.}
\end{enumerate}
That’s it! That’s the kernel trick, which allows us to sidestep a lot of expensive calculations. Normally, the kernel is linear, and we get a linear classifier. However, by using a nonlinear kernel (like above) we can get a nonlinear classifier without transforming the data at all: we only change the dot product to that of the space that we want and SVM will happily chug along.\\
Note that the kernel trick isn’t actually part of SVM. It can be used with other linear classifiers such as logistic regression. A support vector machine only takes care of finding the decision boundary. 
\subsection{Differences between SVM and Logistic Regression}
\begin{definition}
\textbf{Logistic Regression:} Logistic regression is an algorithm that is used in solving classification problems. It is a predictive analysis that describes data and explains the relationship between variables. Logistic regression is applied to an input variable (X) where the output variable (y) is a discrete value which ranges between 1 (yes) and 0 (no).
\end{definition} \\
\begin{definition}
\textbf{Support Vector Machine:} The support vector machine is a model used for both classification and regression problems though it is mostly used to solve classification problems. The algorithm creates a hyperplane or line(decision boundary) which separates data into classes. It uses the kernel trick to find the best line separator (decision boundary that has same distance from the boundary point of both classes). It is a clear and more powerful way of learning complex non linear functions.
\end{definition}\\ %\newpage
%\begin{table}
%\centering 
\newline\begin{tabular}{|c|c|} \hline
Problems that can solved & Problems that can solved using SVM \\
    using Logistic Regression  & \\ \hline
1.Cancer detection — can be used to detect  & 1.Image classification.        \\
if a patient have cancer (1) or not(0). &  \\ \hline
2.Test score — predict if a student passed(1)  & 2. Recognizing handwriting.    \\ or failed(0) a test. &      \\  \hline
3.Marketing — predict if a customer will           & 3. Cancer Detection \\ purchase a product(1) or not(0). &  \\ \hline
\end{tabular}
%\end{table}
\newline\newline\textbf{\underline{Differences}}
\begin{enumerate}
    \item SVM tries to finds the “best” margin (distance between the line and the support vectors) that separates the classes and this reduces the risk of error on the data, while logistic regression does not, instead it can have different decision boundaries with different weights that are near the optimal point. 
    \begin{figure}[!htb]
    \centering
    \includegraphics[width = 0.5\textwidth]{diff.png}
    \label{fig:fig2}
     \end{figure}
     \item SVM works well with unstructured and semi-structured data like text and images while logistic regression works with already identified independent variables.
     \item SVM is based on geometrical properties of the data while logistic regression is based on statistical approaches.
     \item The risk of overfitting is less in SVM, while Logistic regression is vulnerable to overfitting.
\end{enumerate}
\textbf{\underline{When To Use Logistic Regression vs Support Vector Machine}}\\ \newline
Depending on the number of training sets (data)/features that you have, you can choose to use either logistic regression or support vector machine. \\
Lets take these as an example where : \\
n = number of features, \\
m = number of training examples. \\
\begin{enumerate}
    \item  If n is large (1–10,000) and m is small (10–1000) : use logistic regression or SVM with a linear kernel.
    \item  If n is small (1–10 00) and m is intermediate (10–10,000) : use SVM with (Gaussian, polynomial etc) kernel.
    \item If n is small (1–10 00), m is large (50,000–1,000,000+): first, manually add more features and then use logistic regression or SVM with a linear kernel.
\end{enumerate}
Generally, it is usually advisable to first try to use logistic regression to see how the model does, if it fails then you can try using SVM without a kernel (is otherwise known as SVM with a linear kernel). Logistic regression and SVM with a linear kernel have similar performance but depending on your features, one may be more efficient than the other. \\
\notte{Logistic regression and SVM are great tools for training classification and regression problems. It is good to know when to use either of them so as to save computational cost and time.}
\notte{\textbf{The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Also, the dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceeds 3.Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.}}
\subsection{Optimization Objective}
The support vector machine objective can seen as a modification to the cost of logistic regression. Consider the sigmoid function, given as,
\begin{align}
    h_\theta(x) = \frac{1}{1 + e^{-z}}
\end{align} \\
where $z = \theta^T x.$ \\
The cost function of logistic regression as in the post Logistic Regression Model, is given by,
\begin{align}
J(\theta) &= -{1 \over m} \sum_{i=1}^m \left( y^{(i)}\,log(h_\theta(x^{(i)}) + (1-y^{(i)})\,log(1 - h_\theta(x^{(i)})) \right) \\
    &= -{1 \over m} \sum_{i=1}^m \left( y^{(i)}\,log(\frac {1} {1 + e^{-\theta^T x}}) + (1-y^{(i)})\,log(1 - \frac {1} {1 + e^{-\theta^T x}}) \right) \label{2}
\end{align}
Each training instance contributes to the cost function the following term,
\begin{align*}
    -y\,log(\frac {1} {1 + e^{-z}}) - (1-y)\,log(1 - \frac {1} {1 + e^{-z}})
\end{align*}
So when y=1, the contributed term is $-log(\frac {1} {1 + e^{-z}})$, which can be seen in the plot below. The cost function of SVM, denoted as $cost_1(z)$, is a modification the former and a close approximation. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{SVM at y=1.png}
\caption{SVM Cost function at y = 1.}
\label{fig:fig2}
\end{figure} \newpage
Similarly, when y=0, the contributed term is $-log(1 - \frac {1} {1 + e^{-z}})$, which can be seen in the plot below. The cost function of SVM, denoted as $cost_0(z)$, is a modification the former and a close approximation.\\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{SVM at y=0.png}
\caption{SVM Cost function at y = 0.}
\label{fig:fig2}
\end{figure}
\notte{While the slope the straight line is not of as much importance, it is the linear approximation that gives SVMs computational advantages that helps in formulating an easier optimization problem.}
Regularized version of eq:\ref{2} can from the post Regularized Logistic Regression can rewritten as, \\
\begin{align}
    J(\theta) = {1 \over m} \sum_{i=1}^m \left( y^{(i)}\,(-log(h_\theta(x^{(i)}))) + (1-y^{(i)})\,(-log(1 - h_\theta(x^{(i)}))) \right) + {\lambda \over 2m } \sum_{j=1}^n \theta_j^2  \label{3}
\end{align}
In order to come up with the cost function for the SVM, eq:\ref{3} is modified by replacing the corresponding cost terms, which gives,
\begin{align}
    J(\theta) = {1 \over m} \sum_{i=1}^m \left( y^{(i)}\,cost_1(z) + (1-y^{(i)})\,cost_0(z) \right) + {\lambda \over 2m } \sum_{j=1}^n \theta_j^2 \label{4}
\end{align}
Following the conventions of SVM the following modifications are made to the cost in eq:\ref{4}, which effectively is a change in notation but not the underlying logic,
\begin{enumerate}
    \item Removing $\frac{1}{m}$ does not affect the minimization logic at all as the minima of a function is not changed by the linear scaling.
    \item Change the form of parameterization from $A+\lambdaB$ to CA+B where it can be intuitively thought that $C=\frac{1}{\lambda}.$
\end{enumerate}
After applying the above changes, eq:\ref{4} gives,
\begin{align}
    J(\theta) = C \sum_{i=1}^m \left[ y^{(i)}\,cost_1(\theta^T x^{(i)}) + (1-y^{(i)})\,cost_0(\theta^T x^{(i)}) \right] + {1 \over 2 } \sum_{j=1}^n \theta_j^2. \label{5}
\end{align}
The SVM hypothesis does not predict probability, instead gives hard class labels,
\begin{align*}
    h_\theta(x) = 
\begin{cases}
1 \text{, if } \theta^Tx \geq 0 \\
0 \text{, otherwise}
\end{cases}
\end{align*}
\subsection{Large Margin Intuition}
In logistic regression, we take the output of the linear function and squash the value within the range of [0,1] using the sigmoid function. If the squashed value is greater than a threshold value(0.5) we assign it a label 1, else we assign it a label 0. In SVM, we take the output of the linear function and if that output is greater than 1, we identify it with one class and if the output is -1, we identify is with another class. Since the threshold values are changed to 1 and -1 in SVM, we obtain this reinforcement range of values([-1,1]) which acts as margin.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{large.png}
\caption{SVM Cost function plots.}
\label{fig:fig3}
\end{figure} \newpage
According to \ref{5} and the plots of the cost function as shown in the image above, the following are two desirable states for SVM,
\begin{enumerate}
    \item If y=1, then $\Theta^{\top}x \geq1$ (not just $\geq0$).
    \item If y=0, then $\Theta^{\top}x \geq-1$ (not just <0).
\end{enumerate}
Let C in eq:\ref{5} be a large value. Consequently, in order to minimize the cost, the corresponding term $\sum_{i=1}^m \left[ y^{(i)}\,cost_1(\theta^T x^{(i)}) + (1-y^{(i)})\,cost_0(\theta^T x^{(i)}) \right]$ must be close to 0. \\
Hence, in order to minimize the cost function, when y=1, $cost_1(\theta^T x)$ should be 0, and similarly, when y=0, $cost_0(\theta^T x)$ should be 0. And thus, from the plots in fig:\ref{fig:fig3}, it is clear that it can only fulfilled by the two states listed above.\\
Following the above intuition, the cost function can we written as,
\begin{align}
    min_\theta J(\theta) = min_\theta {1 \over 2 } \sum_{j=1}^n \theta_j^2 \label{6}
\end{align}
subject to contraints,
\begin{align*}
\theta^{\top}x^{(i)} &\geq 1 \text{, if } y^{(i)}=1 \\
\theta^{\top}x^{(i)} &\leq -1 \text{, if } y^{(i)}=0
\end{align*}
What this basically leads to is the selection of a decision boundary that tries to maximize the margin from the support vectors as shown in the plot below. This maximization of the margin as seen for decision boundary A increases the robustness over decision boundaries with lesser margins like B. And it is this property of the SVMs that attributes the name \textbf{large margin classifier} to it.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{large-margin-decision-boundary.png}
\caption{Large Margin Decision Boundary.}
\label{fig:fig3}
\end{figure} \newpage
\textbf{Effect of Parameter C}
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{effect-of-regularization.png}
\caption{Effect of Parameter C.}
\label{fig:fig3}
\end{figure} \\
The effect of C can be considered as reciprocal of regularization parameter, 
$\lambda$. This is more clear from Fig-5. A single outlier, can make the model choose the decision boundary with smaller margin if the value of C is large. A small value of C ensures that the outliers are overlooked and best approximation of large margin boundary is determined.
\subsection{Mathematical Background for SVM}
Vector Inner Product: Consider two vectors, v and w, given by, \\
\begin{center}
\begin{math}
v = \begin{bmatrix}v_1 \\ v_2 \end{bmatrix} \\
w = \begin{bmatrix}w_1 \\ w_2 \end{bmatrix}
\end{math} 
\end{center}\\
Then, the inner product or the dot product is defined as $v^{\top}w = w^{\top}v.$ \\
Norm of a vector, v, denoted as $\| v\|$ is the euclidean length of the vector given by the pythagoras theorem as,
\begin{align}
    \| v\| = \sqrt{\sum_{i=0}^n v_{i}^2}\; \epsilon\; \Re \label{7}
\end{align}
The inner product can also be defined as,
\begin{align}
\text{Inner Product(v, w)} &= v^{\top}w = w^{\top}v = \sum_{i=0}^n v_i .w_i \\
    &= \| v\|. \| w\|cos\theta = p.\| v\| \label{8}
\end{align}
where $\| w\|.cos(\theta)$ can be described as the projection of vector w onto vector v which can be either positive or negative signed based on the angle $\theta$ between the vectors as shown in the image below. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{dot-product.png}
\caption{Dot Product.}
\label{fig:fig3}
\end{figure}
\textbf{SVM Decision Boundary:} From \ref{6}, the optimization statement can be written as,
\begin{align}
    min_\theta \, {1 \over 2 } \sum_{j=1}^n \theta_j^2 \label{9}
\end{align}
subject to contraints,
\begin{align}
\theta^{\top}x^{(i)} &\geq 1 \text{, if } y^{(i)}=1 \\
\theta^{\top}x^{(i)} &\leq -1 \text{, if } y^{(i)}=0 \label{10}
\end{align}
Let $\theta_0$=0 and n=2, i.e. number of features is 2 for simplicity, then \ref{9} can be written as,
\begin{align}
    min_\theta \, \frac{1}{2} (\theta_1^2 + \theta_1^2) = {1 \over 2 } \sqrt{(\theta_1^2 + \theta_1^2)}^2 =  \frac{1}{2} \| \theta \|^2 \label{11}
\end{align}
Using \ref{8}, $\theta^{\top}x^{(i)}$ in \ref{10} can be written as,
\begin{align}
    \theta^{\top}x^{(i)} = p^{(i)}. \| \theta \| \label{12}
\end{align}
The plot of \ref{12} can be seen below, \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{dot-product-in-svm.png}
\caption{Dot Product in SVM.}
\label{fig:fig3}
\end{figure} \newpage
Hence, using \ref{11} and \ref{12}, the optimization objective in \ref{9} and the constraints in \ref{10} are written as,
\begin{align}
    min_\theta \, \frac{1}{2} \| \theta\|^2 \label{13}
\end{align}
subject to contraints,
\begin{align}
p^{(i)}. \| \theta\| &\geq 1 \text{, if } y^{(i)}=1 \\
p^{(i)}. \| \theta\| &\leq -1 \text{, if } y^{(i)}=0 \label{14}
\end{align}
where $p^{(i)}$ is the projection of $x^{(i)}$ onto vector $\theta$.\\
Consider two decision boundaries, A and B, and their respective perpendicular parameters, $\theta_A$ and $\theta_B$ as shown in the plot below. As a consequence of choosing $\theta_0=0$ for simplification, all the corresponding decision boundaries pass through the origin.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{choosing-large-margin.png}
\caption{ Choosing Large Margin Classifier.}
\label{fig:fig3}
\end{figure} \\
Based on the two training examples of either class chosen, close to the boundaries, it can be seen that the magnitude of projection is more in case of $\theta_B$ than $\theta_A$. This basically tells that it would be possible to choose smaller values of $\theta$ and satisfy \ref{13} and \ref{14} if the value of projection p is bigger and hence, the decision boundary, B is more favourable to the optimization objective. \\
\textbf{Why is decision boundary perpendicular to the $\theta$?} \\
Consider two points $x_1$ and $x_2$ on the decision boundary given by,
\begin{align}
    \theta\,x + c= 0 \label{15}
\end{align}
Since the two points are on the line, they must satisfy \ref{15}. Substitution leads to the following,
\begin{align}
    \theta\,x_1 + c= 0 \label{16}
    \theta\,x_2 + c= 0 \label{17}
\end{align}
Subtracting \ref{17} from \ref{16},
\begin{align}
    \theta\,(x_1 - x_2) = 0 \label{18}
\end{align}
Since $x_1$ and $x_2$ lie on the line, the vector $(x_1 - x_2)$ is on the line too. Following the property of orthogonal vectors, 1\ref{18} is possible only if $\theta$ is orthogonal or perpendicular to $(x_1 - x_2)$, and hence perpendicular to the decision boundary.
\subsection{Kernels in SVM}
When dealing with non-linear decision boundaries, a learning method like logistic regression relies on high order polynomial features to find a complex decision boundary and fit the dataset, i.e. predict y=1 if,
\begin{align}
    \theta_0\,f_0 + \theta_1\,f_1 + \theta_2\,f_2 + \theta_3\,f_3 + \cdots \geq 0 \label{19}
\end{align}
where $f_0 = x_0,\, f_1=x_1,\, f_2=x_2,\, f_3=x_1x_2,\, f_4=x_1^2,\, \cdots.$\\
A natural question that arises is if there are choices of better/different features than in \ref{19}? A SVM does this by picking points in the space called \textbf{landmarks} and defining functions called \textbf{similarity} corresponding to the landmarks.
\begin{figure}[!htb]
\centering 
\includegraphics[width = 0.5\textwidth]{svm-landmarks.png}
\caption{SVM Landmarks.}
\label{fig:fig3}
\end{figure}\\ \newpage
\textbf{How many landmarks do we need?}\\
It might surprise you that given m training samples, the location of landmarks is exactly the location of your m training samples.
\begin{figure}[!htb]
\centering 
\includegraphics[width = 1.0\textwidth]{l-m.png}
\caption{Number of Landmarks.}
\label{fig:fig3}
\end{figure}\\
That is saying Non-Linear SVM recreates the features by comparing each of your training sample with all other training samples. Thus the number of features for prediction created by landmarks is the the size of training samples. For a given sample, we have updated features as below:
\begin{figure}[!htb]
\centering 
\includegraphics[width = 0.8\textwidth]{f-i.png}
\caption{f and k relation.}
\label{fig:fig3}
\end{figure}\\ \newpage
Say, there are three landmarks defined, $l^{(1)}, l^{(2)} and\ l^{(3)}$ as shown in the plot above, the for any given x, $f_1, f_2 and\ f_3$ are defined as follows,
\begin{align}
f_1 &= similarity(x, l^{(1)}) = exp \left(- \frac {\| x - l^{(1)} \|^2} {2 \sigma^2} \right) \\
f_2 &= similarity(x, l^{(2)}) = exp \left(- \frac {\| x - l^{(2)} \|^2} {2 \sigma^2} \right) \\ \label{20}
f_3 &= similarity(x, l^{(3)}) = exp \left(- \frac {\| x - l^{(3)} \|^2} {2 \sigma^2} \right) \\
    & \vdots 
\end{align}
Here, the similarity function is mathematically termed a \textbf{kernel.} The specific kernel used in \ref{20} is called the \textbf{Guassian Kernel}. Kernels are sometimes also denoted as $k(x, l^{(i)})$\\
Consider $f_1$ from \ref{20}. If there exists x close to landmark $l^{(1)}$, then $\|x-l^{(1)}\| \approx 0$ and hence, $f_1 \approx 1$. Similarly for a x far from the landmark, $\|x-l^{(1)}\|$ will be a larger value and hence exponential fall will cause $f_1 \approx 0$. So effectively the choice of landmarks has helped in increasing the number of features x had from 2 to 3. which can be helpful in discrimination.\\
For a gaussian kernel, the value of $\sigma$ defines the spread of the normal distribution. If $\sigma$ is small, the spread will be narrower and when its large the spread will be wider.\\
Also, the intuition is clear about how landmarks help in generating the new features. Along with the values of parameter, $\theta$ and $\sigma$, various different decision boundaries can be achieved.\\
\textbf{How to choose optimal landmarks?}\\
In a complex machine learning problem it would be advantageous to choose a lot more landmarks. This is generally acheived by choosing landmarks at the point of the training examples, i.e. landmarks equal to the number of training examples are chosen, ending up in $l^{(1)}, l^{(2)}, \cdots, l^{(m)}$ if there are m training examples. This translates to the fact that each feature is a measure of how close is an instance to the existing points of the class, leading to generation of new feature vectors. \\
\notte{\textbf{For SVM training, given training examples, x, features f are computed, and y =1, if $\theta^{\top} f \geq 0.$}}
The training objective from \ref{5} is modified as follows,
\begin{align}
    min_\theta \, C \sum_{i=1}^m \left[ y^{(i)}\,cost_1(\theta^T f^{(i)}) + (1-y^{(i)})\,cost_0(\theta^T f^{(i)}) \right] + {1 \over 2 } \sum_{j=1}^m \theta_j^2 \label{21}
\end{align}
In this case, n=m in \ref{5} by the virtue of procedure used to choose f.\\
\notte{\textbf{The regularization term in \ref{21} can be written as $\theta^{\top} \theta$. But in practice most SVM libraries, instead $\theta^{\top} M\theta$
, which can be considered a scaled version is used as it gives certain optimization benefits and scaling to bigger training sets, which will be taken up at a later point in maybe another post.}}
While the kernels idea can be applied to other algorithms like logistic regression, the computational tricks that apply to SVMs do not generalize as well to other algorithms.
\notte{\textbf{Hence, SVMs and Kernels tend to go particularly well together.}}
\subsection{Bias/Variance}
\begin{enumerate}
    \item $c=\frac{1}{\lambda}$,
    \begin{itemize}
        \item \textbf{Large C:} Low bias, High Variance.
        \item \textbf{Small C:} High bias, Low Variance.
    \end{itemize}
    \item $\sigma$,
    \begin{itemize}
        \item \textbf{Large $\sigma^2$:} High Bias, Low Variance (Features vary more smoothly).
        \item \textbf{Small $\sigma^2$:} Low Bias, High Variance (Features vary less smoothly).
    \end{itemize}
\end{enumerate}
\subsection{Choice of Kernels}
\begin{enumerate}
    \item \textbf{Linear Kernel:} is equivalent to a no kernel setting giving a standard linear classifier given by,
    \begin{align}
        \theta_0\,x_0 + \theta_1\,x_1 + \theta_2\,x_2 + \theta_3\,x_3 + \cdots \geq 0 \label{22}
    \end{align}
    Linear kernels are used when the number of training data is less but the number of features in the training data is huge.
    \item Gaussian Kernel: Make a choice of $\sigma^2$ to adjust the bias/variance trade-off. \\
    Gaussian kernels are generally used when the number of training data is huge and the number of features are small.
    \notte{\textbf{Feature scaling is important when using SVM, especially Gaussian Kernels, because if the ranges vary a lot then the similarity feature would be dominated by features with higher range of values.}}
    \notte{\textbf{All the kernels used for SVM, must satisfy Mercer’s Theorem, to make sure that SVM optimizations do not diverge.}}\\
    Some other kernels known to be used with SVMs are:
    \begin{itemize}
        \item Polynomial kernels, $k(x,l)=(x^{\top}l+constant)^{degree}.$ \item Esoteric kernels, like string kernel, chi-square kernel, histogram intersection kernel, ...
    \end{itemize}
\end{enumerate}
\subsection{Multi-Class Classification}
\begin{enumerate}
    \item Most SVM libraries have multi-class classification.
    \item Alternatively, one may use one-vs-all technique to train k different SVMs and pick class with largest $\theta^{\top}x.$
\end{enumerate}
\textbf{Logistic Regression vs SVM}
\begin{enumerate}
    \item If n is large relative to m, use logistic regression or SVM with linear kernel, like if n = 10000, m = 10-1000.
    \item If n is small and m is intermediate, use SVM with gaussian kernel, like if n = 1-1000, m = 10-10000.
    \item If n is small and m is large, create/add more features, then use logistic regression or SVM with no kernel, as with huge datasets SVMs struggle with gaussian kernels, like if n = 1-1000, m = 50000+.
\end{enumerate}
\notte{\textbf{Logistic Regression and SVM without a kernel (with linear kernel) generally give very similar. A neural network would work well on these training data too, but would be slower to train.}}
Also, the optimization problem of SVM is a convex problem, so the issue of getting stuck in local minima is non-existent for SVMs.
\subsection{Cost function for SVM}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.4\textwidth]{h-t.png}
\caption{Hypothesis and Regularized Cost Function.}
\label{fig:fig3}
\end{figure}\\
\subsection{What are the advantages of Artificial Neural Networks over Support Vector Machines?}
Both Support Vector Machines (SVMs) and Artificial Neural Networks (ANNs) are supervised machine learning classifiers. An ANN is a parametric classifier that uses hyper-parameters tuning during the training phase. An SVM is a non-parametric classifier that finds a linear vector (if a linear kernel is used) to separate classes. Actually, in terms of the model performance, SVMs are sometimes equivalent to a shallow neural network architecture. Generally, an ANN will outperform an SVM when there is a large number of training instances, however, neither outperforms the other over the full range of problems.\\
We can summarize the advantages of the ANN over the SVM as follows:
ANNs can handle multi-class problems by producing probabilities for each class. In contrast, SVMs handle these problems using independent one-versus-all classifiers where each produces a single binary output. For example, a single ANN can be trained to solve the hand-written digits problem while 10 SVMs (one for each digit) are required.\\
Another advantage of ANNs, from the perspective of model size, is that the model is fixed in terms of its inputs nodes, hidden layers, and output nodes; in an SVM, however, the number of support vector lines could reach the number of instances in the worst case.\\
The SVM does not perform well when the number of features is greater than the number of samples. More work in feature engineering is required for an SVM than that needed for a multi-layer Neural Network.\\
On the other hand, SVMs are better than ANNs in certain respects:\\
In comparison to SVMs, ANNs are more prone to becoming trapped in local minima, meaning that they sometime miss the global picture.\\
While most machine learning algorithms can overfit if they don’t have enough training samples, ANNs can also overfit if training goes on for too long - a problem that SVMs do not have.\\
SVM models are easier to understand. There are different kernels that provide a different level of flexibilities beyond the classical linear kernel, such as the Radial Basis Function kernel (RBF). Unlike the linear kernel, the RBF can handle the case when the relation between class labels and attributes is nonlinear.\\
For a review of the practical results, the following link to benchmarks of the performance of SVMs and ANNs on a variety datasets is useful:
\begin{center}
    \underline{\href{https://web.archive.org/web/20120304030602/http://indiji.com/svm-vs-nn.html}{Support Vector Machine vs Artificial Neural Networks.}}
\end{center}
\end{document}