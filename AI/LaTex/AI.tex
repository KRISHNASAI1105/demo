\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{mathtools}  
\usepackage{diffcoeff}
\usepackage{pgfplots}
\usepackage{comment}
\usepackage{hyperref}
% \usepackage{babel}
 \usepackage[useregional]{datetime2}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage{subcaption}
\usepackage[export]{adjustbox}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage{blindtext}
\usepackage[most]{tcolorbox}
\usepackage{lipsum}
\usepackage{array}
\usepackage{booktabs}
\usepackage[svgnames, table]{xcolor}
\usepackage{tabularx, makecell, linegoal}
\usepackage{diagbox}
\usepackage{slashbox}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{animate}
\usepackage{animate,media9,movie15}
\newcommand{\RomanNumeralCaps}[1]
    {\MakeUppercase{\romannumeral #1}}

\newlength{\Lnote}
\newcommand{\notte}[1]
     {\addtolength{\leftmargini}{4em}
        \settowidth{\Lnote}{\textbf{Note:~}}
        \begin{quote}
            \rule{\dimexpr\textwidth-2\leftmargini}{1pt}\\
                        \mbox{}\hspace{-\Lnote}\textbf{Note:~}%
                                            #1\\[-0.5ex] 
            \rule{\dimexpr\textwidth-2\leftmargini}{1pt}
        \end{quote}
        \addtolength{\leftmargini}{-4em}}

\newcommand{\mynote}[1]{\medskip\par\textbf{\small Note}\quad\setlength{\extrarowheight}{2pt}\begin{tabularx}{\linegoal}{X}
\Xhline{1pt}
\Xhline{1pt}
\end{tabularx}}

\title{\textbf{Machine Learning}}
\author{\textbf{Nagubandi Krishna Sai} \\ \textbf{MS20BTECH11014}}

\graphicspath{{images/}}

\begin{document}

\maketitle

% \section{Contents}

\tableofcontents{}

\chapter{Fundamentals of Machine Learning}
\section{Supervised Learning}
Supervised Learning gives "correct answers", the output values are same as real life values. \\
In Supervised Learning, we are given a set of data and we know what our correct output should look like, having an idea that there is relationship between the input and the output. \\
Supervised Learning problems has two types of problems, 
\begin{enumerate}
    \item Regression. 
    \item Classification. 
\end{enumerate} 

\subsection{Linear Regression}
In regression type of problems, we are trying to predict results within a continuous output, means that we are trying to map input variables to some continuous function. \\
Linear regression has real-valued output, but the output will be same or near valued to the actual output. 
\begin{enumerate}
    \item m = Number of training examples. 
    \item x's = "input" variable (or) feature.
    \item y's = "output (or) target" variable.
    \item (x,y) = one training example.
    \item ($x^{(i)}$,$y^{(i)}$) = $i^{th}$ training example.
\end{enumerate} 
Example : \newline
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} \hline
        Size of feet^2 (x) & Price(\$) in 1000's (y)  \\ \hline
        2104 & 460  \\ \hline
        1416 & 232  \\ \hline
        1534 & 315  \\ \hline
        852 & 178  \\ \hline
        \vdots & \vdots  \\ \hline
    \end{tabular}
    \caption{Training set of housing prices.}
    \label{tab:my_label}
\end{table}
\begin{enumerate}
    \item m = 47.
    \item $x^{(1)}$ = 2104 and $y^{(1)}$ = 460. 
    \item $x^{(2)}$ = 1416 and $y^{(2)}$ = 232.
\end{enumerate}
\begin{center}
\begin{tikzpicture}
\node[draw] at (2.5,2.5) {Training set};
\node[draw] at (2.5,0.5) {Learning algorithm};
\node[draw] at (2.5,-2.5) {h};
\node (A) at (2.5,2.3) {};
\node (B) at (2.5,0.7) {};
\draw[->, to path={-\ (\tikztotarget)}]
  (A) edge (B) ;
\node (A) at (2.5,0.3) {};
\node (B) at (2.5,-2.3) {};
\draw[->, to path={-\ (\tikztotarget)}]
  (A) edge (B) ;
\node[draw] at (6.5,1.5) {Training set is feeded to learning algorithm};
\node[draw] at (9,-2.5) {(hypothesis, This is a function that takes input x and estimate value of y.)}
\end{tikzpicture}
\end{center} \\
\subsection{Hypothesis for Linear Regression}
\begin{equation}
    h_\theta(x) = \theta_0 + \theta_1.x 
\end{equation} 
\begin{center}
    $\theta_i$'s = parameters.
\end{center} \\
This type of hypothesis mode is called "Linear regression with one variable" (or) "Univariate linear regression."  \\
\subsection{Cost function for Linear Regression}
Cost function helps us know that how well to fit the best possible straight line over the given data. \\ \newline
Q\rangle\; How\ to\ choose\ $\theta_i$'s\ ? 
\begin{enumerate}
    \item Choose\ $\theta_i$ 's so that $h_\theta(x)$ is close to y for our training example (x,y). 
    \item minimise $\theta_0$,$\theta_1$ so, that [$h_\theta(x)$ - y] is small.
\end{enumerate}
\begin{equation}
    J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^{m}[h_\theta(x^{(i)}) - y^{(i)}]^2
\end{equation}
\begin{center}
    J($\theta_0$,$\theta_1$) = Cost function (or) Squared error function.
\end{center} \\
\subsection{Gradient descent for Linear Regression}
Gradient descent is used to minimise cost function(J) in linear regression. \\
Gradient descent is used in many areas to minimise many functions in ML/AI. \\
\textbf{Gradient descent algorithm,} \\
\begin{equation}
    \text{Repeat\ until\ convergence\ (minimum)} \Bigg\{ \theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1),\ for\ j=0, j=1.
\end{equation}
\begin{enumerate}
    \item := is Assignment operator.
    \item $\alpha$ is learning rate.
    \item $\frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1)$ is derivative. 
\end{enumerate} \\
Gradient descent is nothing but the derivative of the Cost function. \\
\begin{equation}
    Slope\ of\ cost\ function\ curve = \frac{\partial J(\theta_1)}{\partial \theta_1},\ when\ \theta_0 = 0.
\end{equation} \\
\textbf{Learning rate,} \\
\begin{enumerate}
    \item If $\alpha$ is too small, gradient descent can be slow. After many such operations(can be infinite times), the '$\theta_1$' could reach "global minimum".
    \item If $\alpha$ is too large, gradient descent can overshoot the minimum. It may "fail to converge (or) even diverge". 
    \item If $\theta_1$ is at the local optima itself when we started or taken $\theta_1$, then there is no use of "$\alpha$ (or) gradient descent". 
\end{enumerate}
\subsection{Linear Regression for multivariables}
\textbf{Hypothesis,} \\
\begin{equation}
    h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n.
\end{equation} \\
In the total context of Supervised learning, hypothesis is just predicting the output. \\
For convenience of notation, declare x_0 = 1\ (x^{(i)}_0 = 1). \\
\begin{math} 
x = \left[\begin{array}{c}
     x_0 \\
     x_1 \\
     x_2 \\
     \vdots \\
     x_n
\end{array}\right] \epsilon\ \Re^{n+1}  \;\;\;\;\;\;\;\;\;  
\theta = \left[\begin{array}{c}
     \theta_0 \\
     \theta_1 \\
     \theta_2 \\
     \vdots \\
     \theta_n
\end{array}\right] \epsilon\ \Re^{n+1}
\end{math} \\
The above matrix is 0 - indexed. \\
\begin{equation}
    h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n. \\
    h_\theta(x) = \theta^{\top} x.
\end{equation} \\
\textbf{Cost function,} \\
\begin{equation}
    J(\theta) = J(\theta_0,\theta_1,\theta_2,...,\theta_n) = \frac{1}{2m}\sum_{i=1}^{m}[h_\theta(x^{(i)}) - y^{(i)}]^2
\end{equation} \\
\textbf{Gradient descent,} 
\begin{center}
\begin{align}
    \theta_j &:= \theta_j - \alpha\frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1,\theta_2,...,\theta_n)\\
    \theta_j &:= \theta_j - \alpha\frac{\partial}{\partial \theta_j} J(\theta) \\
    \theta_j &:= \theta_j - \alpha\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^{(i)}) - y^{(i)}]x^{(i)}_j
\end{align} 
\end{center} \\
\textbf{Feature scaling,} \\
Get every feature into approximately -1\le x_i \le 1 \ range. \\
\textbf{Mean normalization,} \\
Replace x_i \ with\ x_i - \mu_i\ to\ make\ features\ have\ approximately\ zero\ mean. \\
\subsection{Polynomial regression}
\textbf{Hypothesis,} \\
\begin{equation}
    h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2 x_2 + \theta_3 x_3.
\end{equation}\\
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|} \hline
         Housing price prediction  \\ \hline 
         x_1 = size \\ \hline
         x_2 = (size)^2 \\ \hline
         x_3 = (size)^3 \\  \hline
    \end{tabular}
    \caption{Features be-like in Polynomial regression.}
    \label{tab:my_label}
\end{table}
\subsection{Normal Equation}
\textbf{Intuition,} \\
\begin{center}
    \begin{math}
    \diff{}{\theta}J(\theta) = 0. 
    \end{math}
\end{center} \\
\textbf{Cost function,} 
\begin{center} \vspace{-9mm}
    \begin{align} 
    \theta\ \epsilon\ \Re^{n+1},\ J(\theta_0,\theta_1,\theta_2,...,\theta_n) = \frac{1}{2m} \sum_{i=1}^{m} [h_\theta(x^{(i)}) - y{(i)}]^2 \\
    \frac{\partial }{\partial \theta_j} J(\theta) = 0,\ (solve\ for\ \theta_0\ ,\theta_1\ ,...\ ,\theta_n)
    \end{align} \\
\end{center}
\textbf{Example,} \\
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|} \hline
         & Size (feet^2) & No.of Bed rooms & No.of floors & Age of home (years) & Price(\$1000) \\ \hline
        x_0 & x_1 & x_2 & x_3 & x_4 & y \\ \hline 
        1 & 2104 & 5 & 1 & 45 & 460 \\ \hline
        1 & 1416 & 3 & 2 & 40 & 232 \\ \hline
        1 & 1534 & 3 & 2 & 30 & 315 \\ \hline
        1 & 1852 & 2 & 1 & 36 & 178 \\ \hline
    \end{tabular}
    \caption{Sample Training set for Multi-Variate Linear regression.}
    \label{tab:my_label}
\end{table} 
\begin{enumerate}
    \item n = number of Features. 
    \item $x_j^{(i)}$ = value of j in the $i^{th}$ training example.
    \item $x_{(i)}$ = the input(features) of the $i^{th}$ training example.
\end{enumerate} \\
\begin{math}
X = \left[\begin{array}{ccccc}
    1 & 2104 & 5 & 1 & 45 \\
    1 & 1416 & 3 & 2 & 40 \\
    1 & 1534 & 3 & 2 & 30 \\
    1 & 852 & 2 & 1 & 36
\end{array} \right] &\;\;\;\;\;\;\;\;\; Y = \left[\begin{array}{c}
    460  \\
    232 \\
    315 \\
    178 
\end{array} \right]
\end{math} \\
X is m$\times$(n+1)-dimensional\ matrix\ and\ Y\ is\ a\ m-dimensional\ vector. \\
\begin{align}
\theta &= {(X^{\top}X)}^{-1} X^{\top} Y
\end{align} \\
The above $\theta$\ value\ is\ optimal\ $\theta$\ value. \\
For Normal equation method, then no need to use \textbf{feature scaling.} \\
We use 'Gradient descent' and 'Normal equation' methods to minimise cost function. \\
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} \hline
        Gradient descent & Normal equation  \\ \hline
        1$\rangle$ Need to choose '$\alpha$'. & 1$\rangle$ No need to choose '$\alpha$'. \\ \hline
        2$\rangle$ Need many iterations. & 2$\rangle$ Don't need to iterate. \\ \hline
        3$\rangle$ Works well even, when 'n' is large. (n>10000) & 3$\rangle$ Need to compute n$\times$ n matrix inverse {(X^{$\top$}X)}^{-1} \\ \hline
         & 4$\rangle$ Works Now if n is very large. \\ \hline
    \end{tabular}
    \caption{Why should we use the particular method? Advantages and Disadvantages of two methods.}
    \label{tab:my_label}
\end{table} 
\begin{align}
\theta &= {(X^{\top}X)}^{-1} X^{\top} Y
\end{align} \\
Q$\rangle$ What is $X^{\top}X$ is non-invertible(singular/degenerate) ? \\
Reasons, 
\begin{enumerate}
    \item Redundant features (linearly dependent)
    \begin{itemize}
        \item $x_1$ = Size in $feet^2$ 
        \item $x_2$ = Size in $m^2$
        \item $x_2$ = ${(3.28)}^2$ $x_1$ , 1m = 3.28feet. 
    \end{itemize}
    \item Too many features. (m$\le$n)
    \begin{itemize}
        \item m = 10
        \item n = 100, $\theta\ \epsilon\ {\Re}^{101}$, Delete some features (or) Regularization.
    \end{itemize}
\end{enumerate} \\
\subsection{Classification} 
The output value 'y' is \textbf{discrete value.} \\
The algorithm used is \textbf{logistic regression.} \\
\subsection{Logistic Regression Model}
$\therefore$ We want $0\le h_\theta(x) \le 1$. \\
\begin{align}
    h_\theta(x) &= g(\theta^{\top} x) \\
    g(z) &= \frac{1}{1+e^{-z}} \\
    h_\theta(x) &= g(\theta^{\top} x) \\
                &= \frac{1}{1+e^{-\theta^{\top} x}}
\end{align} \\
The above g(z) is called sigmoid function (or) logistic function. \\
\textbf{Graph,} \\
\begin{center}
\begin{tikzpicture}
\begin{axis}
\addplot[color=red]{1/(1+exp(-x))};
\end{axis}
\end{tikzpicture}
\end{center}
\begin{align}
    g(z) &\ge 0.5,\ when\ z \ge 0. \\
    h_\theta(x) = g(\theta^{\top} x) &\ge 0.5,\ when\ \theta^{\top} x \ge 0.
\end{align} 
\subsection{Interpretation of hypothesis output for Logistic Regression}
\begin{align}
    h_\theta(x) &= P(y=1|x;\theta) \\
    P(y=0|x;\theta) &+ P(y=1|x;\theta) = 1 \\
    P(y=0|x;\theta) &= 1-P(y=1|x;\theta) \\
                    &= 1-h_\theta(x) \\
\end{align} 
\therefore\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; y = 0 (or) 1. \\
\textbf{Decision boundary,} \\
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    enlargelimits=false,
]
\addplot[
    only marks,
    scatter,
    mark=x,
    mark size=2.9pt]
table[meta=ma]
{scattered_example.dat};
\end{axis}
\end{tikzpicture}
\end{center} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.45\textwidth]{Screenshot (79).png}
\caption{A simple decision boundary looks like.}
\label{fig:fig2}
\end{figure} \\
For, the above diagram decision boundary will be a line separating the two output values of y(y=0 (or) y=1). \\
\begin{align}
    h_\theta(x) &\ge 0.5 \rightarrow y=1. \\
    h_\theta(x) &< 0.5 \rightarrow y=0. \\
    h_\theta(x) &= g(\theta^{\top} x) \\
                &= g(\theta_0 + \theta_1x_1 + \theta_2x_2)
\end{align} 
\begin{math}
\theta = \left[\begin{array}{c}
    \theta_0  \\
    \theta_1 \\
    \theta_2
\end{array} \right] & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; x = \left[\begin{array}{c}
    1  \\
    x_1 \\
    x_2
\end{array} \right]
\end{math} \\
\textbf{Example,} \\
Let, \\
\begin{center}
\begin{math}
\theta = \left[\begin{array}{c}
    -3  \\
    1 \\
    1
\end{array} \right] 
\end{math}
\end{center}
\begin{align}
    y = 1, if\ \theta^{\top} x &\ge 0 \\
               -3+x_1+x_2 &\ge 0 \\
               x_1+x_2 &\ge 3,\ y=1 \\
               x_1+x_2 &<3,\ y=0.
\end{align} 
\textbf{Non-Linear Decision Boundary,} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{Screenshot (83).png}
\end{figure} \\
So, in the above non-linear classification, the \textbf{decision boundary is a circle} of radius of 1unit. \\
\begin{align}
    Inside\ circle,\ y &= 0 \\
    Outside\ circle,\ y &= 1.
\end{align} \\
\subsection{Cost function for Logistic Regression}
\textbf{Training set,} \\
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} \hline
        x^{(1)} & y^{(1)} \\ \hline
        x^{(2)} & y^{(2)} \\ \hline
        x^{(3)} & y^{(3)} \\ \hline
        \vdots & \vdots \\ \hline
        x^{(m)} & y^{(m)} \\ \hline
    \end{tabular}
    \caption{Training set of m-examples for Logistic Regression}
    \label{tab:my_label}
\end{table} \\
\begin{math}
x = \left[\begin{array}{c}
     x_0 \\
     x_1 \\
     x_2 \\
     \vdots \\
     x_n
\end{array}\right] \epsilon\ \Re^{n+1}\ -\ n\ features.\ x_0 = 1,\ y\ \epsilon\ {0,1}. 
\end{math} \\
For linear regression, \\
\begin{align}
    J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} [h_\theta(x^{(i)} - y^{(i)}]^2 \\
    J(\theta) = \frac{1}{m} \sum_{i=1}^{m} Cost(h_\theta(x^{(i)}, y^{(i)}) \\
    Cost(h_\theta(x^{(i)}), y^{(i)}) = \frac{1}{2} [h_\theta(x^{(i)} - y^{(i)}]^2
\end{align} \\
\begin{center}
\begin{tikzpicture}
\begin{axis}[title={Convex function},
    xlabel={x-axis},
    ylabel={y-axis}]
\addplot[color=red]{x^2};
\end{axis}
\end{tikzpicture}
\end{center}
The above graph is a convex. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.45\textwidth]{Non-convex(10.png}
\caption{Graph of non-convex function.}
\label{fig:fig2}
\end{figure} \\
The below graph is a non-convex.\\
If we use the cost function of linear regression in logistic regression, the the we would get non-convex cost function, because the \textbf{hypothesis is} $\frac{1}{1+e^{-\theta^{\top}x}}$. \\
For Logistic regression, 
\begin{align}
    Cost(h_\theta(x^{(i)}), y^{(i)}) =
    \begin{cases}
    -\log(1-h_\theta(x)), & if\ y = 0\\
    -\log(h_\theta(x)), & if\ y=1.
    \end{cases}
\end{align} \\
If y=1, \\
% \begin{center}
% \begin{tikzpicture}
% \begin{axis}[title={Convex function},
%     xlabel={h_\theta(x)}
%     ylabel={Cost function}]
% \addplot[color=red]{-ln(x)};
% \end{axis}
% \end{tikzpicture}
% \end{center}
\begin{center}
\setlength{\unitlength}{0.8cm}
\begin{picture}(6,6)
\thicklines
\put(1,1){\line(1,0){5}}
\put(1,1){\line(0,1){5}}
\qbezier(1.3,6)(1.5,1.693147)(6,1)
\put(3,0.5){$h_\theta(x)$}
\put(0.5,2.5){\begin{turn}{90}Cost function\end{turn}}
\put(5.95,0.9){$|$}
\put(5.95,0.3){1}
\put(0.95,0.9){$|$}
\put(1,0.3){0}
\end{picture}
\end{center} \\
If y=0, \\
\begin{center}
\setlength{\unitlength}{0.8cm}
\begin{picture}(6,6)
\thicklines
\put(1,1){\line(1,0){5}}
\put(1,1){\line(0,1){5}}
\qbezier(1,1)(5,1.3)(6,6)
\put(3,0.5){$h_\theta(x)$}
\put(0.5,2.5){\begin{turn}{90}Cost function\end{turn}}
\put(5.95,0.9){$|$}
\put(5.95,0.3){1}
\put(0.95,0.9){$|$}
\put(1,0.3){0}
\end{picture}
\end{center} \\
\textbf{Simplified Cost function,} \\
\begin{align}
    Cost(h_\theta(x^{(i)}), y^{(i)}) &= -(1-y)\log(1-h_\theta(x)) -y\log(h_\theta(x)).\ \forall\ y\ \epsilon\ \{0,1\}. \\
    If\ y &= 1: Cost(h_\theta(x), y) = -\log(h_\theta(x)). \\
    If\ y &= 0: Cost(h_\theta(x), y) = -\log(1-h_\theta(x)). 
\end{align}
\begin{align}
    Cost function = J(\theta) &= \frac{1}{m} \sum_{i=1}^{m} Cost(h_\theta(x^{(i)}, y^{(i)}) \\
    &= \frac{1}{m} [-\sum_{i=1}^{m} (1-y^{(i)})\log(1-h_\theta(x^{(i)})) +y^{(i)} \log(h_\theta(x^{(i)}))] 
\end{align}
\subsection{Gradient Descent for Logistic Regression}
\begin{align}
    \text{Repeat\ until\ convergence\ (minimum)}& \Bigg\{ \theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j} J(\theta) \\
    \frac{\partial}{\partial \theta_j} J(\theta) &= \frac{1}{m} \sum_{i=1}^{m} [h_\theta(x^{(i)}) - y^{(i)}]x_j^{(i)}
\end{align}
\subsection{Optimization algorithm}
\begin{enumerate}
    \item Gradient descent. 
    \item Conjugate gradient.
    \item BFGS.
    \item L - BFGS.
\end{enumerate}
These are the 4 algorithms to minimise \textbf{cost function.} \\
Advantages of the \textbf{last three advanced optimization algorithm.} 
\begin{itemize}
    \item No need to manually pick $\alpha$.
    \item Often faster than Gradient descent.
    \item They themselves choose $\alpha$, for faster convergence.
\end{itemize} 
\subsection{Multiclass Classification : One-vs-All}
\begin{align*}
y &\in \lbrace0, 1 ... n\rbrace \\
h_\theta^{(0)}(x) &= P(y = 0 | x ; \theta) \\
h_\theta^{(1)}(x) &= P(y = 1 | x ; \theta) \\
&\vdots \\
h_\theta^{(n)}(x) &= P(y = n | x ; \theta) \\
\mathrm{prediction} &= \max_i( h_\theta ^{(i)}(x) ) 
\end{align*}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{multi class.png}
\caption{Multiclass being classified into n-classifiers for n-types of output.}
\label{fig:fig2}
\end{figure} 
To summarize, 
\begin{enumerate}
    \item Train a logistic regression classifier $h_\theta(x)$ for each class to predict the probability that y=i.
    \item To make a prediction on a new x, pick the class that maximizes  $h_\theta(x)$
\end{enumerate}
\subsection{Problem of Overfitting,} 
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{Screenshot (89).png}
\caption{Example for underfit and overfit hypothesis.}
\label{fig:fig2}
\end{figure} \\
Similar for Logistic Regression. \\
\textbf{Addressing Overfitting,} 
\begin{enumerate}
    \item Reduce number of features.
    \begin{itemize}
        \item Manually select which features to keep.
        \item Model selection algorithm.
    \end{itemize}
    \item Regularization
    \begin{itemize}
        \item Keep all features, but reduce magnitude/values of parameters $\thicklines_j$.
        \item Works well when we have a lot of features, each of which contributes a bit to predict $y^{(i)}$.
    \end{itemize}
\end{enumerate}
\subsection{Regularization}
Small values for parameters $\theta_0,\theta_1,\theta_2,...,\theta_n$.
\begin{enumerate}
    \item Simpler hypothesis.
    \item Less prone to overfitting.
\end{enumerate}
\textbf{Regularized Cost function,}
\begin{align}
    J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \bigg[[h_\theta(x^{(i)} - y^{(i)}]^2 + \lambda\sum_{j=1}^{n} \theta_j^2\bigg] 
\end{align} \\
If '$\lambda$' is extremely large, then the cost function will become underfitting (doesn't fit to our training data). \\
\begin{align*} 
\text{Repeat}\ \lbrace & \\
\theta_0 &:= \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\
\theta_j &:= \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\\
\rbrace
\end{align*}
The term $\frac{\lambda}{m} \theta_j$ performs our regularization. With some manipulation our update rule can also be represented as:
\begin{align}
\theta_j := \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
\end{align}
The first term in the above equation, $1 - \alpha\frac{\lambda}{m}$ will always be less than 1. Intuitively you can see it as reducing the value of $\theta_j$ by some amount on every update. Notice that the second term is now exactly the same as it was before. \\
\textbf{Normal equation after regularization,}
\begin{align}
\theta &= {(X^{\top}X) + \lambda.L}^{-1} X^{\top} Y
\end{align} 
\begin{math}
where\ L = \left[\begin{array}{ccccc}
    0 &  &  &  &   \\
     & 1 &  &  &   \\
     &  & 1 &  & \\
     &  &  & \ddots &  \\
     &  &  &  & 1 \\
\end{array} \right]
\end{math} 
\subsection{Regularized Logistic Regression}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{regular logis 1.png}
\caption{Regularization of a overfit data.}
\label{fig:fig2}
\end{figure}
\textbf{Cost function,} 
\begin{align}
J(\theta) = \frac{1}{m} \bigg[-\sum_{i=1}^{m} (1-y^{(i)})\log(1-h_\theta(x^{(i)})) +y^{(i)} \log(h_\theta(x^{(i)}))\bigg] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
\end{align} 
\newpage The second sum, $\sum_{j=1}^n \theta_j^2$ means to explicitly exclude the bias term, $\theta_0$. I.e. the θ vector is indexed from 0 to n (holding n+1 values, $\theta_0$ through $\theta_n$), and this sum explicitly skips $\theta_0$, by running from 1 to n, skipping 0. Thus, when computing the equation, we should continuously update the two following equations: 
\begin{align*} 
\text{Repeat}\ \lbrace & \\
\theta_0 &:= \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\
\theta_j &:= \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\\
\rbrace
\end{align*}
\section{Neural Networks}
\subsection{Model representation I}
Let's examine how we will represent a hypothesis function using neural networks. At a very simple level, neurons are basically computational units that take inputs (dendrites) as electrical inputs (called \textbf{"spikes"}) that are channeled to outputs (axons). In our model, our dendrites are like the input features $x_1\cdots x_n$, and the output is the result of our hypothesis function. In this model our $x_0$ input node is sometimes called the \textbf{"bias unit".} It is always equal to 1. In neural networks, we use the same logistic function as in classification, $\frac{1}{1 + e^{-\theta^Tx}}$, yet we sometimes call it a sigmoid (logistic) activation function. In this situation, our \textbf{"theta" parameters} are sometimes called \textbf{"weights".} \\
A simple representation looks like : 
\begin{center}
\begin{math}
\left[ \begin{array}{c}
    x_0x_1x_2  \\
\end{array}\right] \rightarrow \left[\; \right] \rightarrow h_\theta(x)
\end{math}
\end{center} \\
Our input nodes (layer 1), also known as the \textbf{"input layer"}, go into another node (layer 2), which finally outputs the hypothesis function, known as the \textbf{"output layer".} \\
We can have intermediate layers of nodes between the input and output layers called the \textbf{"hidden layers."} \\
In this example, we label these intermediate or \textbf{"hidden"} layer nodes $a^2_0, \cdots, a^2_na$ and call them \textbf{"activation units."} 
\begin{center}
    \begin{enumerate}
        \item $a_i^{(j)}$ = "activation" of unit i in layer j
        \item $\Theta^{(j)}$ = matrix of weights controlling function mapping from layer j to layer j+1
    \end{enumerate}
\end{center}
If we had one hidden layer, it would look like : \\
\begin{center}
\begin{math}
\left[ \begin{array}{c}
    x_0x_1x_2x_3  \\
\end{array}\right] \rightarrow \left[a_1^{(2)}a_2^{(2)}a_3^{(2)} \right] \rightarrow h_\theta(x)
\end{math}
\end{center}
The values for each of the \textbf{"activation"} nodes is obtained as follows : \\
\begin{align}
    a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \\
    a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \\
    a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \\
    h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})
\end{align}
This is saying that we compute our activation nodes by using a 3$\times$4 matrix of parameters. We apply each row of the parameters to our inputs to obtain the value for one activation node. Our hypothesis output is the logistic function applied to the sum of the values of our activation nodes, which have been multiplied by yet another parameter matrix $\Theta^{(2)}$ containing the weights for our second layer of nodes. \\
Each layer gets its own matrix of weights, $\Theta^{(j)}$. \\
The dimensions of these matrices of weights is determined as follows : \\ 
If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1} \times (s_j + 1)$. \\
The +1 comes from the addition in $\Theta^{(j)}$ of the \textbf{"bias nodes,"} $x_0$ and $\Theta_0^{(j)}$. In other words the output nodes will not include the bias nodes while the inputs will. \\ 
\subsection{Model representation II}
we'll do a vectorized implementation of the above functions. We're going to define a new variable $z_k^{(j)}$ that encompasses the parameters inside our g function. \\
\begin{align}
    a_1^{(2)} &= g(z_1^{(2)}) \\
    a_2^{(2)} &= g(z_2^{(2)}) \\
    a_3^{(2)} &= g(z_3^{(2)}) \\
    x &= a^{(1)} \\
    a^{(2)} &= g(\theta^{(1)} x) \\
    &= g(\theta^{(1)} a^{(1)}) = g(z^{(2)}) \\
    h_\theta(x) &= g(\theta^{(2)} a^{(2)}) \\
    &= g(z^{(3)})
\end{align}
In other words, for layer j=2 and node k, the variable z will be : \\
\begin{align}
    z_k^{(2)} = \Theta_{k,0}^{(1)} x_0 + \Theta_{k,1}^{(1)} x_1 + \cdots + \Theta_{k,n}^{(1)} x_n
\end{align}
The vector representation of x and $z^{j}$ is : \\
\begin{math}
x = \left[\begin{array}{c}
x_0 \\
x_1 \\
\vdots \\
x_n
\end{array} \right] \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
z^{(j)} = \left[\begin{array}{c}
z_1^{(j)} \\
z_2^{(j)} \\
\vdots \\
z_n^{(j)}
\end{array} \right]
\end{math} \\
Setting x = $a^{(1)}$, we can rewrite the equation as :
\begin{align}
    z^{(j)} = \theta^{(j-1)} a^{(j-1)}
\end{align} 
We are multiplying our matrix $\Theta^{(j-1)}$ with dimensions $s_j\times$ (n+1)(where $s_j$ is the number of our activation nodes) by our vector $a^{(j-1)}$ with height (n+1). This gives us our vector $z^{(j)}$ with height $s_j$. Now we can get a vector of our activation nodes for layer j as follows : \\
\begin{align} 
a^{(j)} = g(z^{(j)})a 
\end{align}
Where our function g can be applied element-wise to our vector $z^{(j)}$ \\
We can then add a bias unit (equal to 1) to layer j after we have computed $a^{(j)}$. This will be element $a_0^{(j)}$ and will be equal to 1. To compute our final hypothesis, let's first compute another z vector : \\
\begin{align}
z^{(j+1)} = \Theta^{(j)}a^{(j)}
\end{align}
We get this final z vector by multiplying the next theta matrix after $\Theta^{(j-1)}$ with the values of all the activation nodes we just got. This last theta matrix $\Theta^{(j)}$ will have only one row which is multiplied by one column $a^{(j)}$ so that our result is a single number. We then get our final result with : \\
\begin{align}
h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)})
\end{align}
Notice that in this last step, between layer j and layer j+1, we are doing exactly the same thing as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypothesis. \\
\textbf{Examples,} \\
A simple example of applying neural networks is by predicting $x_1$ AND $x_2$, which is the logical 'and' operator and is only true if both $x_1$ and $x_2$ are 1.
\begin{align*}
h_\Theta(x) &= g(-30 + 20x_1 + 20x_2) \\
x_1 &= 0 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-30) \approx 0 \\
x_1 &= 0 \ \ and \ \ x_2 = 1 \ \ then \ \ g(-10) \approx 0 \\
x_1 &= 1 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-10) \approx 0 \\
x_1 &= 1 \ \ and \ \ x_2 = 1 \ \ then \ \ g(10) \approx 1
\end{align*}
So we have constructed one of the fundamental operations in computers by using a small neural network rather than using an actual AND gate. Neural networks can also be used to simulate all the other logical gates. The following is an example of the logical operator 'OR', meaning either $x_1$ is true or $x_2$ is true, or both :
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{OR 1.png}
\caption{Hypothesis of neural networks is represented through logical gates.}
\label{fig:fig2}
\end{figure} \\
The $Θ^{(1)}$ matrices for AND, NOR, and OR are : 
\begin{align*}
AND:\\
\Theta^{(1)} &=\begin{bmatrix}-30 & 20 & 20\end{bmatrix} \\
NOR:\\
\Theta^{(1)} &= \begin{bmatrix}10 & -20 & -20\end{bmatrix} \\
OR:\\
\Theta^{(1)} &= \begin{bmatrix}-10 & 20 & 20\end{bmatrix} \\
\end{align*}
We can combine these to get the XNOR logical operator (which gives 1 if $x_1$ and $x_2$ are both 0 or both 1). 
\begin{align*}
\begin{bmatrix}
x_0 \\
x_1 \\
x_2\end{bmatrix} 
\rightarrow \begin{bmatrix}
a_1^{(2)} \\
a_2^{(2)} \end{bmatrix}
\rightarrow\begin{bmatrix}
a^{(3)}\end{bmatrix} 
\rightarrow h_\Theta(x)
\end{align*}
Let's write out the values for all our nodes :
\begin{align*}
a^{(2)} = g(\Theta^{(1)} \cdot x) \\
a^{(3)} = g(\Theta^{(2)} \cdot a^{(2)}) \\
h_\Theta(x) = a^{(3)}
\end{align*}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{XNOR.png}
\caption{Hypothesis in XNOR logical gate.}
\label{fig:fig2}
\end{figure} 
\subsection{Multiclass Classification}
To classify data into multiple classes, we let our hypothesis function return a vector of values. Say we wanted to classify our data into one of four categories. We will use the following example to see how this classification is done. This algorithm takes as input an image and classifies it accordingly : \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{neural multi.png}
\caption{Multiclass Classification for different outputs for different things.}
\label{fig:fig2}
\end{figure} \\
\notte{\textbf{As a general rule, the when the number of classes are greater than 2, then the classes should be one hot encoded to ease the process of classification. It can be seen an defining individual logistic units for determining whether or not it belongs to that class.}} \\ \newpage
We can define our set of resulting classes as y : \\
\begin{figure}[h]
\centering
\includegraphics[width = 1.2\textwidth]{y.png}
\caption{The output 'y' of neural networks.}
\label{fig:fig2}
\end{figure} \\
Each $y^{(i)}$ represents a different image corresponding to either a car, pedestrian, truck, or motorcycle. The inner layers, each provide us with some new information which leads to our final hypothesis function. The setup looks like : \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.2\textwidth]{h.png}
\caption{Hypothesis calculation of neural networks.}
\label{fig:fig2}
\end{figure} \\ \newpage
Our resulting hypothesis for one set of inputs may look like : 
\begin{center}
    \begin{math}
    h_\theta(x) = \left[\begin{array}{c}
        0\ 0\ 1\ 0  \\
    \end{array} \right]
    \end{math}
\end{center} \\
In which case our resulting class is the third one down, or $h_\Theta(x)_3$, which represents the motorcycle. \\
\subsection{Cost function for Neural networks}
Let's declare some variables. 
\begin{enumerate}
    \item L = total number of layers in the network.
    \item $s_l$ = number of units (not counting bias unit) in layer l.
    \item K = number of output units/classes.
\end{enumerate}
\textbf{The cost function for regularized logistic regression,}
\begin{align}
    J(\theta) = \frac{1}{m} \bigg[-\sum_{i=1}^{m} (1-y^{(i)})\log(1-h_\theta(x^{(i)})) +y^{(i)} \log(h_\theta(x^{(i)}))\bigg] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
\end{align} \\
\textbf{The cost function for Neural networks,}
\begin{align}
    J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}\sum_{k=1}^{K} \bigg[(1-y^{(i)}_k)\log(1-(h_\theta(x^{(i)}))_k) +y^{(i)}_k \log((h_\theta(x^{(i)}))_k)\bigg] + \frac{\lambda}{2m} \sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}} (\Theta_{j,i}^{(l)})^2
\end{align} \\
In the regularization part, after the square brackets, we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term. \\
\begin{enumerate}
    \item The double sum simply adds up the logistic regression costs calculated for each cell in the output layer.
    \item The triple sum simply adds up the squares of all the individual Θs in the entire network.
    \item The i in the triple sum does not refer to training example i.
\end{enumerate}
\subsection{Backpropagation Algorithm}
\textbf{"Backpropagation"} is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression. Our goal is to compute : 
\begin{align}
    \min_\Theta J(\Theta)
\end{align}
That is, we want to minimize our cost function J using an optimal set of parameters in theta. In this section we'll look at the equations we use to compute the partial derivative of J(Θ) :
\begin{align}
    \frac{\partial}{\partial \Theta_{j,i}^{(l)}} J(\Theta)
\end{align}
\textbf{Backpropagation algorithm,}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{Back propagation.png}
\caption{Backpropagation algorithm.}
\label{fig:fig2}
\end{figure} 
\begin{enumerate}
    \item Given training set $\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace$.
    \begin{itemize}
        \item Set $\Delta^{(l)}_{i,j} :=$ 0 for all (l,i,j), (hence you end up having a matrix full of zeros)
    \end{itemize}
    \item For training example t = 1 to m :
    \begin{itemize}
        \item Set $a^{(1)} := x^{(t)}$.
        \item Perform forward propagation to compute $a^{(l)}$ for l=2,3,…,L.
        \item Using $y^{(t)}$, compute $\delta^{(L)} = a^{(L)} - y^{(t)}$.
        Where L is our total number of layers and $a^{(L)}$ is the vector of outputs of the activation units for the last layer. So our \textbf{"error values"} for the last layer are simply the differences of our actual results in the last layer and the correct outputs in y. To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left :
        \begin{figure}[!htb]
        \centering
        \includegraphics[width = 1.1\textwidth]{Back gradient.png}
        \caption{Calculation of values of activation and $\Theta$.}
        \label{fig:fig2}
        \end{figure} 
        \item Compute $\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}$ using $\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})$. \\
        The delta values of layer l are calculated by multiplying the delta values in the next layer with the theta matrix of layer l. We then element-wise multiply that with a function called $g^\prime$, or g-prime, which is the derivative of the activation function g evaluated with the input values given by $z^{(l)}$. \\
        The g-prime derivative terms can also be written out as :
        \begin{align}
            g^\prime(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})
        \end{align}
        \item  $\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}$ (or) with vectorization, $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$. \\
        Hence we update our new $\Delta$ matrix.
        \begin{enumerate}
            \item $D_{i,j}^{(l)} := \frac{1}{m} (\Delta_{i,j}^{(l)} + \lambda\Theta_{i,j}^{(l)})$, if j\neq 0. \\
            \item $D_{i,j}^{(l)} := \frac{1}{m} (\Delta_{i,j}^{(l)}$, if j = 0.
        \end{enumerate}
        The capital-delta matrix D is used as an "accumulator" to add up our values as we go along and eventually compute our partial derivative. Thus we get $\frac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta) = D_{ij}^{(l)}$.
    \end{itemize}
\end{enumerate}
If we consider simple non-multiclass classification (k = 1) and disregard regularization, the cost is computed with :
\begin{align}
    Cost(t) = (1-y^{(t)})\log(1-h_\theta(x^{(t)})) +y^{(t)} \log(h_\theta(x^{(t)}))
\end{align}
Intuitively, $\delta_j^{(l)}$ is the \textbf{"error"} for $a^{(l)}_j$ (unit j in layer l). More formally, the delta values are actually the derivative of the cost function :
\begin{align}
    \delta_{j}^{(l)} = \frac{\partial}{\partial z_j^{(l)}} cost(t)
\end{align}
Our derivative is the slope of a line tangent to the cost function, so the steeper the slope the more incorrect we are. Let us consider the following neural network below and see how we could calculate some $\delta_j^{(l)}$ :
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{Forward.png}
\caption{The forward propagation in backward propagation for $\delta_j^{(l)}$.}
\label{fig:fig2}
\end{figure} \newpage
In the image above, to calculate $\delta_2^{(2)}$, we multiply the weights $\Theta_{12}^{(2)}$ and $\Theta_{22}^{(2)}$ by their respective $\delta$ values found to the right of each edge. So we get $\delta_2^{(2)} = \Theta_{12}^{(2)}*\delta_1^{(3)}+\Theta_{22}^{(2)}*\delta_2^{(3)}$. To calculate every single possible $\delta_j^{(l)}$, we could start from the right of our diagram. We can think of our edges as our $\Theta_{ij}$. Going from right to left, to calculate the value of $\delta_j^{(l)}$, you can just take the over all sum of each weight times the $\delta$ it is coming from. Hence, another example would be $\delta_2^{(3)} =\Theta_{12}^{(3)}*\delta_1^{(4)}$.
\subsection{Gradient Checking}
Gradient checking will assure that our backpropagation works as intended. We can approximate the derivative of our cost function with :
\begin{align}
    \frac{\partial}{\partial \theta} J(\Theta) \approx \frac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}
\end{align}
With multiple theta matrices, we can approximate the derivative with respect to $\Theta_j$ as follows :
\begin{align}
    \frac{\partial}{\partial \theta_j} J(\Theta) \approx \frac{J(\Theta_1,...,\Theta_j + \epsilon,..., \Theta_n) - J(\Theta_1,...,\Theta_j - \epsilon,..., \Theta_n)}{2\epsilon}
\end{align}
A small value for ${\epsilon}$ (epsilon) such as ${\epsilon = 10^{-4}}$, guarantees that the math works out properly. If the value for $\epsilon$ is too small, we can end up with numerical problems. \\
We previously saw how to calculate the deltaVector. So once we compute our gradApprox vector, we can check that gradApprox $\approx$ deltaVector. \\
Once you have verified once that your backpropagation algorithm is correct, you don't need to compute gradApprox again. The code to compute gradApprox can be very slow.
\subsection{Random Intialization}
Initializing all theta weights to zero does not work with neural networks. When we backpropagate, all nodes will update to the same value repeatedly. Instead we can randomly initialize our weights for our $\Theta$ matrices using the following method :
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{random.png}
\caption{Random intialization to begin and get into the problem.}
\label{fig:fig2}
\end{figure} 
\newpage Hence, we initialize each $\Theta^{(l)}_{ij}$ to a random value between $[-\epsilon,\epsilon]$. Using the above formula guarantees that we get the desired bound. \\
\boxed{\textbf{The epsilon used above is unrelated to the epsilon from Gradient Checking.}}
\subsection{Choosing Neural network}
\begin{enumerate}
    \item Pick a network architecture.
    \item Choose the layout of your neural network.
    \item Including how many hidden units in each layer and how many layers in total you want to have.
    \begin{itemize}
        \item Number of input units = dimension of features $x^{(i)}$.
        \item Number of output units = number of classes.
        \item Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units).
        \item \textbf{Defaults} : 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.
    \end{itemize}
\end{enumerate}
\subsection{Training a Neural Network}
\begin{enumerate}
    \item Randomly initialize the weights.
    \item Implement forward propagation to get $h_\Theta(x^{(i)})$. \\
    \item Implement the cost function.
    \item Implement backpropagation to compute partial derivatives. ($\frac {\partial} {\partial \Theta_{jk}^{(l)}} J(\Theta)$)
    \item Use gradient checking to compare ($\frac {\partial} {\partial \Theta_{jk}^{(l)}} J(\Theta)$) given by backpropagation vs the numerical estimate of gradient. S0, to confirm that your backpropagation works. Then disable gradient checking.
    \item Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.
\end{enumerate}
\notte{\textbf{Ideally, you want $h_\Theta(x^{(i)}) \approx≈ y^{(i)}$. This will minimize our cost function. However, keep in mind that $J(\Theta)$ is not convex and thus we can end up in a local minimum instead. }}
\subsection{Neural network Architecture}
This usually means to pick the connectivity pattern between the neurons. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{network.png}
\caption{Different architectures of neural networks.}
\label{fig:fig2}
\end{figure} \\
Above figure shows a few examples of network architectures. It can be seen that the three networks have the same number of input and output units. This is so because the input units equals input features in a given training example while the output units equals the number of target classes. \\
\notte{\textbf{For neural networks the cost function, $J(\Theta)$ is non-convex and hence susceptible to local minima. But in practice it does not present a serious problem in the implementation.}}
\chapter{Deciding whether the algorithm is perfect or not}
\section{Evaluating the algorithm}
\subsection{Evaluating a Hypothesis}
\textbf{Fails to generalize to new examples not in training set.} \\
\textbf{Once we have done some trouble shooting for errors in our predictions by : } 
\begin{enumerate}
    \item Getting more training examples.
    \item Trying smaller sets of features.
    \item Trying additional features.
    \item Trying polynomial features.
    \item Increasing or decreasing $\lambda$.
\end{enumerate}
We can move on to evaluate our new hypothesis. \\
A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a training set and a test set. Typically, the training set consists of 70$\%$ of your data and the test set is the remaining 30$\%$. \\
The new procedure using these two sets is then :  \newpage
\textbf{Dataset,} \\
\begin{table}[h!]
    \centering
    \begin{tabular}{c|ccc}
        Size & Price  \\ \hline
        2104 & 400  &  & (x^{(1)},y^{(1)})\\
        1600 & 330  &  & (x^{(2)},y^{(1)})\\
        2400 & 369  &  & (x^{(3)},y^{(1)})\\
        1416 & 232  & $\rightarrow$ &  \vdots\\
        3000 & 540  &  &  \vdots\\
        1985 & 300  &  &  \vdots\\
        1534 & 315 &  & (x^{(m)},y^{(m)})\\ \hline
        1427 & 199  &  & (x^{(1)}_{test},y^{(1)}_{test})\\
        1380 & 212  & $\longmapsto$ & (x^{(2)}_{test},y^{(2)}_{test})\\
        1494 & 243  &  & (x^{(m_{test})}_{test},y^{(m_{test})}_{test})\\ \hline
    \end{tabular}
    \caption{Evaluating your hypothesis}
    \label{tab:my_label}
\end{table}
\textbf{Model selection,}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{model.png}
\caption{Model selectio process.}
\label{fig:fig2}
\end{figure}
\begin{enumerate}
    \item $m_{test}$ = number of test example. 
    \item Learn $\Theta$ and minimize $J_{train}(\Theta)$ using the training set.
    \item Compute the test set error $J_{test}(\Theta)$.
\end{enumerate}
\subsection{The test set error}
\begin{enumerate}
    \item For linear regression: $J_{test}(\Theta) = \dfrac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_\Theta(x^{(i)}_{test}) - y^{(i)}_{test})^2$.
    \item For classification Misclassification error (aka 0/1 misclassification error):
    \begin{align}
        err(h_\Theta(x),y) = \begin{cases}
        1 & if\ h_\Theta(x) \geq 0.5\ and\ y = 0\ or\ h_\Theta(x) < 0.5\ and\ y = 1 \\
        0 & otherwise
        \end{cases}
    \end{align}
\end{enumerate}
This gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is :
\begin{align}
\text{Test Error} = \dfrac{1}{m_{test}} \sum^{m_{test}}_{i=1} err(h_\Theta(x^{(i)}_{test}), y^{(i)}_{test})
\end{align}
This gives us the proportion of the test data that was misclassified.
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{train test.png}
\caption{Cost function for training, cross-validation and test error.}
\label{fig:fig2}
\end{figure}
\subsection{Model Selection and Train/ Validation/ Test Sets}
Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could over fit and as a result your predictions on the test set would be poor. The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than the error on any other data set. \\
Given many models with different polynomial degrees, we can use a systematic approach to identify the 'best' function. In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result. \\
One way to break down our dataset into the three sets is :
\begin{enumerate}
    \item Training set: 60$\%$.
    \item Cross validation set: 20$\%$.
    \item Test set: 20$\%$.
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{dataset.png}
\caption{Separation of dataset into trsin, cross-validation and test sets.}
\label{fig:fig2}
\end{figure}
We can now calculate three separate error values for the three different sets using the following method:
\begin{enumerate}
    \item Optimize the parameters in Θ using the training set for each polynomial degree.
    \item Find the polynomial degree d with the least error using the cross validation set.
    \item Estimate the generalization error using the test set with $J_{test}(\Theta^{(d)})$, (d = theta from polynomial with lower error).
\end{enumerate}
\boxed{\textbf{This way, the degree of the polynomial d has not been trained using the test set.}}
\subsection{Diagnosing Bias vs Variance}
In this section we examine the relationship between the degree of the polynomial (d) and the underfitting (or) overfitting of our hypothesis.
\begin{itemize}
    \item We need to distinguish whether \textbf{bias} or \textbf{variance} is the problem contributing to bad predictions.
    \item High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two.
\end{itemize}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{d bais variance.png}
\caption{Diagnosing bias and variance.}
\label{fig:fig2}
\end{figure}
The training error will tend to \textbf{decrease} as we increase the degree d of the polynomial. \\
At the same time, the cross validation error will tend to \textbf{decrease} as we increase d up to a point, and then it will \textbf{increase} as \textbf{d} is increased, forming a convex curve. \\
\textbf{High\ bias\ (underfitting)}\ :\ both\ $J_{train}(\Theta)$\ and\ $J_{CV}(\Theta)$\ will\ be\ high.\ Also,\ $J_{CV}(\Theta)\ \approx$\ $J_{train}(\Theta)$. \newline
\textbf{High\ variance\ (overfitting)}\ :\ $J_{train}(\Theta)$\ will\ be\ low\ and\ $J_{CV}(\Theta)$\ will\ be\ much\ greater\ than\ $J_{train}(\Theta)$.
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{bias variance.png}
\caption{graph for optimal value of 'd'.}
\label{fig:fig2}
\end{figure}
\subsection{Regularization}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{regular bias.png}
\caption{Regularization for Linear regression.}
\label{fig:fig2}
\end{figure} 
In the figure below, we see that as $\lambda$ increases, our fit becomes more rigid. On the other hand, as $\lambda$ approaches 0, we tend to over overfit the data. So how do we choose our parameter $\lambda$ to get it 'just right' ? In order to choose the model and the regularization term $\lambda$, we need to:
\newpage
\begin{enumerate}
    \item Create a list of lambdas (i.e. $\lambda$ ∈ {0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24}).
    \item Create a set of models with different degrees or any other variants.
    \item Iterate through the $\lambda$s and for each $\lambda$ go through all the models to learn some $\Theta$. 
    \item Compute the cross validation error using the learned $\Theta$ (computed with $\lambda$) on the $J_{CV}(\Theta)$ without regularization or $\lambda$ = 0.
    \item Select the best combo that produces the lowest error on the cross validation set.
    \item Using the best combo Θ and λ, apply it on $J_{test}(\Theta)$ to see if it has a good generalization of the problem.
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{regular lam.png}
\caption{Regularization parameter.}
\label{fig:fig2}
\end{figure}
\textbf{Machine Learning Diagnostics are tests that help gain insight about what would or would not work with a learning algorithm, and hence give guidance about how to improve the performance. These can take time to implement, but are still worth venturing into during the time of uncertainties.}
\subsection{Learning Curves}
Training an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence :
\begin{enumerate}
    \item As the training set gets larger, the error for a quadratic function increases.
    \item The error value will plateau out after a certain m, or training set size.
\end{enumerate}
\textbf{Experiencing high bias :} \\
\textbf{Low training set size:} causes $J_{train}(\Theta)$ to be low and $J_{CV}(\Theta)$ to be high. \\
\textbf{Large training set size:} causes both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ to be high with $J_{train}(\Theta) \approx J_{CV}(\Theta)$. \\
If a learning algorithm is suffering from \textbf{high bias}, getting more training data will not (by itself) help much.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.8\textwidth]{overfit.png}
\caption{High variance.}
\label{fig:fig2}
\end{figure}
\newline \textbf{Experiencing high variance:} \\
\textbf{Low training set size:} $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be high. \\
\textbf{Large training set size:} $J_{train}(\Theta)$ increases with training set size and $J_{CV}(\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\Theta) < J_{CV}(\Theta)$ but the difference between them remains significant. \\
If a learning algorithm is suffering from \textbf{high variance}, getting more training data is likely to help.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.8\textwidth]{underfit 1.png}
\caption{High bais.}
\label{fig:fig2}
\end{figure}
\subsection{Deciding What to Do Next Revisited}
Our decision process can be broken down as follows:
\begin{enumerate}
    \item \textbf{Getting more training examples:} Fixes high variance.
    \item \textbf{Trying smaller sets of features:} Fixes high variance.
    \item \textbf{Adding features:} Fixes high bias.
    \item \textbf{Adding polynomial features:} Fixes high bias.
    \item \textbf{Decreasing $\lambda$:} Fixes high bias.
    \item \textbf{Increasing $\lambda$:} Fixes high variance.
\end{enumerate}
\subsubsection{Diagnosing Neural Networks}
\begin{enumerate}
    \item A neural network with fewer parameters is \textbf{prone to underfitting}. It is also \textbf{computationally cheaper.}
    \item A large neural network with more parameters is \textbf{prone to overfitting.} It is also \textbf{computationally expensive.} In this case you can use regularization (increase $\lambda$) to address the overfitting.
\end{enumerate}
Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best. 
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{neural.png}
\caption{Regularization for neural networks with overfitting hypothesis.}
\label{fig:fig2}
\end{figure}
\subsubsection{Machine learning diagnostic}
\begin{definition}
\textbf{Diagnostic:} A test that you can run to gain insight what is/ isn’t working with a learning algorithm, and gain guidance as to how best to improve its performance. \\
Diagnostic can take time to implement, but doing so can be a very good use of your time.
\end{definition}
\subsubsection{Model Complexity Effects:}
\begin{enumerate}
    \item Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.
    \item Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.
    \item In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.
\end{enumerate}
\subsection{Example on Building a spam classifier}
\textbf{Spam classifier is a problem of classification under Supervised learning.} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{Screenshot (113).png}
\caption{Classifier of mails based spam/non-spam..}
\label{fig:fig2}
\end{figure}
\textbf{System Design Example:} \\
Given a data set of emails, we could construct a vector for each email. Each entry in this vector represents a word. The vector normally contains 10,000 to 50,000 entries gathered by finding the most frequently used words in our data set.  If a word is to be found in the email, we would assign its respective entry a 1, else if it is not found, that entry would be a 0. Once we have all our x vectors ready, we train our algorithm and finally, we could use it to classify if an email is a spam (or) not. \\
As we will train our algorithm by giving a large set of data, and it will classify on what terms the emails are being classified into spam and non-spam, by seeing and learning from that our algorithm classifies the given set of emails given by us into spam and non-spam. \\
\textbf{So how could you spend your time to improve the accuracy of this classifier?}
\begin{enumerate}
    \item Collect lots of data (for example \textbf{"honeypot"} project but doesn't always work).
    \item Develop sophisticated features (for example: using email header data in spam emails).
    \item Develop algorithms to process your input in different ways (recognizing misspellings in spam).
\end{enumerate}
\boxed{\textbf{It is difficult to tell which of the options will be most helpful.}}
\subsection{Error Analysis}
The recommended approach to solving machine learning problems is to:
\begin{enumerate}
    \item Start with a simple algorithm, implement it quickly, and test it early on your cross validation data.
    \item Plot learning curves to decide if more data, more features, etc. are likely to help.
    \item Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made.
\end{enumerate}
For example, assume that we have 500 emails and our algorithm misclassifies a 100 of them. We could manually analyze the 100 emails and categorize them based on what type of emails they are. We could then try to come up with new cues and features that would help us classify these 100 emails correctly. Hence, if most of our misclassified emails are those which try to steal passwords, then we could find some features that are particular to those emails and add them to our model. We could also see how classifying each word according to its root changes our error rate: \\
We do \textbf{error analysis} to know that whether our algorithm is working fine (or) not. As we take certain cases(even though we are taking large set of data, there could be a corner case that our algorithm might not encountered in the given data set) and it is must that we take 20$\%$ each of cross-validation set and test set. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{Screenshot (116).png}
\caption{Error analysis.}
\label{fig:fig2}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{imp.png}
\caption{Examples of Numerical evaluation.}
\label{fig:fig2}
\end{figure} \newpage
It is very important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm's performance. For example if we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a 3$\%$ error rate instead of 5$\%$, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a 3.2$\%$ error rate instead of 3$\%$, then we should avoid using this new feature.  Hence, we should try new things, get a numerical value for our error rate, and based on our result decide whether we want to keep the new feature or not. 
\subsection{Error metrics for skewed classes}
\subsubsection{Skewed classes}
\begin{definition}
\textbf{Skewed classes} : Skewed classes basically refer to a dataset, wherein the number of training example belonging to one class out-numbers heavily the number of training examples beloning to the other. \\
Consider a binary classification, where a cancerous patient is to be detected based on some features. And say only 1 of the data provided has cancer positive. In a setting where having cancer is labelled 1 and not cancer labelled 0, if a system naively gives the prediction as all 0’s, still the prediction accuracy will be 99$\%$. \\
Therefore, it can be said with conviction that the accuracy metrics or mean-squared error for skewed classes, is not a proper indicator of model performance. Hence, there is a need for a different error metric for skewed classes.
\end{definition}
Skewed class distributions present a challenge in many different domains. Specifically, most supervised machine learning algorithms exhibit poor performance when faced with skewed class distributions. This is referred to as the class imbalance problem. \\
Class imbalance often occurs in real-life datasets involving rare events such as detecting certain medical conditions, fraudulent transactions. \\
When the majority of data items in your dataset represents items belonging to one class, we say the dataset is skewed or imbalanced. For better understanding, lets consider a binary classification problem, cancer detection. Say we’ve five thousand instances in our dataset but only five hundred positive instances, i.e., instances were cancer was actually present. Then we’ve an imbalanced dataset. This happens more often with datasets in real life as the chances of finding cancer among all the checks that happen or a fraudulent transaction among all the transactions that occur daily is comparatively low.\\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{class.png}
\caption{Sample output of a Classification problem.}
\label{fig:fig2}
\end{figure}
\newpage\textbf{Why does it matter if the dataset is skewed?} \\
When your dataset do not represent all classes of data equally, the model might overfit to the class that’s represented more in your dataset and become oblivious to the existence of the minority class. It might even give you a good accuracy but fail miserably in real life. In our example, a model that keeps predicting that there’s no cancer every single time will also have a good accuracy as the occurrence of cancer itself will be rare among the inputs. But it will fail when an actual case of cancer is subjected to classification, failing its original purpose. \\
\newline\textbf{Different ways to deal with an imbalanced dataset} \\
A widely adopted technique for dealing with highly unbalanced datasets is called resampling. Resampling is done after the data is split into training, test and validation sets. Resampling is done only on the training set or the performance measures could get skewed. Resampling can be of two types:
\begin{enumerate}
    \item Over-sampling.
    \item Under-sampling. 
\end{enumerate} 
Under sampling involves removing samples from the majority class and over-sampling involves adding more examples from the minority class . The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{under over.png}
\caption{Undersampling and Oversampling.}
\label{fig:fig2}
\end{figure}
\textbf{Another technique similar to upsampling is to create synthetic samples. Adding synthetic samples is also only done after the train-test split, into the training data.}
\subsubsection{Precision/Recall}
\boxed{\textbf{Note: y = 1 is the rarer class among the two.}} \\
In a binary classification, one of the following four scenarios may occur,
\begin{itemize}
    \item \textbf{True Positive (TP):} the model predicts 1 and the actual class is 1.
    \item \textbf{True Negative (TN):} the model predicts 0 and the actual class is 0.
    \item \textbf{False Positive (FP):} the model predicts 1 but the actual class is 0.
    \item \textbf{False Negative (FN):} the model predicts 0 but the actual class is 1.
\end{itemize}
\subsubsection{Confusion matrix}
\textbf{Confusion\ matrix} is a table that tells us how well our model has performed after it has been trained. \\
A confusion matrix is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. \\
\begin{table}[h!]
\begin{tabular}{ll|l|l|}
\cline{3-4}
          &  &   \multicolumn{2}{l|}{\quad\;\;\; Predicted} \\ \cline{3-4} 
          &  &   Negative       & Positive                  \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Actual}} & Negative & a & b \\ \cline{2-4} 
\multicolumn{1}{|c|}{}                        & Positive & c & d \\ \hline
\end{tabular}
\end{table}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.1\textwidth]{matrix.png}
\caption{Basic binary confusion matrix.}
\label{fig:fig2}
\end{figure}
\begin{align} 
    \text{Accuracy} &= \dfrac{TP + TN}{TP + TN + FP + FN} \\
    \text{Specificity} &= \dfrac{TN}{FP + TN} \\
    \textbf{Sensitivity} &= \dfrac{TN}{TN + FP}
\end{align}
Then, precision and recall can be defined as : \\
\begin{definition}
\textbf{Precision :} Of all patients where we predicted y = 1, what fraction actually has cancer?
\end{definition}
\begin{align}
    \text{Precision} &= \dfrac{TP}{TP + FP} 
\end{align}
\begin{definition}
\textbf{Recall :} Of all patients that actually have cancer, what fraction did we correctly detect as having cancer?
\end{definition}
\begin{align}
    \text{Recall} &= \dfrac{TP}{TP + FN}
\end{align}
\textbf{Recall defines of all the actual y = 1, which ones did the model predict correctly.} \\
\textbf{Now, if we evaluate a scenario where the classifier predicts all 0’s then the recall of the model will be 0, which then points out the inability of the system.} \\
\subsection{Trading of precision and recall}
\textbf{By changing the threshold value for the classifier confidence, one can adjust the precision and recall for the model.} \\
For example, in a logistic regression the threshold is generally at 0.5. If one increases it, we can be sure that of all the predictions made more will be correct, hence, high precision. But there are also higher chances of missing the positive cases, hence, the lower recall. \\
Similarly, if one decreases the threshold, then the chances of false positives increases, hence low precision. Also, there is lesser probability of missing the actual cases, hence high recall. \\
A precision-recall trade - off curve may look like one among the following,
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{Screenshot (132).png}
\caption{Trading of precision and recall.}
\label{fig:fig2}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{Screenshot (137).png}
\caption{Precision/Recall for different values of threshold hypothesis.}
\label{fig:fig2}
\end{figure} \newpage
\subsection{F Score}
Given two pairs of precision and recall, how to choose the better pair. One of the options would be to choose the one which higher average. That is not the ideal solution as the pair with (precision=0.02 and recall=1) has a better mean than the pair (precision= 0.5 and recall=0.4).
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{Screenshot (133).png}
\caption{Calculation of F score through precision/recall.}
\label{fig:fig2}
\end{figure}
Enter F Score or $F_1$ Score, which is the harmonic mean of precision and recall, defined as,
\begin{align}
    F_1 = 2\frac{P\times R}{P+R}
\end{align}
The above formula has advantage over the average method because, if either precision or recall is small, the the numerator product P∗R will weigh the F-Score low and consequently lead to choosing the better pair of precision and recall. So, \newpage
\begin{enumerate}
    \item If P=0 (or) R=0, then $F_1$=0.
    \item If P=1 and R=1, then $F_1$=1.
\end{enumerate}
\notte{\textbf{One reasonable way of automatically choosing threshold for classifier is to try a range of them on the cross-validation set and pick the one that gives the highest F-Score.}}
\subsubsection{Sensitivity/Specifivity}
\textbf{Apart from precision and recall, sensitivity and specifivity are among the most used error metrics in classfication.}
\begin{enumerate}
    \item \textbf{Sensitivity or True Positive Rate (TPR)} is another name for recall and is also called \textbf{hit rate.}
    \item \textbf{Specificity (SPC) or True Negative Rate.}
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{Screenshot (134).png}
\caption{Designing a high accuracy learning system.}
\label{fig:fig2}
\end{figure} \newpage
\subsubsection{Using Large Datasets}
For a high bias problem in the model, gathering more and more data will not help the model improve. \\
But under certain conditions, getting a lot of data and training on a certain type of training algorithm can be an effective way to improve the learning algorithm’s performance. \\
The following are the conditions that should be met for the above statement to hold true,
\begin{enumerate}
    \item The features, x, must have sufficient information to predict y
    accurately. One way to test this would be to check if human expert can make a confident predition using the features.
    \item Using a learning algorithm with a large number of parameters to learn (e.g. logistic regression, linear regression, neural network with many hidden units etc.). What this truly accomplishes is that these algorithms are \textbf{low bias algorithm} due to the large number of learnable parameters.
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{Screenshot (135).png}
\caption{Data Rationale}
\label{fig:fig2}
\end{figure} \newpage
In such settings, where the problem of high bias is removed by the virtue of highly parametrized learning algorithms, a large dataset ensures the \textbf{low variance.} Hence, under the listed settings a large number of dataset is almost always going to help improve the model performance.
\chapter{Support Vector Machine (SVM)}
\textbf{A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they're able to categorize new text.} \\
\newline\textbf{Compared to newer algorithms like neural networks, they have too main advantages: higher speed and better performance with a limited number of samples (in the thousands). This makes the algorithm very suitable for text classification problems,where it's common to have access to a dataset of at most a couple of thousands of tagged samples.}
A SVM is a discriminative classifier formally defined by a separating hyperplane. Given labeled training data, the algorithm outputs an optimal hyperplane which categorizes new examples. \\
\subsection{How does SVM work?}
\textbf{The linear SVM classifier works by drawing a straight line between two classes. All the data points that fall on one side of the line will be labeled as one class and all the points that fall on the other side will be labeled as the second. Sounds simple enough, but there’s an infinite amount of lines to choose from. How do we know which line will do the best job of classifying the data? This is where the LSVM algorithm comes in to play. The LSVM algorithm will select a line that not only separates the two classes but stays as far away from the closest samples as possible. In fact, the “support vector” in “support vector machine” refers to two position vectors drawn from the origin to the points which dictate the decision boundary.} \\
The basics of support vector machine and how it works are best understood with a simple example. Let's imagine we have two tags: red and blue, and our data has two features: x and y. We want a classifier that, given a pair of (x,y) coordinates, outputs if it’s either red or blue. We plot our already labeled training data on a plane: \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{data.png}
\caption{Our labeled data}
\label{fig:fig2}
\end{figure}
A support vector machine takes these data points and outputs the hyperplane (which in two dimensions it’s simply a line) that best separates the tags. This line is the \textbf{decision boundary}: anything that falls to one side of it we will classify as blue, and anything that falls to the other as red. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{plot_hyperplanes_2.png}
\caption{The best hyperplane is simply a line}
\label{fig:fig2}
\end{figure}
But, what exactly is the best hyperplane? For SVM, it’s the one that maximizes the margins from both tags. In other words: the hyperplane (remember it's a line in this case) whose distance to the nearest element of each tag is the largest. \\
\textbf{According to the SVM algorithm we find the points closest to the line from both the classes.These points are called support vectors. Now, we compute the distance between the line and the support vectors. This distance is called the margin. Our goal is to maximize the margin. The hyperplane for which the margin is maximum is the optimal hyperplane.} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{imp 1.png}
\caption{Not all hyperplanes are created equal}
\label{fig:fig2}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{imp 2.png}
\caption{More information regarding SVM and hyperplane}
\label{fig:fig2}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{hyper.png}
\end{figure} \newpage
\textbf{It is a very powerful classification algorithm to maximize the margin among class variables. This margin (support vector) represents the distance between the separating hyperplanes (decision boundary). The reason to have decision boundaries with large margin is to separate positive and negative hyperplanes with adjustable bias-variance proportion. The goal is to separate so that negative samples would fall under negative hyperplane and positive samples would fall under positive hyperplane. SVM is not as prone to outliers as it only cares about the points closest to the decision boundary. It changes its decision boundary depending on the placement of the new positive or negative events.} \\
\textbf{The decision boundary is much more important for Linear SVM’s – the whole goal is to place a linear boundary in a smart way. There isn’t a probabilistic interpretation of individual classifications, at least not in the original formulation.} \\
\textbf{Hence, key points are:}
\begin{enumerate}
    \item SVM try to maximize the margin between the closest support vectors whereas logistic regression maximize the posterior class probability.
    \item SVM is deterministic (but we can use Platts model for probability score) while LR is probabilistic.
    \item For the kernel space, SVM is faster.
\end{enumerate}
\begin{tabular}{|c|c|} \hline
Logistic Regression & Support Vector Machine \\ \hline
1. It is an algorithm used for solving & 1. It is a model used for both \\
 classification problems. & classification and linear regression. \\ \hline
2. It is not used to find the best margin, & 2. It tries to find the "best" margin (distance \\
 instead, it can have different decision & between the line and the support vectors) \\
  boundaries with different weights that are & that separates the classes and thus reduces \\
   near the optimal point. & the risk of error on the data. \\ \hline
3. It works with already identified identified & 3. It works well with unstructured and semi-  \\
  independent variable. &  structured data like text and images. \\ \hline
4. It is based on statistical approach. & 4. It is based on geometrical properties \\
 &  of the data.  \\ \hline
5. It is vulnerable to overfitting. &  5. The risk of overfitting is less in SVM. \\ \hline
\end{tabular}
\begin{figure}[!htb]
\centering 
\includegraphics[width = 1.0\textwidth]{Screenshot (184).png}
\caption{Cost function for SVM.}
\label{fig:fig3}
\end{figure}
\begin{figure}[!htb]
\centering 
\includegraphics[width = 1.0\textwidth]{Screenshot (183).png}
\caption{SVM Hypothesis.}
\label{fig:fig3}
\end{figure}
\newpage 
\subsection{SVM for Non-linear data}
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{small and large.png}
\caption{Difference between small and large margin.}
\label{fig:fig2}
\end{figure}
It’s pretty clear that there’s not a linear decision boundary (a single straight line that separates both tags). However, the vectors are very clearly segregated and it looks as though it should be easy to separate them.\\
This data is clearly not linearly separable. We cannot draw a straight line that can classify this data. But, this data can be converted to linearly separable data in higher dimension. Lets add one more dimension and call it z-axis. Let the co-ordinates on z-axis be governed by the constraint, \\
So here’s what we’ll do: we will add a third dimension. Up until now we had two dimensions: x and y. We create a new z dimension, and we rule that it be calculated a certain way that is convenient for us: z = $x^2 + y^2$ (you’ll notice that’s the equation for a circle). \\
This will give us a three-dimensional space. Taking a slice of that space, it looks like this: \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{z.png}
\caption{From a different perspective, the data is now in two linearly separated groups.}
\label{fig:fig2}
\end{figure}
Now, the SVM produces best hyperplane to the linearly separated data, \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{bh.png}
\caption{Best hyperplane is produced.}
\label{fig:fig2}
\end{figure} \newpage
Now the data is clearly linearly separable. Let the purple line separating the data in higher dimension be z=k, where k is a constant. Since, $z=x^2+y^2$ we get $x^2 + y^2 = k$; which is an equation of a circle. So, we can project this linear separator in higher dimension back in original dimensions using this transformation. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{non b h.png}
\caption{Decision boundary in original dimensions.}
\label{fig:fig2}
\end{figure} \\
And there we go! Our decision boundary is a circumference of radius 1, which separates both tags using SVM.\\
Thus we can classify data by adding an extra dimension to it so that it becomes linearly separable and then projecting the decision boundary back to original dimensions using mathematical transformation. But finding the correct transformation for any given dataset isn’t that easy. Thankfully, we can use kernels in sklearn’s SVM implementation to do this job.
\subsection{Hyperplane}
Now that we understand the SVM logic lets formally define the hyperplane. \\
\notte{A hyperplane in an n-dimensional Euclidean space is a flat, n-1 dimensional subset of that space that divides the space into two disconnected parts.} \\
\textbf{For example let’s assume a line to be our one dimensional Euclidean space(i.e. let’s say our datasets lie on a line). Now pick a point on the line, this point divides the line into two parts. The line has 1 dimension, while the point has 0 dimensions. So a point is a hyperplane of the line.} \\
\textbf{For two dimensions we saw that the separating line was the hyperplane. Similarly, for three dimensions a plane with two dimensions divides the 3d space into two parts and thus act as a hyperplane. Thus for a space of n dimensions we have a hyperplane of n-1 dimensions separating it into two parts.} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.35\textwidth]{p h.png}
\caption{Possible hyperplanes.}
\label{fig:fig2}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{h 3d.png}
\caption{Hyperplanes in 2D and 3D feature space.}
\label{fig:fig2}
\end{figure}
\subsection{Tuning parameters}
Parameters are arguments that you pass when you create your classifier. Following are the important parameters for SVM-
\begin{enumerate}
    \item \textbf{C($\frac{1}{\lambda}$):} \\
    It controls the trade off between smooth decision boundary and classifying training points correctly. A large value of c means you will get more training points correctly. \\
    \begin{figure}[!htb]
    \centering
    \includegraphics[width = 0.35\textwidth]{c.png}
    \caption{Smooth decision boundary vs classifying all points correctly.}
    \label{fig:fig2}
    \end{figure}  \\ 
    Consider an example as shown in the figure \ref{fig:fig2}. There are a number of decision boundaries that we can draw for this dataset. Consider a straight (green colored) decision boundary which is quite simple but it comes at the cost of a few points being misclassified. These misclassified points are called outliers. We can also make something that is considerably more wiggly(sky blue colored decision boundary) but where we get potentially all of the training points correct. Of course the trade off having something that is very intricate, very complicated like this is that chances are it is not going to generalize quite as well to our test set. So something that is simple, more straight maybe actually the better choice if you look at the accuracy. Large value of c means you will get more intricate decision curves trying to fit in all the points. Figuring out how much you want to have a smooth decision boundary vs one that gets things correct is part of artistry of machine learning. So try different values of c for your dataset to get the perfectly balanced curve and avoid over fitting.
    \item \textbf{Gamma:} \\
    It defines how far the influence of a single training example reaches. If it has a low value it means that every point has a far reach and conversely high value of gamma means that every point has close reach. \\
    If gamma has a very high value, then the decision boundary is just going to be dependent upon the points that are very close to the line which effectively results in ignoring some of the points that are very far from the decision boundary. This is because the closer points get more weight and it results in a wiggly curve as shown in previous graph.On the other hand, if the gamma value is low even the far away points get considerable weight and we get a more linear curve.
\end{enumerate}
\subsection{Algorithm}
Suppose, we had a vector w which is always normal to the hyperplane (perpendicular to the line in 2 dimensions). We can determine how far away a sample is from our decision boundary by projecting the position vector of the sample on to the vector w. As a quick refresher, the dot product of two vectors is proportional to the projection of the first vector on to the second.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.35\textwidth]{dot.png}
\caption{Projection vector.}
\label{fig:fig2}
\end{figure}  \\
If it’s a positive sample, we’re going to insist that the proceeding decision function (the dot product of w and the position vector of a given sample plus some constant) returns a value greater than or equal to 1.
\begin{align}
    \Vec{w}.\Vec{x}_+ + b \geq 1.
\end{align} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.35\textwidth]{dot plane.png}
\caption{Dot product representation for '+' samples.}
\label{fig:fig2}
\end{figure}  \\ \newpage
Similarly, if it’s a negative sample, we’re going to insist that the proceeding decision function returns a value smaller than or equal to -1.
\begin{align}
    \Vec{w}.\Vec{x}_- + b \leq -1.
\end{align} \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.35\textwidth]{dot plane 1.png}
\caption{Dot product representation for '-' samples.}
\label{fig:fig2}
\end{figure}  \\
In other words, we won’t consider any samples located between the decision boundary and support vectors. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.35\textwidth]{dot plane 2.png}
\label{fig:fig2}
\end{figure} 
we introduce an additional variable stickily for convenience. The variable y will be equal to positive one for all positive samples and negative one for all negative samples. \\
\begin{center}
$y_i$\begin{cases}
     +1\ for\ +\ samples. \\
     -1\ for\ -\ samples.
\end{cases}
\end{center}
After multiplying by y, the equations for the positive and negative samples are equal to one another. \\
\begin{align*}
    y_i(\Vec{w}.\Vec{x}_+ + b) &\geq 1. \\
    y_i(\Vec{w}.\Vec{x}_- + b) &\leq 1.
\end{align*}
Meaning, we can simplify the constraints down to a single equation. \\
\begin{align*}
    y_i(\Vec{w}.\Vec{x}_i + b) - 1 = 0.
\end{align*} \\
Next, we need to address the process by which we go about maximizing the margin. To get an equation for the width of the margin, we subtract the first support vector from the one below it and the multiply the result by the unit vector of w which is always perpendicular to the decision boundary.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.35\textwidth]{width.png}
\caption{Width of the lines produced by samples.}
\label{fig:fig2}
\end{figure}
\begin{align*}
    width = (\Vec{x_+} - \Vec{x_-}) . \frac{\Vec{w}}{\| w\| }
\end{align*}
Using the constraints from above and a bit of algebra, we get the following.
\begin{align*}
    y_i(\Vec{w}.\Vec{x_i} + b) - 1 &= 0. \\
    (1)(\Vec{w}.\Vec{x}_+ + b) - 1 &= 0. \\
    \Vec{w}.\Vec{x}_+ = 1 - b \tag{1} \\
    (-1)(\Vec{w}.\Vec{x}_- + b) - 1 &= 0. \\
    \Vec{w}.\Vec{x}_- = -1 - b \tag{2} \\
    width = ((\Vec{x_+}.\Vec{w}) - (\Vec{x_-}.\Vec{w}))&\frac{1}{\| w\|} \\
    width = ((1-b) - (-1-b))&\frac{1}{\| w\|} \\
    width &= \frac{2}{\| w\|}
\end{align*} \\
Therefore, in order to select the optimal decision boundary, we must maximize the equation we just computed. We apply a few more tricks before proceeding.
(refer to the $\underline{\href{https://www.youtube.com/watch?v=_PwhiWxHK8o}{MIT Lecture}}$). \\
\begin{center}
    \begin{align*}
        \max&\frac{2}{\| w\|} \\
        \textbf{\text{which is &proportional to:}} \\
        \max&\frac{1}{\| w\|} \\
        \textbf{\text{which is &equivalent to:}} \\
        \min&\| w\| \\
        \textbf{\text{we're going to cheat a little and use:}} \\
        \min\frac{1}{2}{\| w\|}^2\;\;\;\;\;\; &\textbf{\text{since}} \;\;\;\;\;\; \diff{1}{x}\frac{1}{2}{x}^2 = x
    \end{align*}
\end{center} \\
Now, in most machine learning algorithms, we’d use something like gradient descent to minimize said function, however, for support vector machines, we use the Lagrangian.In essence, using Lagrangian, we can solve for the global minimum like we’d do in high school level calculus (i.e. take the derivative of the function and make it equal to zero). The Lagrange tells us to subtract the cost function by the summation over all the constraints where each of those constraints will be multiplied by some constant alpha (normally written as lambda for the Lagrangian). \\
\begin{align*}
    L = \frac{1}{2}{\| w\|}^2 - &\sum_{i}^{n} \alpha_i [y_i(\Vec{w}.\Vec{x} + b) - 1] \\
    \frac{\partial L}{\partial w} = \Vec{w} - &\sum_{i}^{n} \alpha_i y_i x_i = 0\\
    \boxed{\Vec{w} = \sum_{i}^{n} \alpha_i y_i x_i} \\
    \frac{\partial L}{\partial b} = &-\sum_{i}^{n} \alpha_i y_i = 0 \\
    \boxed{\sum_{i}^{n} \alpha_i y_i = 0}
\end{align*}
Then, we perform some more algebra, plugging the equations we found in the previous step back into the original equation.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{equations.png}
\label{fig:fig2}
\end{figure} \\
Before we can proceed any further, we need to express the equation in terms of matrices instead of summations. The reason being, the \boxed{qp} function from the \underline{\href{https://cvxopt.org/userguide/coneprog.html#quadratic-programming}{CVXOPT}} library, which we’ll use to solve the Lagrangian, accepts very specific arguments. Thus, we need to go from: \\
\textbf{As this is still beyond your capability. Let us stop it here. But, I will provide link for futher understandings.} \\
\begin{center}
    \underline{\href{https://towardsdatascience.com/support-vector-machine-python-example-d67d9b63f1c8}{Machine Learning with python.}}
\end{center}
\subsection{The Kernel}
In our example we found a way to classify nonlinear data by cleverly mapping our space to a higher dimension. However, it turns out that calculating this transformation can get pretty computationally expensive: there can be a lot of new dimensions, each one of them possibly involving a complicated calculation. Doing this for every vector in the dataset can be a lot of work, so it’d be great if we could find a cheaper solution. \\
And we’re in luck! Here’s a trick: SVM doesn’t need the actual vectors to work its magic, it actually can get by only with the dot products between them. This means that we can sidestep the expensive calculations of the new dimensions.\\
This is what we do instead:
\begin{enumerate}
    \item Imagine the new space we want: \\
    $z = x^2 + y^2.$
    \item Figure out what the dot product in that space looks like: \\
    a·b = xa·xb + ya·yb + za·zb \\
    a·b = xa·xb + ya·yb + (xa² + ya²)·(xb² + yb²)
    \item Tell SVM to do its thing, but using the new dot product — we call this a \textbf{kernel function.}
\end{enumerate}
That’s it! That’s the kernel trick, which allows us to sidestep a lot of expensive calculations. Normally, the kernel is linear, and we get a linear classifier. However, by using a nonlinear kernel (like above) we can get a nonlinear classifier without transforming the data at all: we only change the dot product to that of the space that we want and SVM will happily chug along.\\
Note that the kernel trick isn’t actually part of SVM. It can be used with other linear classifiers such as logistic regression. A support vector machine only takes care of finding the decision boundary. 
\subsection{Differences between SVM and Logistic Regression}
\begin{definition}
\textbf{Logistic Regression:} Logistic regression is an algorithm that is used in solving classification problems. It is a predictive analysis that describes data and explains the relationship between variables. Logistic regression is applied to an input variable (X) where the output variable (y) is a discrete value which ranges between 1 (yes) and 0 (no).
\end{definition} \\
\begin{definition}
\textbf{Support Vector Machine:} The support vector machine is a model used for both classification and regression problems though it is mostly used to solve classification problems. The algorithm creates a hyperplane or line(decision boundary) which separates data into classes. It uses the kernel trick to find the best line separator (decision boundary that has same distance from the boundary point of both classes). It is a clear and more powerful way of learning complex non linear functions.
\end{definition}\\ %\newpage
%\begin{table}
%\centering 
\newline\begin{tabular}{|c|c|} \hline
Problems that can solved & Problems that can solved using SVM \\
    using Logistic Regression  & \\ \hline
1.Cancer detection — can be used to detect  & 1.Image classification.        \\
if a patient have cancer (1) or not(0). &  \\ \hline
2.Test score — predict if a student passed(1)  & 2. Recognizing handwriting.    \\ or failed(0) a test. &      \\  \hline
3.Marketing — predict if a customer will           & 3. Cancer Detection \\ purchase a product(1) or not(0). &  \\ \hline
\end{tabular}
%\end{table}
\newline\newline\textbf{\underline{Differences}}
\begin{enumerate}
    \item SVM tries to finds the “best” margin (distance between the line and the support vectors) that separates the classes and this reduces the risk of error on the data, while logistic regression does not, instead it can have different decision boundaries with different weights that are near the optimal point. 
    \begin{figure}[!htb]
    \centering
    \includegraphics[width = 0.5\textwidth]{diff.png}
    \label{fig:fig2}
     \end{figure}
     \item SVM works well with unstructured and semi-structured data like text and images while logistic regression works with already identified independent variables.
     \item SVM is based on geometrical properties of the data while logistic regression is based on statistical approaches.
     \item The risk of overfitting is less in SVM, while Logistic regression is vulnerable to overfitting.
\end{enumerate}
\textbf{\underline{When To Use Logistic Regression vs Support Vector Machine}}\\ \newline
Depending on the number of training sets (data)/features that you have, you can choose to use either logistic regression or support vector machine. \\
Lets take these as an example where : \\
n = number of features, \\
m = number of training examples. \\
\begin{enumerate}
    \item  If n is large (1–10,000) and m is small (10–1000) : use logistic regression or SVM with a linear kernel.
    \item  If n is small (1–10 00) and m is intermediate (10–10,000) : use SVM with (Gaussian, polynomial etc) kernel.
    \item If n is small (1–10 00), m is large (50,000–1,000,000+): first, manually add more features and then use logistic regression or SVM with a linear kernel.
\end{enumerate}
Generally, it is usually advisable to first try to use logistic regression to see how the model does, if it fails then you can try using SVM without a kernel (is otherwise known as SVM with a linear kernel). Logistic regression and SVM with a linear kernel have similar performance but depending on your features, one may be more efficient than the other. \\
\notte{Logistic regression and SVM are great tools for training classification and regression problems. It is good to know when to use either of them so as to save computational cost and time.}
\notte{\textbf{The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Also, the dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceeds 3.Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.}}
\subsection{Optimization Objective}
The support vector machine objective can seen as a modification to the cost of logistic regression. Consider the sigmoid function, given as,
\begin{align}
    h_\theta(x) = \frac{1}{1 + e^{-z}}
\end{align} \\
where $z = \theta^T x.$ \\
The cost function of logistic regression as in the post Logistic Regression Model, is given by,
\begin{align}
J(\theta) &= -{1 \over m} \sum_{i=1}^m \left( y^{(i)}\,log(h_\theta(x^{(i)}) + (1-y^{(i)})\,log(1 - h_\theta(x^{(i)})) \right) \\
    &= -{1 \over m} \sum_{i=1}^m \left( y^{(i)}\,log(\frac {1} {1 + e^{-\theta^T x}}) + (1-y^{(i)})\,log(1 - \frac {1} {1 + e^{-\theta^T x}}) \right) \label{2}
\end{align}
Each training instance contributes to the cost function the following term,
\begin{align*}
    -y\,log(\frac {1} {1 + e^{-z}}) - (1-y)\,log(1 - \frac {1} {1 + e^{-z}})
\end{align*}
So when y=1, the contributed term is $-log(\frac {1} {1 + e^{-z}})$, which can be seen in the plot below. The cost function of SVM, denoted as $cost_1(z)$, is a modification the former and a close approximation. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{SVM at y=1.png}
\caption{SVM Cost function at y = 1.}
\label{fig:fig2}
\end{figure} \newpage
Similarly, when y=0, the contributed term is $-log(1 - \frac {1} {1 + e^{-z}})$, which can be seen in the plot below. The cost function of SVM, denoted as $cost_0(z)$, is a modification the former and a close approximation.\\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{SVM at y=0.png}
\caption{SVM Cost function at y = 0.}
\label{fig:fig2}
\end{figure}
\notte{While the slope the straight line is not of as much importance, it is the linear approximation that gives SVMs computational advantages that helps in formulating an easier optimization problem.}
Regularized version of eq:\ref{2} can from the post Regularized Logistic Regression can rewritten as, \\
\begin{align}
    J(\theta) = {1 \over m} \sum_{i=1}^m \left( y^{(i)}\,(-log(h_\theta(x^{(i)}))) + (1-y^{(i)})\,(-log(1 - h_\theta(x^{(i)}))) \right) + {\lambda \over 2m } \sum_{j=1}^n \theta_j^2  \label{3}
\end{align}
In order to come up with the cost function for the SVM, eq:\ref{3} is modified by replacing the corresponding cost terms, which gives,
\begin{align}
    J(\theta) = {1 \over m} \sum_{i=1}^m \left( y^{(i)}\,cost_1(z) + (1-y^{(i)})\,cost_0(z) \right) + {\lambda \over 2m } \sum_{j=1}^n \theta_j^2 \label{4}
\end{align}
Following the conventions of SVM the following modifications are made to the cost in eq:\ref{4}, which effectively is a change in notation but not the underlying logic,
\begin{enumerate}
    \item Removing $\frac{1}{m}$ does not affect the minimization logic at all as the minima of a function is not changed by the linear scaling.
    \item Change the form of parameterization from $A+\lambdaB$ to CA+B where it can be intuitively thought that $C=\frac{1}{\lambda}.$
\end{enumerate}
After applying the above changes, eq:\ref{4} gives,
\begin{align}
    J(\theta) = C \sum_{i=1}^m \left[ y^{(i)}\,cost_1(\theta^T x^{(i)}) + (1-y^{(i)})\,cost_0(\theta^T x^{(i)}) \right] + {1 \over 2 } \sum_{j=1}^n \theta_j^2. \label{5}
\end{align}
The SVM hypothesis does not predict probability, instead gives hard class labels,
\begin{align*}
    h_\theta(x) = 
\begin{cases}
1 \text{, if } \theta^Tx \geq 0 \\
0 \text{, otherwise}
\end{cases}
\end{align*}
\subsection{Large Margin Intuition}
In logistic regression, we take the output of the linear function and squash the value within the range of [0,1] using the sigmoid function. If the squashed value is greater than a threshold value(0.5) we assign it a label 1, else we assign it a label 0. In SVM, we take the output of the linear function and if that output is greater than 1, we identify it with one class and if the output is -1, we identify is with another class. Since the threshold values are changed to 1 and -1 in SVM, we obtain this reinforcement range of values([-1,1]) which acts as margin.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{large.png}
\caption{SVM Cost function plots.}
\label{fig:fig3}
\end{figure} \newpage
According to \ref{5} and the plots of the cost function as shown in the image above, the following are two desirable states for SVM,
\begin{enumerate}
    \item If y=1, then $\Theta^{\top}x \geq1$ (not just $\geq0$).
    \item If y=0, then $\Theta^{\top}x \geq-1$ (not just <0).
\end{enumerate}
Let C in eq:\ref{5} be a large value. Consequently, in order to minimize the cost, the corresponding term $\sum_{i=1}^m \left[ y^{(i)}\,cost_1(\theta^T x^{(i)}) + (1-y^{(i)})\,cost_0(\theta^T x^{(i)}) \right]$ must be close to 0. \\
Hence, in order to minimize the cost function, when y=1, $cost_1(\theta^T x)$ should be 0, and similarly, when y=0, $cost_0(\theta^T x)$ should be 0. And thus, from the plots in fig:\ref{fig:fig3}, it is clear that it can only fulfilled by the two states listed above.\\
Following the above intuition, the cost function can we written as,
\begin{align}
    min_\theta J(\theta) = min_\theta {1 \over 2 } \sum_{j=1}^n \theta_j^2 \label{6}
\end{align}
subject to contraints,
\begin{align*}
\theta^{\top}x^{(i)} &\geq 1 \text{, if } y^{(i)}=1 \\
\theta^{\top}x^{(i)} &\leq -1 \text{, if } y^{(i)}=0
\end{align*}
What this basically leads to is the selection of a decision boundary that tries to maximize the margin from the support vectors as shown in the plot below. This maximization of the margin as seen for decision boundary A increases the robustness over decision boundaries with lesser margins like B. And it is this property of the SVMs that attributes the name \textbf{large margin classifier} to it.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{large-margin-decision-boundary.png}
\caption{Large Margin Decision Boundary.}
\label{fig:fig3}
\end{figure} \newpage
\textbf{Effect of Parameter C}
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{effect-of-regularization.png}
\caption{Effect of Parameter C.}
\label{fig:fig3}
\end{figure} \\
The effect of C can be considered as reciprocal of regularization parameter, 
$\lambda$. This is more clear from Fig-5. A single outlier, can make the model choose the decision boundary with smaller margin if the value of C is large. A small value of C ensures that the outliers are overlooked and best approximation of large margin boundary is determined.
\subsection{Mathematical Background for SVM}
Vector Inner Product: Consider two vectors, v and w, given by, \\
\begin{center}
\begin{math}
v = \begin{bmatrix}v_1 \\ v_2 \end{bmatrix} \\
w = \begin{bmatrix}w_1 \\ w_2 \end{bmatrix}
\end{math} 
\end{center}\\
Then, the inner product or the dot product is defined as $v^{\top}w = w^{\top}v.$ \\
Norm of a vector, v, denoted as $\| v\|$ is the euclidean length of the vector given by the pythagoras theorem as,
\begin{align}
    \| v\| = \sqrt{\sum_{i=0}^n v_{i}^2}\; \epsilon\; \Re \label{7}
\end{align}
The inner product can also be defined as,
\begin{align}
\text{Inner Product(v, w)} &= v^{\top}w = w^{\top}v = \sum_{i=0}^n v_i .w_i \\
    &= \| v\|. \| w\|cos\theta = p.\| v\| \label{8}
\end{align}
where $\| w\|.cos(\theta)$ can be described as the projection of vector w onto vector v which can be either positive or negative signed based on the angle $\theta$ between the vectors as shown in the image below. \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{dot-product.png}
\caption{Dot Product.}
\label{fig:fig3}
\end{figure}
\textbf{SVM Decision Boundary:} From \ref{6}, the optimization statement can be written as,
\begin{align}
    min_\theta \, {1 \over 2 } \sum_{j=1}^n \theta_j^2 \label{9}
\end{align}
subject to contraints,
\begin{align}
\theta^{\top}x^{(i)} &\geq 1 \text{, if } y^{(i)}=1 \\
\theta^{\top}x^{(i)} &\leq -1 \text{, if } y^{(i)}=0 \label{10}
\end{align}
Let $\theta_0$=0 and n=2, i.e. number of features is 2 for simplicity, then \ref{9} can be written as,
\begin{align}
    min_\theta \, \frac{1}{2} (\theta_1^2 + \theta_1^2) = {1 \over 2 } \sqrt{(\theta_1^2 + \theta_1^2)}^2 =  \frac{1}{2} \| \theta \|^2 \label{11}
\end{align}
Using \ref{8}, $\theta^{\top}x^{(i)}$ in \ref{10} can be written as,
\begin{align}
    \theta^{\top}x^{(i)} = p^{(i)}. \| \theta \| \label{12}
\end{align}
The plot of \ref{12} can be seen below, \\
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{dot-product-in-svm.png}
\caption{Dot Product in SVM.}
\label{fig:fig3}
\end{figure} \newpage
Hence, using \ref{11} and \ref{12}, the optimization objective in \ref{9} and the constraints in \ref{10} are written as,
\begin{align}
    min_\theta \, \frac{1}{2} \| \theta\|^2 \label{13}
\end{align}
subject to contraints,
\begin{align}
p^{(i)}. \| \theta\| &\geq 1 \text{, if } y^{(i)}=1 \\
p^{(i)}. \| \theta\| &\leq -1 \text{, if } y^{(i)}=0 \label{14}
\end{align}
where $p^{(i)}$ is the projection of $x^{(i)}$ onto vector $\theta$.\\
Consider two decision boundaries, A and B, and their respective perpendicular parameters, $\theta_A$ and $\theta_B$ as shown in the plot below. As a consequence of choosing $\theta_0=0$ for simplification, all the corresponding decision boundaries pass through the origin.
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.5\textwidth]{choosing-large-margin.png}
\caption{ Choosing Large Margin Classifier.}
\label{fig:fig3}
\end{figure} \\
Based on the two training examples of either class chosen, close to the boundaries, it can be seen that the magnitude of projection is more in case of $\theta_B$ than $\theta_A$. This basically tells that it would be possible to choose smaller values of $\theta$ and satisfy \ref{13} and \ref{14} if the value of projection p is bigger and hence, the decision boundary, B is more favourable to the optimization objective. \\
\textbf{Why is decision boundary perpendicular to the $\theta$?} \\
Consider two points $x_1$ and $x_2$ on the decision boundary given by,
\begin{align}
    \theta\,x + c= 0 \label{15}
\end{align}
Since the two points are on the line, they must satisfy \ref{15}. Substitution leads to the following,
\begin{align}
    \theta\,x_1 + c= 0 \label{16} \\
    \theta\,x_2 + c= 0 \label{17}
\end{align}
Subtracting \ref{17} from \ref{16},
\begin{align}
    \theta\,(x_1 - x_2) = 0 \label{18}
\end{align}
Since $x_1$ and $x_2$ lie on the line, the vector $(x_1 - x_2)$ is on the line too. Following the property of orthogonal vectors, \ref{18} is possible only if $\theta$ is orthogonal or perpendicular to $(x_1 - x_2)$, and hence perpendicular to the decision boundary.
\subsection{Kernels in SVM}
When dealing with non-linear decision boundaries, a learning method like logistic regression relies on high order polynomial features to find a complex decision boundary and fit the dataset, i.e. predict y=1 if,
\begin{align}
    \theta_0\,f_0 + \theta_1\,f_1 + \theta_2\,f_2 + \theta_3\,f_3 + \cdots \geq 0 \label{19}
\end{align}
where $f_0 = x_0,\, f_1=x_1,\, f_2=x_2,\, f_3=x_1x_2,\, f_4=x_1^2,\, \cdots.$\\
A natural question that arises is if there are choices of better/different features than in \ref{19}? A SVM does this by picking points in the space called \textbf{landmarks} and defining functions called \textbf{similarity} corresponding to the landmarks.
\begin{figure}[!htb]
\centering 
\includegraphics[width = 0.5\textwidth]{svm-landmarks.png}
\caption{SVM Landmarks.}
\label{fig:fig3}
\end{figure}\\ \newpage
\textbf{How many landmarks do we need?}\\
It might surprise you that given m training samples, the location of landmarks is exactly the location of your m training samples.
\begin{figure}[!htb]
\centering 
\includegraphics[width = 1.0\textwidth]{l-m.png}
\caption{Number of Landmarks.}
\label{fig:fig3}
\end{figure}\\
That is saying Non-Linear SVM recreates the features by comparing each of your training sample with all other training samples. Thus the number of features for prediction created by landmarks is the the size of training samples. For a given sample, we have updated features as below:
\begin{figure}[!htb]
\centering 
\includegraphics[width = 0.8\textwidth]{f-i.png}
\caption{f and k relation.}
\label{fig:fig3}
\end{figure}\\ \newpage
Say, there are three landmarks defined, $l^{(1)}, l^{(2)} and\ l^{(3)}$ as shown in the plot above, the for any given x, $f_1, f_2 and\ f_3$ are defined as follows,
\begin{align}
f_1 &= similarity(x, l^{(1)}) = exp \left(- \frac {\| x - l^{(1)} \|^2} {2 \sigma^2} \right) \\
f_2 &= similarity(x, l^{(2)}) = exp \left(- \frac {\| x - l^{(2)} \|^2} {2 \sigma^2} \right) \\ \label{20}
f_3 &= similarity(x, l^{(3)}) = exp \left(- \frac {\| x - l^{(3)} \|^2} {2 \sigma^2} \right) \\
    & \vdots 
\end{align}
Here, the similarity function is mathematically termed a \textbf{kernel.} The specific kernel used in \ref{20} is called the \textbf{Guassian Kernel}. Kernels are sometimes also denoted as $k(x, l^{(i)})$\\
Consider $f_1$ from \ref{20}. If there exists x close to landmark $l^{(1)}$, then $\|x-l^{(1)}\| \approx 0$ and hence, $f_1 \approx 1$. Similarly for a x far from the landmark, $\|x-l^{(1)}\|$ will be a larger value and hence exponential fall will cause $f_1 \approx 0$. So effectively the choice of landmarks has helped in increasing the number of features x had from 2 to 3. which can be helpful in discrimination.\\
For a gaussian kernel, the value of $\sigma$ defines the spread of the normal distribution. If $\sigma$ is small, the spread will be narrower and when its large the spread will be wider.\\
Also, the intuition is clear about how landmarks help in generating the new features. Along with the values of parameter, $\theta$ and $\sigma$, various different decision boundaries can be achieved.\\
\textbf{How to choose optimal landmarks?}\\
In a complex machine learning problem it would be advantageous to choose a lot more landmarks. This is generally acheived by choosing landmarks at the point of the training examples, i.e. landmarks equal to the number of training examples are chosen, ending up in $l^{(1)}, l^{(2)}, \cdots, l^{(m)}$ if there are m training examples. This translates to the fact that each feature is a measure of how close is an instance to the existing points of the class, leading to generation of new feature vectors. \\
\begin{figure}[!htb]
\centering 
\includegraphics[width = 1.0\textwidth]{Screenshot (181).png}
\caption{Kernels and Similarity.}
\label{fig:fig3}
\end{figure}
\begin{figure}[!htb]
\centering 
\includegraphics[width = 0.9\textwidth]{Screenshot (182).png}
\caption{Guassian distribution.}
\label{fig:fig3}
\end{figure}
\notte{\textbf{For SVM training, given training examples, x, features f are computed, and y =1, if $\theta^{\top} f \geq 0.$}}
The training objective from \ref{5} is modified as follows,
\begin{align}
    min_\theta \, C \sum_{i=1}^m \left[ y^{(i)}\,cost_1(\theta^T f^{(i)}) + (1-y^{(i)})\,cost_0(\theta^T f^{(i)}) \right] + {1 \over 2 } \sum_{j=1}^m \theta_j^2 \label{21}
\end{align}
In this case, n=m in \ref{5} by the virtue of procedure used to choose f.\\
\notte{\textbf{The regularization term in \ref{21} can be written as $\theta^{\top} \theta$. But in practice most SVM libraries, instead $\theta^{\top} M\theta$
, which can be considered a scaled version is used as it gives certain optimization benefits and scaling to bigger training sets, which will be taken up at a later point in maybe another post.}}
While the kernels idea can be applied to other algorithms like logistic regression, the computational tricks that apply to SVMs do not generalize as well to other algorithms.
\notte{\textbf{Hence, SVMs and Kernels tend to go particularly well together.}}
\subsection{Bias/Variance}
\begin{enumerate}
    \item $c=\frac{1}{\lambda}$,
    \begin{itemize}
        \item \textbf{Large C:} Low bias, High Variance.
        \item \textbf{Small C:} High bias, Low Variance.
    \end{itemize}
    \item $\sigma$,
    \begin{itemize}
        \item \textbf{Large $\sigma^2$:} High Bias, Low Variance (Features vary more smoothly).
        \item \textbf{Small $\sigma^2$:} Low Bias, High Variance (Features vary less smoothly).
    \end{itemize}
\end{enumerate}
\subsection{Choice of Kernels}
\begin{enumerate}
    \item \textbf{Linear Kernel:} is equivalent to a no kernel setting giving a standard linear classifier given by,
    \begin{align}
        \theta_0\,x_0 + \theta_1\,x_1 + \theta_2\,x_2 + \theta_3\,x_3 + \cdots \geq 0 \label{22}
    \end{align}
    Linear kernels are used when the number of training data is less but the number of features in the training data is huge.
    \item Gaussian Kernel: Make a choice of $\sigma^2$ to adjust the bias/variance trade-off. \\
    Gaussian kernels are generally used when the number of training data is huge and the number of features are small.
    \notte{\textbf{Feature scaling is important when using SVM, especially Gaussian Kernels, because if the ranges vary a lot then the similarity feature would be dominated by features with higher range of values.}}
    \notte{\textbf{All the kernels used for SVM, must satisfy Mercer’s Theorem, to make sure that SVM optimizations do not diverge.}}\\
    Some other kernels known to be used with SVMs are:
    \begin{itemize}
        \item Polynomial kernels, $k(x,l)=(x^{\top}l+constant)^{degree}.$ \item Esoteric kernels, like string kernel, chi-square kernel, histogram intersection kernel, ...
    \end{itemize}
\end{enumerate}
\subsection{Multi-Class Classification}
\begin{enumerate}
    \item Most SVM libraries have multi-class classification.
    \item Alternatively, one may use one-vs-all technique to train k different SVMs and pick class with largest $\theta^{\top}x.$
\end{enumerate}
\textbf{Logistic Regression vs SVM}
\begin{enumerate}
    \item If n is large relative to m, use logistic regression or SVM with linear kernel, like if n = 10000, m = 10-1000.
    \item If n is small and m is intermediate, use SVM with gaussian kernel, like if n = 1-1000, m = 10-10000.
    \item If n is small and m is large, create/add more features, then use logistic regression or SVM with no kernel, as with huge datasets SVMs struggle with gaussian kernels, like if n = 1-1000, m = 50000+.
\end{enumerate}
\notte{\textbf{Logistic Regression and SVM without a kernel (with linear kernel) generally give very similar. A neural network would work well on these training data too, but would be slower to train.}}
Also, the optimization problem of SVM is a convex problem, so the issue of getting stuck in local minima is non-existent for SVMs.
\subsection{Cost function for SVM}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.4\textwidth]{h-t.png}
\caption{Hypothesis and Regularized Cost Function.}
\label{fig:fig3}
\end{figure}\\
\notte{\textbf{Like Logistic Regression, SVM’s cost function is convex as well. The most popular optimization algorithm for SVM is Sequential Minimal Optimization that can be implemented by ‘libsvm’ package in python. SMO solves a large quadratic programming(QP) problem by breaking them into a series of small QP problems that can be solved analytically to avoid time-consuming process to some degree. In terms of detailed calculations, It’s pretty complicated and contains many numerical computing tricks that makes computations much more efficient to handle very large training datasets.}}
\notte{\textbf{In summary, if you have large amount of features, probably Linear SVM or Logistic Regression might be a choice. If you have small number of features (under 1000) and not too large size of training samples, SVM with Gaussian Kernel might work for you data well.}}
\subsection{What are the advantages of Artificial Neural Networks over Support Vector Machines?}
Both Support Vector Machines (SVMs) and Artificial Neural Networks (ANNs) are supervised machine learning classifiers. An ANN is a parametric classifier that uses hyper-parameters tuning during the training phase. An SVM is a non-parametric classifier that finds a linear vector (if a linear kernel is used) to separate classes. Actually, in terms of the model performance, SVMs are sometimes equivalent to a shallow neural network architecture. Generally, an ANN will outperform an SVM when there is a large number of training instances, however, neither outperforms the other over the full range of problems.\\
We can summarize the advantages of the ANN over the SVM as follows:
ANNs can handle multi-class problems by producing probabilities for each class. In contrast, SVMs handle these problems using independent one-versus-all classifiers where each produces a single binary output. For example, a single ANN can be trained to solve the hand-written digits problem while 10 SVMs (one for each digit) are required.\\
Another advantage of ANNs, from the perspective of model size, is that the model is fixed in terms of its inputs nodes, hidden layers, and output nodes; in an SVM, however, the number of support vector lines could reach the number of instances in the worst case.\\
The SVM does not perform well when the number of features is greater than the number of samples. More work in feature engineering is required for an SVM than that needed for a multi-layer Neural Network.\\
On the other hand, SVMs are better than ANNs in certain respects:\\
In comparison to SVMs, ANNs are more prone to becoming trapped in local minima, meaning that they sometime miss the global picture.\\
While most machine learning algorithms can overfit if they don’t have enough training samples, ANNs can also overfit if training goes on for too long - a problem that SVMs do not have.\\
SVM models are easier to understand. There are different kernels that provide a different level of flexibilities beyond the classical linear kernel, such as the Radial Basis Function kernel (RBF). Unlike the linear kernel, the RBF can handle the case when the relation between class labels and attributes is nonlinear.\\
For a review of the practical results, the following link to benchmarks of the performance of SVMs and ANNs on a variety datasets is useful:
\begin{center}
    \underline{\href{https://web.archive.org/web/20120304030602/http://indiji.com/svm-vs-nn.html}{Support Vector Machine vs Artificial Neural Networks.}}
\end{center}
\chapter{Unsupervised Learning}
Unsupervised learning is a type of machine learning in which models are trained using unlabeled dataset and are allowed to act on that data without any supervision.\\
\textbf{Unsupervised learning works by analyzing the data without its labels for the hidden structures within it, and through determining the correlations, and for features that actually correlate two data items. It is being used for clustering, dimensionality reduction, feature learning, density estimation, etc.}\\
\textbf{\textit{\textsc{Unsupervised learning is a type of algorithm that learns patterns from untagged data. The hope is that, through mimicry, the machine is forced to build a compact internal representation of its world and then generate imaginative content.}}}
\subsection{Clustering in Unsupervised Learning}
Cluster analysis, or clustering, is an unsupervised machine learning task. It involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space.\\
\newline\textbf{What is Clustering?} \\
“Clustering” is the process of grouping similar entities together. The goal of this unsupervised machine learning technique is to find similarities in the data point and group similar data points together.
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.0\textwidth]{Screenshot (160).png}
\caption{Clustering the data into groups.}
\label{fig:fig3}
\end{figure} \newpage
\newline\textbf{Why use Clustering?}\\
Grouping similar entities together help profile the attributes of different groups. In other words, this will give us insight into underlying patterns of different groups. There are many applications of grouping unlabeled data, for example, you can identify different groups/segments of customers and market each group in a different way to maximize the revenue. Another example is grouping documents together which belong to the similar topics etc.\\
Clustering is also used to reduces the dimensionality of the data when you are dealing with a copious number of variables.\\
\newline\textbf{How does Clustering algorithms work?}
There are many algorithms developed to implement this technique but for this post, let’s stick the most popular and widely used algorithms in machine learning.
\begin{enumerate}
    \item K-mean Clustering.
    \item Hierarchical Clustering
\end{enumerate}
For this course, we will not dicusss Hierarchical Clustering.
\subsection{K-mean Clustering}
\begin{enumerate}
    \item It starts with K as the input which is how many clusters you want to find. Place K centroids in random locations in your space.
    \item Now, using the euclidean distance between data points and centroids, assign each data point to the cluster which is close to it.
    \item Recalculate the cluster centers as a mean of data points assigned to it.
    \item Repeat 2 and 3 until no further changes occur.
\end{enumerate}
\subsection{How to choose k?}
One of the methods is called \textbf{“Elbow”} method can be used to decide an optimal number of clusters. Here you would run K-mean clustering on a range of K values and plot the “percentage of variance explained” on the Y-axis and “K” on X-axis.\\
In the picture below you would notice that as we add more clusters after 3 it doesn't give much better modeling on the data. The first cluster adds much information, but at some point, the marginal gain will start dropping.
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{elbow.png}
\caption{Elbow Method.}
\label{fig:fig3}
\end{figure} \\
\subsection{Things to remember when using clustering algorithm:}
\begin{enumerate}
    \item Standardizing variables so that all are on the same scale. It is important when calculating distances.
    \item Treat data for outliers before forming clusters as it can influence the distance between the data points.
\end{enumerate} \newpage
\section{K-Means Algorithm}
\notte{\textbf{K-means clustering, a method from vector quantization, aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.}}
\subsection{Introduction}
\begin{enumerate}
    \item K-means clustering is one of the most popular clustering algorithms.
    \item It gets it name based on its property that it tries to find most optimal user specified k number of clusters in a any dataset. The quality of the dataset and their seperability is subject to implementation details, but it is fairly straight forward iterative algorithm.
    \item It basically involves a random centroid initialization step followed by two steps, namely, cluster assignment step, and centroid calculation step that are executed iteratively until a stable mean set is arrived upon. It becomes more clear in the animation below.
    \begin{figure}[!htb]
\begin{subfigure}{0.6\textwidth}
\includegraphics[width=0.9\linewidth, height=6cm]{Screenshot (161).png} 
\caption{Random initialization for two cluster centroids.}
\label{fig:subim1}
\end{subfigure}\;\;\;
\begin{subfigure}{0.6\textwidth}
\includegraphics[width=0.9\linewidth, height=6cm]{Screenshot (162).png}
\caption{All the data or points has been divided by taking the points near to blue/red centroids.}
\label{fig:subim2}
\end{subfigure}
\caption{The data points which are near to blue centroid are labeled as blue and similar to red, if the clustering contains 3 types then we would take blue,red and green has cluster centroids.}
\label{fig:image2}
\end{figure}
    \begin{figure}[!htb]
\begin{subfigure}{0.6\textwidth}
\includegraphics[width=0.9\linewidth, height=6cm]{Screenshot (163).png} 
\caption{After classifying data into blue/red. Take the average of each data and move the centroid to the average point.}
\label{fig:subim1}
\end{subfigure}\;\;\;
\begin{subfigure}{0.6\textwidth}
\includegraphics[width=0.9\linewidth, height=6cm]{Screenshot (170).png}
\caption{Again moving to average point, take near data points for blue as blue and for red as red, now again take the average of new classified data and move the centroids to the average point. }
\label{fig:subim2}
\end{subfigure}
\caption{Repeat the process classifying and reshuffling the data until the cluster centroids remain constant at the some points.}
\label{fig:image2}
\end{figure}\newpage
    \item \textbf{Cluster Assignment:} Assign each data point to one of the two clusters based on its distance from them. A point is assigned to the cluster, whose centroid it is closer to.
    \item \textbf{Move Centroid:} After cluster assignment, centroids are moved to the mean of clusters formed. And then the process is repeated. After a certain number of steps the centroids will no longer move around and then the iterations can stop.
\end{enumerate}
\subsection{Algorithm}
\textbf{Input:}
\begin{enumerate}
    \item K, number of clusters.
    \item Training set, $x^{(1)}, x^{(2)},\cdots, x^{(m)}.$
    \item where: \\ $x^{(i)}\ \epsilon\ \Re^n$, as there are no bias terms, $x_0=1.$
\end{enumerate}
\textbf{Algorithm:}
\begin{enumerate}
    \item Randomly initialize K cluster centroids, $\mu_1, \mu_2, \cdots, \mu_k\ \epsilon\ \Re^n.$
    \item Then,
    \begin{align*}
Repeat \{ \\
    \text{for }i &= 1\text{ to }m \\
        & c^{(i)} =\text{ index (from }1\text{ to }K\text{) of centroid closest to }x^{(i)}\text{, i.e., } min_k \lVert x^{(i)} - \mu_k \rVert^2 \\
    \text{for }k &= 1\text{ to }K \\
        & \mu_k =\text{ average (mean) of points assigned to cluster, }k\text{, i.e., } \frac {\text{sum of } x^{(i)}\text{, where }c^{(i)} = k} {\text{number of }c^{(i)} = k} \\
\} \tag{4.0} \label{40}
    \end{align*}
\end{enumerate}
\notte{It is common to apply k-means to a non-seperated clusters. This has particular applications in segmentation problems, like market segmentation or population division based on pre-selected features.}
\subsection{Optimization Objective}
\textbf{Notation:}
\begin{enumerate}
    \item $c^{(i)}$ - index of cluster $\{1, 2,\cdots, K\}$ to which example $x^{(i)}$ is currently assigned.
    \item $\mu_k$ - cluster centroid K, $\mu_k\ \in\ \Re^n$.
    \item cluster centroid of the cluster to which the example $x^{(i)}$ is assigned.
\end{enumerate}
Following the above notation, the cost function of the k-means clustering is given by,
\begin{align}
J(c^{(1)}, c^{(2)}, \cdots, c^{(m)}, \mu_1, \mu_2, \cdots, \mu_K) = {1 \over m} \sum_{i=1}^m \lVert x^{(i)} - \mu_{c^{(i)}} \rVert^2  \label{41}
\end{align}
Hence the optimization objective is,
\begin{align}
    \nonumber \min_&{c^{(1)}, c^{(2)}, \cdots, c^{(m)},} J(c^{(1)}, c^{(2)}, \cdots, c^{(m)}, \mu_1, \mu_2, \cdots, \mu_K) \\
    &\mu_1, \mu_2, \cdots, \mu_K\label{42}
\end{align}
\notte{\textbf{The cost function in \ref{41} is called distortion cost function or the distortion of k-means clustering.}}
It can argued that the k-means algorithm in \ref{40}, is implementing the cost function optimization. This is so because the first step of k-mean clustering, i.e. the cluster assignment step is nothing but the minimization of the cost w.r.t. $c^{(1)}, c^{(2)},\cdots, c^{(m)}$ as this step involves assigning a data point to the nearest possible cluster. Similarly the second step, i.e. moving the centroid step is the minimization of the clustering cost w.r.t. $\mu_1, \mu_2, \cdots, \mu_K$ as the most optimal position of centroid for minimizing the distortion for a given set of points is the mean position.\\
One handy way of checking if the clustering alorithm is working correctly is to plot distortion as a function of number of iterations. As both the steps in the k-means are calculated steps for minimization it is always going to decrease or remain approximately constant as the number of iterations increase.
\subsection{Random Initialization}
There are various ways for randomly picking out K<m cluster centroids, but the most recommended one involves picking K randomly picked training examples and initialize $\{\mu_1, \mu_2, \cdots, \mu_K\}$ equal to these K examples.\\
Based on initialization, it is possible that k-means could converge to different centroids or stuck in some local optima. One possible solution to this is to try multiple random initializations and then choose the one with the least distortion. It’s fairly usual to run k-means around 50-1000 times with random initialization to make sure that it does not get stuck in local optima.\\
Generally the trick of multiple random initializations will help only if the number of clusters is small, i.e. between 2-10. For higher number of clusters the multiple number of random initializations are less likely to help improve the distortion cost function.
\subsection{Choosing the Number of Clusters}
\begin{enumerate}
    \item One way of choosing the number of clusters is by manually visualizing the data.
    \item Sometimes it is ambiguous as to how many clusters exist in the dataset and in such cases it’s rather more useful to choose the number of clusters on the basis of end goal or the number of clusters that serve well the later down stream goal that needs to be extracted from the datasets.
    \item \textbf{Elbow Method:} On plotting the distortion as a function of number of clusters, K, this methods says that the optimal number of cluster at the point the elbow occurs as can be seen for line B in the plot below. It is a reasonable way of choosing the number of clusters. But this method does not always work because the sometimes the plot would look like line A which does not have clear elbow to choose.
    \begin{figure}[!htb]
\centering
\includegraphics[width = 1.\textwidth]{fig-2-elbow-method.png}
\caption{Elbow Method.}
\label{fig:fig3}
\end{figure}
\end{enumerate}
\notte{As a strict rule in k-means, as the number of cluster K increases, the distortion would decrease. But after some point the increase in cluster would not give much decrease in the distortion.}
\chapter{Principal Component Analysis (PCA)}
\textbf{A mathematical procedure that transforms a number of (possibly) correlated variables into a (possibly smaller) number of uncorrelated variables called principal components.}
\section{Introduction}
For a given dataset, PCA tries to find a lower dimensional surface onto which these points can be projected while minimizing the approximation losses. For example consider the dataset (marked by blue dots’s) in $\Re^2$ in the the plot below. The line formed by the red x’s is the projection of the data from $\Re^2$ to $\Re$.\\
\textbf{Principal Component Analysis (PCA)} is a statistical procedure that uses an orthogonal transformation which converts a set of correlated variables to a set of uncorrelated variables. \textbf{PCA} is a most widely used tool in exploratory data analysis and in \textbf{machine learning} for predictive models. Moreover, PCA is an unsupervised statistical technique used to examine the interrelations among a set of variables. It is also known as a general factor analysis where regression determines a line of best fit.\\
\textbf{PCA is an unsupervised machine learning algorithm that attempts to reduce the dimensionality (number of features) within a dataset while still retaining as much information as possible.}\\
Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\\
Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\\
So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible.
\begin{figure}[!htb]
\centering
\includegraphics[width = 1\textwidth]{fig-1-pca-projection.png}
\caption{PCA Projection.}
\label{fig:fig3}
\end{figure}
\section{STEP BY STEP EXPLANATION OF PCA}
\begin{enumerate}
    \item \textbf{STEP 1: STANDARDIZATION}\\
    The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis.\\
    More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. So, transforming the data to comparable scales can prevent this problem.\\
    Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable.
    \begin{align}
        z = \frac{value - Mean}{Standard deviation.}
    \end{align}
    Once the standardization is done, all the variables will be transformed to the same scale.
    \item \textbf{STEP 2: COVARIANCE MATRIX COMPUTATION}\\
    The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or in other words, to see if there is any relationship between them. Because sometimes, variables are highly correlated in such a way that they contain redundant information. So, in order to identify these correlations, we compute the covariance matrix.\\
    The covariance matrix is a $p\times\ p$ symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a $3\times\ 3$ matrix of this from:\\
    \begin{center}
    \begin{math}
    \left[\begin{array}{ccc}
        Cov(x,x) & Cov(x,y) & Cov(x,z) \\
        Cov(y,x) & Cov(y,y) & Cov(y,z) \\
        Cov(z,x) & Cov(z,y) & Cov(z,z) \\
    \end{array}\right]
    \end{math}
    \end{center}\\
    Since the covariance of a variable with itself is its variance (Cov(a,a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal.\\
    \newline\textbf{What do the covariances that we have as entries of the matrix tell us about the correlations between the variables?}\\
    It’s actually the sign of the covariance that matters :
    \begin{itemize}
        \item If positive then : the two variables increase or decrease together (correlated).
        \item If negative then : One increases when the other decreases (Inversely correlated).
    \end{itemize}
    Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step.
    \item \textbf{STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS}\\
    Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components.\\
    Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below.\\
    \begin{figure}[!htb]
    \centering
    \includegraphics[width = 1\textwidth]{Principal Component Analysis Principal Components.png}
    \caption{Percentage of variance for each by PC.}
    \label{fig:fig3}
    \end{figure}
    Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables.\\
    An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables.\\
    Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible.
\end{enumerate}
\section{Implementation}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.75\textwidth]{fig-2-pca-projection-with-errors.png}
    \caption{PCA Projection, Components, and Projection Errors.}
    \label{fig:fig3}
    \end{figure}
From the above plot it is easier to point out what exactly PCA is doing. The green points show the original data. So all PCA is trying to do is to find the orthogonal components along which the eigenvalues are maximized which is basically a fancy way of saying that PCA finds a feature set in the order of decreasing variance for a given dataset. In the above example, the red vector is displaying higher variance and is the first component, while the blue vector is displaying relatively less variance.\\
As there are as many principal components as there are variables in the data, principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin).
\notte{\textbf{Performing PCA for number of components greater than the current number of dimensions is useless as the data is preserved with 100$\%$ variance in the current dimension and no new dimension can help enhance that metric.}}\\
So, when dimensionality reduction is done using PCA as can be seen in the red dots, the projection is done along the more dominant feature among the two as it is more representative of the data among the two dimensions. Also, it can be seen that the red vector lies on a line than minimizes the projection losses represented by the green lines from the original data point to the projected data points.\\
The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance.\\
This continues until a total of p principal components have been calculated, equal to the original number of variables.\\
\notte{\textbf{Mean Normalization and feature scaling are a must before performing the PCA, so that the variance of a component is not affected by the disparity in the range of values.}}
Generalizing to n-dimensional data the same technique can be used to reduce the data to k-dimensions in a similar way by finding the hyper-surface with least projection error.
\section{Projection vs Prediction}
\notte{\textbf{PCA is not Linear Regression.}}
In linear regression, the aim is predict a given dependent variable, y based on independent variables, x, i.e. minimize the prediction error. In contrast, PCA does not have a target variable, y, it is mere feature reduction by minimizing the projection error. The difference is clear from the plot in \ref{fig:fig4}.
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.75\textwidth]{fig-3-projection-vs-prediction.png}
    \caption{PCA Projection, Components, and Projection Errors.}
    \label{fig:fig4}
    \end{figure}\\
The blue points display the prediction based on linear regression, while the red points display the projection on the reduced dimension. The optimization objectives of the two algorithms are different. While linear regression is trying to minimize the squared errors represented by the blue lines, PCA is trying to minimize the projection errors represented by the red lines.
\section{Mean Normalization and Feature Scaling}
It is important to have both these steps in the preprocessing, before PCA is applied. The mean of any feature in a design matrix can be calculated by,
\begin{align}
    \mu_j = {1 \over m} \sum_{i=1}^m x_j^{(i)} \label{43}
\end{align}
Following the calculation of means, the normalization can be done by replacing each $x_j$ by $x_j - \mu_j$. Similarly, feature scaling is done by replacing each $x_j^{(i)}$ by,
\begin{align}
    \frac {x_j^{(i)} -\mu_j} {s_j} \label{44}
\end{align}
where $s_j$ is some measure of the range of values of feature j. It can be $max(x_j) - min(x_j)$ or more commonly the standard deviation of the feature.
\section{PCA Algorithm}
\begin{enumerate}
    \item Compute covariance matrix given by,
    \begin{align}
        \Sigma = {1 \over m} \sum_{i}^n \left( x^{(i)} \right) \left( x^{(i)} \right)^T \label{45}
    \end{align}
    \notte{\textbf{All covariance matrices, $\Sigma$, satisfy a mathematical property called symmetric positive definite. (Will take up in future posts.)}}
    \item Follow by this, eigen vectors and eigen values are calculated. There are various ways of doing this, most popularly done by singular value decomposition (SVD) of the covariance matrix. SVD returns three different matrices given by,
    \begin{align}
        [U,S,V] = SVD(\Sigma) \label{46}
    \end{align}
    where,
    \begin{itemize}
        \item $\Sigma$ is a $n * n$ matrix because each $x^{(i)}$ is a $n * 1$ vector.
        \item U is the $n * n$ matrix where each column represents a component of the PCA. In order to reduce the dimensionality of the data, one needs to choose the first k columns to form a matrix, $U_{reduce}$, which is a $n ∗ k$ matrix.
    \end{itemize}
    So the dimensionally compressed data is given by,
    \begin{align}
        z^{(i)} = U_{reduce}^T x^{(i)} \label{47}
    \end{align}
    Since, $U_{reduce}^{\top}$ is $k * n$ matrix and $x^{(i)}$ is $n * 1$ vector, the product, $z^{(i)}$ is a $k * 1$ vector with reduced number of dimensions.\\
    Given a reduced representation, $z^{(i)}$, we can find its approximate reconstruction in the higher dimension by,
    \begin{align}
        x_{approx}^{(i)} = U_{reduce} \cdot z^{(i)} \label{48}
    \end{align}
    Since, $U_{reduce}$ is $n * k$ matrix and $z^{(i)}$ is $k * 1$ vector, the product, $x_{approx}^{(i)}$ is a $n * 1$ vector with the original number of dimensions.
\end{enumerate}
\subsection{Number of Principal Components}
How to determine the number pricipal components to retain during the dimensionality reduction?\\
Consider the following two metrics
\begin{enumerate}
    \item The objective of PCA is to minimize the projection error given by,
    \begin{align}
        {1 \over m} \sum_{i=1}^m \lVert x^{(i)} - x_{approx}^{(i)} \rVert^2 \label{49} 
    \end{align}
    \item Total variation in the data is given by,
    \begin{align}
        {1 \over m} \sum_{i=1}^m \lVert x^{(i)} \rVert^2 \label{50} 
    \end{align}
\end{enumerate}
\textbf{Rule of Thumb} is, choose the smallest value of k, such that,
\begin{align}
    \frac { {1 \over m} \sum_{i=1}^m \lVert x^{(i)} - x_{approx}^{(i)} \rVert^2} { {1 \over m} \sum_{i=1}^m \lVert x^{(i)} \rVert^2} \leq 0.01 (\text{or } 1\%) \label{51}
\end{align}
i.e. 99$\%$ of the variance is retained (Generally values such as 95−90$\%$ variance retention are used). It will be seen overtime than often the amount of dimensions reduced is significant while maintaining the 99$\%$ variance. (because many features are highly correlated.)
\notte{\textbf{Talking about the amount of variance retained in more informative than citing the number of principal components retained.}}
So for choosing k the following method could be used,
\begin{enumerate}
    \item Try PCA for k = 1.
    \item Compute $U_{reduce},\ z^{(1)}, z^{(2)}, \cdots, z^{(m)},\ x_{approx}^{(1)}, x_{approx}^{(2)}, \cdots, x_{approx}^{(m)}.$
    \item Check variance retention using \ref{51}.
    \item Repeat the steps for k = 2, 3,$\cdots$ to satisfy \ref{51}.
\end{enumerate}
There is an easy work around to bypass this tedious process by using \ref{46}. The matrix S returned by SVD is a diagonal matrix of eigenvalues corresponding to each of the components in U. S is a $n ∗ n$ matrix with diagonal eigenvalues $s_{11}, s_{22}, \cdots, s_{nn}$ and off-diagonal elements equal to 0. Then for a given value of k,
\begin{align}
    \frac { {1 \over m} \sum_{i=1}^m \lVert x^{(i)} - x_{approx}^{(i)} \rVert^2} { {1 \over m} \sum_{i=1}^m \lVert x^{(i)} \rVert^2} = 1 - \frac {\sum_{i=1}^k s_{ii}} {\sum_{i=1}^n s_{ii}} \label{52}
\end{align}
Using \ref{52}, \ref{51} can be written as,
\begin{align}
    \frac {\sum_{i=1}^k s_{ii}} {\sum_{i=1}^n s_{ii}} \gt 0.99 (\text{or } 99\%) \label{53} 
\end{align}
Now, it is easier to calculate the variance retained by iterating over values of k and calculating the value in \ref{53}.
\notte{\textbf{The value in \ref{52} is a good metrics to cite as the performance of PCA, as to how well is the reduced dimensional data representing the original data.}}
\section{Suggestions for Using PCA}
\begin{enumerate}
    \item \textbf{Speed up a learning algorithm} by reducing the number of features by applying PCA and choosing top-k to maintain 99$\%$ variance. PCA should be only applied on the training data to get the $U_{reduce}$ and not on the cross-validation or test data. This is because $U_{reduce}$ is parameter of the model and hence should be only learnt on the training data. Once the matrix is determined, the same mapping can be applied on the other two sets.
    \notte{\textbf{Run PCA only on the training data, not on cross-validation or test data.}}
    \item If using PCA for visualization, it does not make sense to choose K>3.
    \item Usage of PCA to reduce overfitting is not correct. The reason it works well in some cases is because it reduces the number of features and hence reduces the variance and increases the bias. But often there are better ways of doing this by using regularization and other similar techniques than use PCA. This would be a bad application of PCA. It is generally adviced against because PCA removes some information without keeping into consideration the target values. While this might work when 99$\%$ of the variance is retained, it may as well on various occasions lead to the loss of some useful information. On the other hand, regularization parameters are more optimal for preventing overfitting because while penalizing overfitting they also keep in context the values of the target vector.
    \notte{\textbf{Do not use PCA to prevent overfitting. Instead look into $\href{https://machinelearningmedium.com/2017/09/08/overfitting-and-regularization/}{regularization}$.}}
    \item It is often worth a shot to try any algorithm without using PCA before diving into dimensionality reduction. So, before implementing PCA, implement the models with original dataset. If this does not give desired result, one should move ahead a try using PCA to reduce the number of features. This would also give a worthy baseline score to match the performance of model against once PCA is applied.
    \item PCA can also be used in cases when the original data is too big for the disk space. In such cases, compressed data will give some benefits of space saving by dimensionality reduction.
\end{enumerate}
\begin{figure}[!htb]
\begin{subfigure}{0.75\textwidth}
\includegraphics[width=1\linewidth, height=6cm]{Screenshot (172).png} 
\caption{Where not to use PCA.}
\label{fig:subim1}
\end{subfigure}\;
\begin{subfigure}{0.75\textwidth}
\includegraphics[width=1\linewidth, height=6cm]{Screenshot (173).png}
\caption{PCA cannot be used to prevent overfitting.}
\label{fig:subim2}
\end{subfigure}
\caption{Defining where not to use PCA.}
\label{fig:image2}
\end{figure}
    \begin{figure}[!htb]
\begin{subfigure}{0.75\textwidth}
\includegraphics[width=1\linewidth, height=6cm]{Screenshot (174).png} 
\caption{Uses of PCA.}
\label{fig:subim1}
\end{subfigure}\;
\begin{subfigure}{0.75\textwidth}
\includegraphics[width=1.0\linewidth, height=6cm]{Screenshot (175).png}
\caption{Speeding up Supervised Learning.}
\label{fig:subim2}
\end{subfigure}
\caption{Applications of PCA.}
\label{fig:image2}
\end{figure}
\chapter{Anomaly Detection}
\notte{\textbf{In data mining, anomaly detection (also outlier detection) is the identification of items, events or observations which do not conform to an expected pattern or other items in a dataset.}}
\begin{enumerate}
    \item Anomalies only occur very rarely in the data.
    \item Their features differ from the normal instances significantly.
\end{enumerate}
\section{Introduction}
\notte{\textbf{Anomaly detection is primarily an unsupervised learning problem, but some aspects of it are like supervised learning problems.}}
Consider a set of points, $\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}$ in a training example (represented by blue points) representing the regular distribution of features $x_1^{(i)}$ and $x_2^{(i)}$. The aim of anomaly detection is to sift out anomalies from the test set (represented by the red points) based on distribution of features in the training example. For example, in the plot below, while point A is not an outlier, point B and C in the test set can be considered to be \textbf{anomalous (or outliers).}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.75\textwidth]{fig-1-anomaly.png}
\caption{Anomaly.}
\label{fig:fig4}
\end{figure}\\\newpage
Formally, in anomaly detection the m training examples are considered to be normal or non-anomalous, and then the algorithm must decide if the next example, $x_{test}$ is anomalous or not. So given the training set, it must come up with a model $p(x)$ that gives the probability of a sample being normal (high probability is normal, low probability is anomaly) and resulting decision boundary is defined by,
\begin{align}
p(x_{test}) &< \epsilon \text{, flag as outlier or anomaly} \\
p(x_{text}) &\geq \epsilon \text{, flag as normal or non-anomalous} \label{54}
\end{align}
\begin{figure}[!htb]
\centering
\includegraphics[width = 0.75\textwidth]{Screenshot (188).png}
\caption{Anomaly detection example.}
\label{fig:fig3}
\end{figure}\\
Some of the popular applications of anomaly detection are,
\begin{enumerate}
    \item \textbf{Fraud Detection:}
    \begin{enumerate}
        \item A observation set $x^{(i)}$ would represent user i′s activities. \item Model $p(x)$ is trained on the data from various users and. 
        \item Unusual users are identified, by checking which have $p(x^{(i)})\ <\ \epsilon$.
    \end{enumerate}
    \item \textbf{Manafacturing:} Based on features of products produced on a production line, one can identify the ones with outlier characteristics for quality control and other such preventive measures.
    \item \textbf{Monitoring Systems in a Data Center:} Based on characteristics of a machine behaviour such as CPU load, memory usage etc. it is possible to identify the anomalous machines and prevent failure of nodes in a data-center and initiate diagnostic measures for maximum up-time.
    \begin{enumerate}
        \item $x^{(i)}$ = features of machine i.
        \item $x_1$ = memory use.
        \item $x_2$ = $\frac{\text{number of disk access}}{sec}.$
        \item $x_3$ = CPU load.
        \item $x_4$ = $\frac{\text{CPU load}}{\text{network traffic}}.$
    \end{enumerate}
\end{enumerate}
\section{Gaussian Distribution}
\notte{\textbf{Gaussian distribution is also called Normal Distribution.}}
For a basic derivation, refer $\href{https://machinelearningmedium.com/2017/07/31/normal-distribution/}{Normal Distribution.}$\\
If $x\ \in\ \Re$, and x follows Gaussian distribution with mean, $\mu$ and variance $\sigma^2$, denoted as,
\begin{align}
    x \sim \mathcal{N}(\mu, \sigma^2) \label{55} 
\end{align}
A standard normal gaussian distribution is a bell-shaped probability distribution curve with mean, $\mu$ = 0 and standard deviation, $\sigma = 1$, as shown in the plot below.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.75\textwidth]{fig-2-gaussian-distribution.png}
\caption{Gaussian Distribution.}
\label{fig:fig4}
\end{figure}\\
The parameters $\mu$ and $\sigma$ signify the centring and spread of the gaussian curve as marked in the plot above. It can also be seen that the density is higher around the mean and reduces rapidly as distance from mean increases.\\
\newline The probability of x in a Gaussian distribution, $\mathcal{N}(\mu, \sigma^2)$ is given by,
\begin{align}
    p(x;\mu, \sigma^2) = {1 \over \sqrt{2\pi} \sigma} exp(- \frac {(x - \mu)^2} {2\sigma^2}) \label{56}
\end{align}
where,
\begin{itemize}
    \item $\mu$ is the mean,
    \item $\sigma$ is the standard deviation ($\sigma^2$ is the variance)
\end{itemize}
The effect of mean and standard deviation on a Gaussian plot can be seen clearly in figure below.
\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{fig-3-effect-of-mean-and-standard-deviation.png}
\caption{Effect of Mean and Standard Deviation.}
\label{fig:fig4}
\end{figure}\\
It can be noticed that, while mean, $\mu$ defines the centering of the distribution, the standard deviation, $\sigma$, defines the spread of the distribution. Also, as the spread increases the height of the plot decreases, because the total area under a probability distribution should always integrate to the value 1.\\
\newline Given a dataset, as in the previous section, $\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}$, it is possible to determine the approximate (or the most fitting) gaussian distribution by using the following \textbf{parameter estimation,}
\begin{align}
    \mu = {1 \over m} \sum_{i=1}^m x^{(i)} \label{57} \\
    \sigma^2 = {1 \over m} \sum_{i=1}^m (x^{(i)} - \mu)^2 \label{58}
\end{align}
\textbf{There is an alternative formula with the constant $\frac{1}{m-1}$ but in machine learning the formulae \ref{57} and \ref{58} are more prevalent. Both are practically very similar for high values of m.}
\begin{figure}[!htb]
\begin{subfigure}{0.75\textwidth}
\centering
\includegraphics[width=1.5\linewidth, height=6cm]{Screenshot (185).png} 
\caption{Anomaly detection example.}
\label{fig:subim1}
\end{subfigure}\;
\begin{subfigure}{0.75\textwidth}
\centering
\includegraphics[width=1.5\linewidth, height=6cm]{Screenshot (186).png}
\caption{Density estimation.}
\label{fig:subim2}
\end{subfigure}
\caption{Examples of Anomaly detection.}
\label{fig:image2}
\end{figure}
\section{Density Estimation Algorithm}
The Gaussian distribution explained above, can be used to model an anomaly detection algorithm for a training data, $\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}$ where each $x^{(i)}$ is a set of n features, $\{x_2^{(i)}, x_2^{(i)}, \cdots, x_n^{(i)}\}$. Then, $p(x)$ in \ref{54} is given by,
\begin{align}
    p(x) = p(x_1; \mu_1, \sigma_1^2) p(x_2; \mu_2, \sigma_2^2) \cdots p(x_n; \mu_n, \sigma_n^2) \label{59}
\end{align}
\notte{\textbf{Assumption: The features $\{x_1, x_2, \cdots, x_n\}$ are independent of each other.}}
where,
\begin{align*}
    x_1 &\sim \mathcal{N}(\mu_1, \sigma_1^2) \\
x_2 &\sim \mathcal{N}(\mu_2, \sigma_2^2) \\
\vdots \\
x_j &\sim \mathcal{N}(\mu_j, \sigma_j^2) \\
\vdots \\
x_n &\sim \mathcal{N}(\mu_n, \sigma_n^2) 
\end{align*}
And, \ref{59}, can be written as,
\begin{align}
    p(x) = \prod_{j=1}^n p(x_j; \mu_j, \sigma_j^2) \label{60}
\end{align}
This estimation of $p(x)$ in \ref{60} is called the density estimation.\\\newpage
To summarize:
\begin{enumerate}
    \item Choose features $x_i$ that are indicative of anomalous behaviour (general properties that define an instance).
    \item Fit parameters, $\mu_1, \cdots, \mu_n, \sigma_1^2, \cdots, \sigma_n^2$, given by,
    \begin{align}
        \mu_j = {1 \over m} \sum_{i=1}^m x_j^{(i)} \label{61}\\
        \sigma_j = {1 \over m} \sum_{i=1}^m (x_j^{(i)} - \mu_j)^2 \label{62}
    \end{align}
    \item Given a new example, compute $p(x)$, using \ref{59} and \ref{56}, and mark as anomalous based on \ref{54}.
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.0\textwidth]{Screenshot (187).png}
\caption{Anomaly detection algorithm.}
\label{fig:fig3}
\end{figure}\\
\section{Implementation}
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.0\textwidth]{Screenshot (190).png}
\caption{Importance of implementation.}
\label{fig:fig3}
\end{figure}\\
\section{Evaluation of Anomaly Detection System}
\notte{\textbf{Single real-valued evaluation metrics would help in considering or rejecting a choice for improvement of an anomaly detection system.}}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\linewidth]{Screenshot (191).png} 
\caption{Division of given data set into training/cross-validation/test sets.}
\label{fig:fig3}
\end{figure}
In order to evaluate an anomaly detection system, it is important to have a labeled dataset (similar to a supervised learning algorithm). This dataset would generally be skewed with a high number of normal cases. In order to evaluate the algorithm follow the steps (y = 0 is normal and y = 1 is anomalous),
\begin{enumerate}
    \item split the examples with y = 0 into 60-20-20 train-validation-test splits.
    \item split the examples with y = 1 into 50-50 validation-test splits.
    \item perform density estimation on the train set.
    \item check the performance on the cross-validation set to find out metrics like true positive, true negative, false positive, false negative, precision/recall, f1-score. \textbf{Accuracy score would not be a valid metric because in most cases the classes would be highly skewed} (refer $\underline{\href{https://machinelearningmedium.com/2018/04/08/error-metrics-for-skewed-data-and-large-datasets/}{\text{Error Metrics for Skewed Data}}}$).
    \item Following this, the value of $\epsilon$ can be altered on the cross-validation set to improved the desired metric in the previous step.
    \item The evalutaion of the final model on the held-out test set would give a unbiased picture of how the model performs.
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (192).png}
\caption{Evaluating the algorithm.}
\label{fig:fig3}
\end{figure}\\
\section{Anomaly Detection vs Supervised Learning}
\notte{\textbf{A natural question arises, “If we have labeled data, why not used a supervised learning algorithm like logistic regression or SVM?”.}}
Even though there are no hard-and-fast rules about when to use what, there a few recommendations based on observations of learning performance of different algorithms in such settings. They are listed below,
\begin{enumerate}
    \item In an anomaly detection setting, it is generally the case that there is a very small number of positive examples (i.e. y = 1 or the anomalous examples) and a large number of negative examples (i.e. y = 0 or normal examples). On the contrary, for supervised learning there is a large number of positive and negative examples.
    \item Many a times there are a variety of anomalies that might be presented by a sample (including anomalies that haven’t been presented so far), and if the number of positive set is small to learn from then anomaly detection algorithm stands a better chance in performing better. On the other hand a supervised learning algorithm needs a bigger set of examples from both positive and negative samples to get a sense of the differentiations among the two are as well as the future anomalies are more likely to be the ones presented so far in the training set.
\end{enumerate}
\hspace{-5mm} \begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} \hline
Anomaly Detection & Supervised Learning \\ \hline
1. Very small number of positive examples(y = 1, & 1. Large number of positive and negative examples. \\
  0-20 is common). Large number of negative &  \\ 
 examples (y = 0). &  \\ \hline
2. Many different "types" of anomalies. Hard & 2. Enough positive examples for algorithm to \\
 for any algorithm to learn from positive examples & get a sense of what positive  \\
  what the anomalies look like; future anomalies & examples are like, future positive  \\
  may look nothing like any of the anomalous & examples likely to be similar\\ 
  examples we've seen so far. &  to ones in training set.\\ \hline
3. Examples, & 3. Examples,  \\
   \;\;\; $\RomanNumeralCaps{1}$. Fraud detection (y=1). & \;\;\ $\RomanNumeralCaps{1}$. Email spam classification. \\
   \;\;\; $\RomanNumeralCaps{2}$. Manufacturing (e.g. aircraft engines). & \;\;\; $\RomanNumeralCaps{2}$. Weather prediction (sunny/rainy/etc.). \\
   \;\;\; $\RomanNumeralCaps{3}$. Monitoring machines in a data center. & \;\;\; $\RomanNumeralCaps{3}$. Cancer classification. \\
   \;\;\; $\vdots$ & \;\;\; $\vdots$ \\ \hline
\end{tabular}
    \caption{Differences between Anomaly detection and Supervised learning.}
    \label{tab:my_label}
\end{table}
\section{Choosing Features}
\notte{\textbf{Feature engineering (or choosing the features which should be used) has a great deal of effect on the performance of an anomaly detection algorithm.}}
\begin{enumerate}
    \item Since the algorithm tries to fit a Gaussian distribution through the dataset, it is always helpful if the the histogram of the data fed to the density estimation looks similar to a Gaussian bell shape.
    \item If the data is not in-line with the shape of a Gaussian bell curve, sometimes a transformation can help bring the feature closer to a Gaussian approximation. \\
    Some of the popular transforms used are,
\begin{enumerate}
    \item $\log x$.
    \item $\log (x+ c)$.
    \item $\sqrt{x}$.
    \item $x^{\frac{1}{3}}$
\end{enumerate}
    \item Choosing of viable feature options for the algorithm sometimes depends on the domain knowledge as it would help selecting the observations that one is targeting as possible features. For example, network load and requests per minute might a good feature for anomaly detection is a data center. Sometimes it possible to come up with combined features to achieve the same objective. So the rule of thumb is to come up with features that are found to differ substantially among the normal and anomalous examples.
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (193).png} 
\caption{Gaussian approximation.}
\label{fig:fig3}
\end{figure}\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (194).png} 
\caption{Error analysis of Anomaly detection.}
\label{fig:fig3}
\end{figure}\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (195).png} 
\caption{Monitoring computers in a data center.}
\label{fig:fig3}
\end{figure}\\
\section{Multivariate Gaussian Distribution}
The $\underline{\href{https://machinelearningmedium.com/2018/05/02/anomaly-detection/#density-estimation-algorithm}{\text{density estimation}}}$ seen earlier had the underlying assumption that the features are independent of each other. While the assumption simplifies the analysis there are various downsides to the assumption as well.\\
Consider the data as shown in the plot below. It can be seen clearly that there is some correlation (negative correlation to be exact) among the two features.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{fig-5-correlated-features.png} 
\caption{Correlated Features.}
\label{fig:fig3}
\end{figure}\\
Univariate Gaussian distribution applied to this data results in the following countour plot, which points to the assumption made in \ref{60}. Because while \textbf{the two features are negatively correlated, the contour plot do not show any such dependency}. On the contrary, if multivariate gaussian distribution is applied to the same data one can point out the correlation. Seeing the difference, it is also clear that the chances of test sets (red points) being marked as normal is lower in multivariate Gaussian than in the other.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{fig-6-univariate-vs-multivariate.png} 
\caption{Univariate vs Multivariate Gaussian.}
\label{fig:fig3}
\end{figure}\\
So, mutlivariate gaussian distribution basically helps model $p(x)$ is one go, unlike \ref{60}, that models individual features $\{x_1, x_2, \cdots, x_n\}$ in x. The multivariate gaussian distribution is given by,
\begin{align}
    p(x; \mu, \Sigma) = \frac {1} {(2\pi)^{n/2} \, |\Sigma|^{1/2}} exp\left(-{1 \over 2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right) \label{63}
\end{align}
where,
\begin{enumerate}
    \item $\mu\ \epsilon\ \Re$ and $\Sigma\ \epsilon\ \Re^{n*n}$ (Covariance matrix) are the parameters of the distribution.
    \item $\|\Sigma\|$ is the determinant of the matrix $\Sigma$.
\end{enumerate}
The density estimation for multivariate gaussian distribution can be done using the following 2 formulae,
\begin{align}
    \mu = {1 \over m} \sum_{i=1}^m x^{(i)}  \label{64} \\
    \Sigma = {1 \over m} \sum_{i=1}^m (x^{(i)} - \mu) (x^{(i)} - \mu)^T  \label{65}
\end{align}
\textbf{Steps in multivariate density estimation:}
\begin{enumerate}
    \item Given a train dataset, estimate the parameters $\mu$ and $\Sigma$ using \ref{64} and \ref{65}.
    \item For a new example x, compute $p(x)$ given by \ref{63}.
    \item Flag as anomaly if $p(x)<\ \epsilon$.
\end{enumerate}
The covariance matrix is the term that brings in the major difference between the univariate and the multivariate gaussian. The effect of covariance matrix and mean shifting can be seen in the plots below.
\notte{\textbf{A covariance matrix is always symmetric about the main diagonal.}}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{fig-7-effect-of-mean-and-covariance.png} 
\caption{Effect of Mean and Covariance Matrix.}
\label{fig:fig3}
\end{figure}\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (196).png} 
\caption{Monitoring machines data center.}
\label{fig:fig3}
\end{figure}\\
\begin{enumerate}
    \item The mean shifts the center of the distribution.
    \item Diagonal elements vary the spread of the distribution along corresponding features (also called the variance).
    \item Off-diagonal elements vary the correlation among the various features.
\end{enumerate}\newpage
Also, the original model in \ref{60} is a special case of the multivariate gaussian distribution where the off-diagonal elements of the covariance matrix are contrained to zero (countours are axis aligned).\newpage
\section{Univariate vs Multivariate Gaussian Distribution}
\begin{enumerate}
    \item Univariate model can be used when the features are manually created to capture the anomalies and the features take unusual combinations of values. Whereas multivariate gaussian can be used when the correlation between features is to be captured as well.
    \item Univariate model is computationally cheaper and hence scales well to the larger dataset (m = 10,000-100,000), whereas the multivariate model is computationally expensive, majorly because of the term $\Sigma_{-1}$.
    \item Univariate model works well for smaller value of m as well. For multivariate model, $m>n$, or else $\Sigma$ is singular and hence not invertible.
Generally multivariate gaussian is used when m is much bigger than n, like m>10n, because $\Sigma$ is a fairly large matrix with around $\frac{n}{2}$ parameters, which would be learnt better in a setting with larger m.
\end{enumerate}
\textbf{A matrix might be singular because of the presence of redundant features, i.e. two features are linearly dependent (or) a feature is a linear combination of a set of other features. Such matrices are non-invertible.}\newpage
\begin{figure}[!htb]
\centering
\includegraphics[width=0.9\textwidth]{Screenshot (221).png} 
\caption{Multivariate Gaussian Distribution.}
\label{fig:fig3}
\end{figure}\\ 
\subsection{Anomaly detection with the Multivariate Gaussian}
\begin{enumerate}
 \item Fit the model $p(x)$ by setting 
 \begin{figure}[!htb]
\centering
\includegraphics[width=0.5\textwidth]{Screenshot (222).png} 
\caption{Graph.}
\label{fig:fig3}
\end{figure}\\
    \begin{align}
        \mu &= {1 \over m} \sum_{i=1}^m x^{(i)} \\ 
        \Sigma &= {1 \over m} \sum_{i=1}^m (x^{(i)} - \mu) (x^{(i)} - \mu)^T 
    \end{align} 
    \item Given a new example x, compute
    \begin{align}
    p(x; \mu, \Sigma) = \frac {1} {(2\pi)^{n/2} \, |\Sigma|^{1/2}} exp\left(-{1 \over 2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right) 
    \end{align}
    Flag an anomaly if $p(x) < \epsilon$
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (223).png} 
\caption{Relation to original model.}
\label{fig:fig3}
\end{figure}\\ 
\begin{table}[h!]
    \begin{tabular}{|c|c|} \hline
    Original model & Multivariate Gaussian \\ \hline
    $p(x_1; \mu_1, \sigma_1^2)\times \cdots\times p(x_n; \mu_n, \sigma_n^2)$ & $p(x; \mu, \Sigma) = \frac {1} {(2\pi)^{n/2} \, |\Sigma|^{1/2}} exp\left(-{1 \over 2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right) $  \\
    1. Manually create features to capture  & 1. Automatically captures correlations  \\
    anomalies where $x_1,x_2$ take unsual  & between features.  \\ 
    combinations of values like $x_3 = \frac{x_1}{x_2}$. & \\ 
    2. Computationally cheaper (alternatively,  & 2. Computationally more expensive.  \\ 
     scales better to large). & \\
    3. OK even if m (training set size) is & 3. Must have $m>n$ or else   \\ 
     \hspace{-5mm}is small. & $\Sigma$ is non-invertible. ($m\geq10n$) \\ \hline
    \end{tabular}
    \caption{Differences between Original model and Multivariate Gaussian.}
    \label{tab:my_label}
\end{table}
\chapter{Recommender Systems}
\textbf{A recommender system or a recommendation system is a subclass of information filtering system that seeks to predict the \textcolor{red}{"rating"} or \textcolor{red}{"preference"} a user would give to an item.}
\section{Introduction}
During the last few decades, with the rise of Youtube, Amazon, Netflix and many other such web services, recommender systems have taken more and more place in our lives. From e-commerce (suggest to buyers articles that could interest them) to online advertisement (suggest to users the right contents, matching their preferences), recommender systems are today unavoidable in our daily online journeys.\\
In a very general way, recommender systems are algorithms aimed at suggesting relevant items to users (items being movies to watch, text to read, products to buy or anything else depending on industries).\\
Recommender systems are really critical in some industries as they can generate a huge amount of income when they are efficient or also be a way to stand out significantly from competitors. As a proof of the importance of recommender systems, we can mention that, a few years ago, Netflix organised a challenges (the “Netflix prize”) where the goal was to produce a recommender system that performs better than its own algorithm with a prize of 1 million dollars to win.\\
In this article, we will go through different paradigms of recommender systems. For each of them, we will present how they work, describe their theoretical basis and discuss their strengths and weaknesses.
\subsection{Outline}
In the first section we are going to overview the two major paradigms of recommender systems : collaborative and content based methods. The next two sections will then describe various methods of collaborative filtering, such as user-user, item-item and matrix factorization. The following section will be dedicated to content based methods and how they work. Finally, we will discuss how to evaluate a recommender system.
\section{Collaborative versus content}
The purpose of a recommender system is to suggest relevant items to users. To achieve this task, there exist two major categories of methods : collaborative filtering methods and content based methods. Before digging more into details of particular algorithms, let’s discuss briefly these two main paradigms.
\subsection{Collaborative filtering methods}
Collaborative methods for recommender systems are methods that are based solely on the past interactions recorded between users and items in order to produce new recommendations. These interactions are stored in the so-called \textcolor{green}{“user-item interactions matrix”.}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Illustration of the user-item interactions matrix..png} 
\caption{Illustration of the user-item interactions matrix.}
\label{fig:fig3}
\end{figure}\\ 
Then, the main idea that rules collaborative methods is that these past user-item interactions are sufficient to detect similar users and/or similar items and make predictions based on these estimated proximities.\\
The class of collaborative filtering algorithms is divided into two sub-categories that are generally called memory based and model based approaches. Memory based approaches directly works with values of recorded interactions, assuming no model, and are essentially based on nearest neighbours search (for example, find the closest users from a user of interest and suggest the most popular items among these neighbours). Model based approaches assume an underlying “generative” model that explains the user-item interactions and try to discover it in order to make new predictions.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Overview of the collaborative filtering methods paradigm..png} 
\caption{Overview of the collaborative filtering methods paradigm.}
\label{fig:fig3}
\end{figure}\\ 
The main advantage of collaborative approaches is that they require no information about users or items and, so, they can be used in many situations. Moreover, the more users interact with items the more new recommendations become accurate: for a fixed set of users and items, new interactions recorded over time bring new information and make the system more and more effective.\\
However, as it only consider past interactions to make recommendations, collaborative filtering suffer from the “cold start problem”: it is impossible to recommend anything to new users or to recommend a new item to any users and many users or items have too few interactions to be efficiently handled. This drawback can be addressed in different way: recommending random items to new users or new items to random users (random strategy), recommending popular items to new users or new items to most active users (maximum expectation strategy), recommending a set of various items to new users or a new item to a set of various users (exploratory strategy) or, finally, using a non collaborative method for the early life of the user or the item.\\
In the following sections, we will mainly present three classical collaborative filtering approaches: two memory based methods (user-user and item-item) and one model based approach (matrix factorisation).
\subsection{Problem Formulation}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (224).png} 
\caption{Example for predicting movie ratings.}
\label{fig:fig3}
\end{figure}\\ \newpage
Give $n_m$ choices and $n_u$ users,
\begin{enumerate}
    \item r(i,j)=1  if user j has rated choice i.
    \item y(i,j)  is the rating given by user j to the choice i, defined only if r(i,j)=1.
\end{enumerate}
So, the objective of the reocmmender system is to use the rated choices by the population of users and predict the ratings that a user would attribute to a choice that is not rated i.e. r(i,j)=0. In most real-world cases such as movie ratings, the number of unrated choices is generally very high and hence is not an elementary/easy problem to solve.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (225).png} 
\caption{Problem formulation.}
\label{fig:fig3}
\end{figure}\\ 
\subsubsection{Content Based Recommendations}
\begin{enumerate}
    \item Each choice is alloted an n number of features and rated along those dimensions.
    \item Following this, for each user j the ratings are regressed as a function of the alloted set of features. 
    \item The learnt parameter for user j, $\theta^{(j)}$ lies in space $\Re^{n+1}$.
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (226).png} 
\caption{Content Based Recommender systems.}
\label{fig:fig3}
\end{figure}\\ 
Summarizing,
\begin{itemize}
    \item $\theta^{(j)}$  is the parameter vector for user j.
    \item $x^{(i)}$ is the feature vector for choice i.
    \item For user j and choice i, predicted rating is given by, $(\theta^{(j)})^{\top}(x^{(i)}).$
\end{itemize}
Suppose user j has rated $m^{(j)}$ choices, then learning $\theta^{(j)}$ can be treated as linear regression problem. So, to learn $\theta^{(j)}$,
\begin{align}
    min_{\theta^{(j)}} {1 \over 2} \sum_{i: r(i, j)=1} ((\theta^{(j)})^T (x^{(i)}) - y^{(i, j)})^2 + {\lambda \over 2} \sum_{k=1}^n (\theta_k^{(j)})^2  \label{66}
\end{align} 
Similarly, to learn $\theta^{(1)}, \theta^{(2)}, \cdots, \theta^{(n_u)}$,
\begin{align}
    min_{\theta^{(1)}, \cdots, \theta^{(n_u)}} {1 \over 2} \sum_{j=1}^{n_u} \sum_{i: r(i, j)=1} ((\theta^{(j)})^T (x^{(i)}) - y^{(i, j)})^2 + {\lambda \over 2} \sum_{j=1}^{n_u} \sum_{k=1}^n (\theta_k^{(j)})^2 \label{67}
\end{align}
where cost function is given by,
\begin{align}
    J(\theta^{(1)}, \cdots, \theta^{(n_u)}) = {1 \over 2} \sum_{j=1}^{n_u} \sum_{i: r(i, j)=1} ((\theta^{(j)})^T (x^{(i)}) - y^{(i, j)})^2 + {\lambda \over 2} \sum_{j=1}^{n_u} \sum_{k=1}^n (\theta_k^{(j)})^2 \label{68}
\end{align}
Gradient Descent Update,
\begin{align}
\theta_k^{(j)} &= \theta_k^{(j)} - \alpha \left( \sum_{i: r(i, j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)}) x_k^{(i)}  \right) \text{, for } k = 0 \\
\theta_k^{(j)} &= \theta_k^{(j)} - \alpha \left( \sum_{i: r(i, j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)}) x_k^{(i)} + \lambda \theta_k^{(j)}\right) \text{, otherwise } \label{69}
\end{align}
where,
\begin{align}
\frac {\partial} {\partial \theta_k^{(j)}} J(\theta^{(1)}, \cdots, \theta^{(n_u)}) &= \sum_{i: r(i, j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)}) x_k^{(i)} \text{, for } k = 0 \\
\frac {\partial} {\partial \theta_k^{(j)}} J(\theta^{(1)}, \cdots, \theta^{(n_u)}) &= \sum_{i: r(i, j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)}) x_k^{(i)} + \lambda \theta_k^{(j)} \text{, otherwise } \label{70}
\end{align}
\textcolor{red}{Note:} \textcolor{green}{By convention, the terms} \textbf{$\frac{1}{m^{(j)}}$} \textcolor{green}{terms are removed from the equations in recommendation systems. But these do not affect the optimization values as these are only constants used for ease of derivations in linear regression cost function.}
\notte{\textbf{The effectiveness of content based recommendation depends of identifying the features properly, which is often not easy.}}
\subsection{Collaborative Filtering}
\notte{\textbf{Collaborative filtering has the intrinsic property of feature learning (i.e. it can learn by itself what features to use) which helps overcome drawbacks of content-based recommender systems.}}
Given the scores y(i,j) for a choice, $i\ \epsilon\ [1, n_m]$ by various users $j\ \epsilon\ [1, n_u]$, and the parameter vector $\theta^{(j)}$ for user j, the algorithm learns the values for the features $x^{(i)}$ applying regression by posing the following optimization problem,
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (227).png} 
\caption{Problem motivation.}
\label{fig:fig3}
\end{figure}\\ 
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (228).png} 
\caption{Optimization objective.}
\label{fig:fig3}
\end{figure}\\
\begin{align}
    min_{x^{(i)}} {1 \over 2} \sum_{j:r(i,j)=1} \left[ (\theta^{(j)})^T x^{(i)} - y(i,j) \right]^2 + {\lambda \over 2} \sum_{k=1}^n \left( x_k^{(i)} \right)^2 \label{71}
\end{align}
Intuitively this boils down to the scenario where given a choice and its ratings by various users and their parameter vectors, the collaborative filitering algorithm tries to find the most optimal features to represent the choice such that the squared error between the two is minimized. Since this is very similar to the linear regression problem, regularization term is introduced to prevent overfitting of the features learnt. Similarly by extending this, it is possible to learn all the features for all the choices $i\ \epsilon\ [1, n_m]$, i.e. given $\theta^{(1)}, \theta^{(2)}, \cdots, \theta^{(n_u)}$ learn, $x^{(1)}, x^{(2)}, \cdots, x^{(n_m)}$,
\begin{align}
    min_{x^{(1)}, \cdots, x^{(n_m)}} {1 \over 2} \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} \left[ (\theta^{(j)})^T x^{(i)} - y(i,j) \right]^2 + {\lambda \over 2} \sum_{i=1}^{n_m} \sum_{k=1}^n \left( x_k^{(i)} \right)^2 \label{72}
\end{align}
Where the updates to the feature vectors will be given by,
\begin{align}
    x_k^{(i)} := x_k^{(i)} - \alpha \left( \sum_{j:r(i,j)=1} \left[ (\theta^{(j)})^T x^{(i)} - y(i,j) \right] \theta_k^{(j)} + \lambda x_k^{(i)} \right) \label{73}
\end{align}
\notte{\textbf{It is possible to arrive at optimal $\theta$ and x by repetitively minimizing them using \ref{69} and \ref{73}}}
But it is also possible to solve for both $\theta$ and x simultaneously, given by an update rule which is nothing but the combination of the earlier two update rules in \ref{68} and \ref{72}. So the resulting cost function is given by,
\begin{align}
J(x^{(1)}, \cdots, x^{(n_m)}, \theta^{(1)}, \cdots, \theta^{(n_u)}) &= {1 \over 2} \sum_{(i,j):r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)})^2 \\ 
&+ {\lambda \over 2} \sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 \\
&+ {\lambda \over 2} \sum_{j=1}^{n_u} \sum_{k=1}^n (\theta_k^{(j)})^2\label{74}
\end{align}
and the minimization objective can be written as,
\begin{align}
    min_{x^{(1)}, \cdots, x^{(n_m)}, \theta^{(1)}, \cdots, \theta^{(n_u)}} J(x^{(1)}, \cdots, x^{(n_m)}, \theta^{(1)}, \cdots, \theta^{(n_u)}) \label{75}
\end{align}
Practically, the minimization objective \ref{75} is equivalent to \ref{69} if 
x is kept constant. Similarly, it’s equivalent to \ref{73} if $\theta$ is kept constant.
\notte{\textbf{In \ref{75}, by convention there is no $x_0=1$ and thus consequently, there in no $\theta_0$, hence leading to $x\ \epsilon\ \Re^n$ and $\theta\ \epsilon\ \Re^n$.}}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (229).png} 
\caption{Collaborative filtering algorithm.}
\label{fig:fig3}
\end{figure}\\ 
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (231).png} 
\caption{Collaborative filtering optimization objective.}
\label{fig:fig3}
\end{figure}\\\newpage
To summarize, the collaborative filtering algorithm has the following steps,
\begin{enumerate}
    \item Initializa $x^{(1)}, \cdots, x^{(n_m)}, \theta^{(1)}, \cdots, \theta^{(n_u)}$ to small random values.
    \item Minimize \ref{74} using gradient descent or any other advance optimization algorithm. The update rules given below can be obtained by following the partial derivatives along x′s and $\theta′s$.
    \begin{align}
x_k^{(i)} &= x_k^{(i)} - \alpha \left( \sum_{j:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y(i,j)) \theta_k^{(j)} + \lambda x_k^{(i)} \right) \\
\theta_k^{(j)} &= \theta_k^{(j)} - \alpha \left( \sum_{i: r(i, j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)}) x_k^{(i)} + \lambda \theta_k^{(j)}\right) \label{76}
\end{align} 
\item For a user with parameter $\theta$ and a choice with learned features x, the predicted star rating is given by $\theta^{\top}x.$
\end{enumerate}
Consequently, the matrix of ratings, Y, can be written as,\\
\begin{center}
\begin{math}
Y = \left[ 
\begin{matrix}
(\theta^{(1)})^T x^{(1)} & (\theta^{(2)})^T x^{(1)} & \cdots & (\theta^{(n_u)})^T x^{(1)} \\
(\theta^{(1)})^T x^{(2)} & (\theta^{(2)})^T x^{(2)} & \cdots & (\theta^{(n_u)})^T x^{(2)} \\
\vdots & \vdots & \ddots & \vdots \\
(\theta^{(1)})^T x^{(n_m)} & (\theta^{(2)})^T x^{(n_m)} & \cdots & (\theta^{(n_u)})^T x^{(n_m)} \\
\end{matrix}
\right ] \label{77}
\end{math}
\end{center}\\
\begin{center}
    \begin{math}
    X = \left[
    \begin{matrix}
    (x^{(1)}) \\
    (x^{(2)}) \\
    \vdots\\
    (x^{(n_m)}) \\
    \end{matrix}\right]
    \end{math}\;\;\; &\;\;\; $\Theta$ = \begin{math}
    \left[ 
    \begin{matrix}
    {(\theta^{(1)})}^{\top}\\
    {(\theta^{(2)})}^{\top}\\
    \vdots\\
    {(\theta^{(n_u)})}^{\top}\\
    \end{matrix}
    \right] 
    \end{math}
\end{center}
Where y(i,j) is the rating for choice i by user j.\\
\newline Vectorized implementation of \ref{77}, is given by,
\begin{align}
    Y = X \Theta^T  \label{78}
\end{align}
Where,
\begin{enumerate}
    \item Each row i in X represents the feature vector of choice i.
    \item Each row j in $\Theta$ represents the parameter vector for user j.
\end{enumerate}
\notte{\textbf{The algorithm discussed is also called low rank matrix factorization which is a property of the matrix Y is linear algebra.}}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (235).png} 
\caption{Finding related information.}
\label{fig:fig3}
\end{figure}\\ 
\section{Similar Recommendations}
After the collaborative filtering algorithm has converged, it can be used to find related choices. For each choice i, a feature vector is learned, $x^{(i)} \in \mathbb{R}^n$. Although it is generally not possible to decipher what the values in the matrix X denote, they encode representative features of the choices in detail. So in order to find choices close to a given choice i, a simple euclidean distance calculation will give the desired results.
\notte{\textbf{If the distance between choices i and j is small, i.e. $\lVert x^{(i)} - x^{(j)} \rVert$ is small, then they are similar.}}
\subsection{Mean Normalization}
Consider a case where one of the users has not rated any of the choices, then the rating matrix Y can be defined as,\\
Since none of the choices are rated by this user, the entries in R matrix corresponding to this user would be all zeros. So, \ref{74} can be written as follows (because ${1 \over 2} \sum_{(i,j):r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)})^2 = 0$),
\begin{align}
    J(x^{(1)}, \cdots, x^{(n_m)}, \theta^{(1)}, \cdots, \theta^{(n_u)}) = {\lambda \over 2} \sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 + {\lambda \over 2} \sum_{j=1}^{n_u} \sum_{k=1}^n (\theta_k^{(j)})^2 \label{79}
\end{align}
Since the updates to $\theta$ corresponding to this user is only governed by this cost function, it would only minimize parameter vector $\theta$. This can be seen easily by setting a low tolerance for the collaborative filtering,\\
Obviously this is not a correct assumption to rate all the choices 0 for a user that has rated none so far. For such a user it would be ideal to predict the rating as the average of ratings attibuted to it by other users so far.\\
Mean normalization helps in acheiving this. In this process each row of the ratings matrix is normalized by its mean and later denormalized after predictions.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (236).png} 
\caption{Mean Normalization.}
\label{fig:fig3}
\end{figure}\\
\chapter{Large Scale Learning}
\notte{\textbf{With the increase in size of training data, it becomes important to optimize algorithm and parallelize processes to minimize the training time and manage resource utilization.}}
\section{Introduction}
The popularity of machine learning techniques have increased in the recent past. One of the reasons leading to this trend is the exponential growth in data available to learn from. Large datasets coupled with a high variance model has the potential to perform well. But as the size of datasets increase, it poses various problems in terms of space and time complexities of the algorithms.
\notte{\textbf{It’s not who has the best algorithm that wins. It’s who has the most data.}}
\begin{align}
    \theta_j := \theta_j - \alpha {1 \over m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right) x_j^{(i)}\label{80}
\end{align}
From \ref{80} above, it can be seen that for each step of gradient descent, summation has to be performed over entire dataset of m examples. While for small datasets it might seem inconsequential, but as the size of datasets increases this would have very high impact on the training time.\\
In such cases, it would also be helpful to plot $\href{https://machinelearningmedium.com/2018/04/02/evaluation-of-learning-algorithm/#learning-curves}{learning curves}$, to check if actually training the model with such high number data samples is really helpful, because if the model has high bias then similar result could be acheived by using a smaller dataset. It would be more helpful to incrase variance of the model in such cases.\\
On the other hand, if the learning curves show that using the larger dataset is indeed helpful, it would be more productive to use more computationally efficient algorithms to train the model such as the ones mentioned in the following sections.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (237).png} 
\caption{Learning with Large data sets.}
\label{fig:fig3}
\end{figure}\\
\section{Stochastic Gradient Descent}
The gradient descent rule presented in \ref{80}, also known as batch gradient descent, has the disadvantage that for each update the summation of update term has to be performed over all the training data.\\
Stochastic gradient descent is an approximation of the batch gradient descent. Each epoch in this algorithm is begun with a random shuffle of the data followed by the following update rule,
\begin{align}
    \theta_j := \theta_j - \alpha \left( h_{\theta}(x^{(i)}) - y^{(i)} \right) x_j^{(i)}\label{81}
\end{align}
i.e. for each training data in the sample dataset, as soon as the cost correponding to that instance is calculated it is used to make an approximate update to the parameters instead of waiting for the summation to finish. While this is not as accurate as the batch gradient descent in reaching the global minimum, it always converges within its close proximity.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (238).png} 
\caption{Linear regression with gradient descent.}
\label{fig:fig3}
\end{figure}\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (239).png} 
\caption{Linear regression with gradient descent.}
\label{fig:fig3}
\end{figure}\\
\notte{\textbf{In practice, stochastic gradient descent speeds up the process of convergence over the traditional batch gradient descent.}}
While learning rate is kept constant in most implementations of stochastic gradient descent, it is observed in practice that it helps to taper off the value of learning rate as the iteration proceeds. It can be done as follows,
\begin{align}
    \alpha = \frac {constant_1} {iteration\_number + constant_2} \label{82}
\end{align}
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}\hline
        Batch gradient descent & Stochastic gradient descent  \\\hline
         &  \\
        $J_{train}(\theta)$ = $\frac{1}{2m} \sum_{i=1}^m {(h_\theta(x^{(i)}) - y^{(i)})}^2$ & $cost(\theta, (x^{(i)}, y^{(i)}))$ = $\frac{1}{2} {(h_\theta(x^{(i)}) - y^{(i)})}^2$ \\
         &  \\\hline
         &  \\
        Repeat $\bigg\{$ &  $J_train(\theta)$ = $\frac{1}{m} \sum_{i=1}^m cost(\theta, (x^{(i)}, y^{(i)}))$ \\
        $\theta_j := \theta_j - \alpha\frac{1}{3} \sum_{i=1}^m {(h_\theta(x^{(i)}) - y^{(i)})}x^{(i)}_j$ & 1. Randomly shuffle (reorder) the training examples.\\
        \;\;\;\;\; (for every j = 0, 1, ..., n) & 2. Repeat $\bigg\{$ \\
        \;\; $\bigg\}$ & \;\;for i := 1, 2, ..., m $\big\{$ \\
         & \;\;\;\;$\theta_j := \theta_j - \alpha{(h_\theta(x^{(i)}) - y^{(i)})}x^{(i)}_j$ \\
          &  \\
          & \;\;\;\;\;\; (for every j = 0, 1, ..., n) \\
           & \;\;\;\;\;\;\; $\big\}$ \\
           & \;\; $\bigg\}$ \\\hline
    \end{tabular}
    \caption{Caption}
    \label{tab:my_label}
\end{table}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (240).png} 
\caption{Stochastic gradient descent.}
\label{fig:fig3}
\end{figure}\\
\section{Mini-Batch Gradient Descent}
While batch gradient descent sums over all the data for a single update iteration of the parameters, the stochastic gradient descent does it by considering individual training examples as and when they are encountered. The \textbf{mini-batch gradient descent} takes the mid-way and uses the summation from only \textbf{b training examples (i.e. batch size)} for every update iteration. Mathematically it can be presented as follows,
\begin{align}
    \theta_j := \theta_j - \alpha {1 \over b} \sum_{i=1}^{i+b} \left( h_{\theta}(x^{(i)}) - y^{(i)} \right) x_j^{(i)} \label{83}
\end{align}
\begin{enumerate}
    \item Compared to stochastic gradient descent, the mini-batch gradient descent will be faster only if vectorized implementation is used for the updates.
    \item Compared to batch gradient descent, the mini-batch gradient descent is faster due to the obvious reason of lesser number of summations that are to be performed for a single update iteration. Also, if both the implementations are vectorized, mini-batch gradient descent will have lower memory usage. The speed of operations depends on the trade-off between the matrix operation complexities and memory usage.
    \item Generally it is observed that mini-batch gradient descent converges faster than both stochastic and batch gradient descent.
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (241).png} 
\caption{Mini-batch gradient descent.}
\label{fig:fig3}
\end{figure}\\
\section{Convergence of Stochastic gradient descent}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (242).png} 
\caption{Checking for convergence.}
\label{fig:fig3}
\end{figure}\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (243).png} 
\caption{Number of iterations w.r.t alpha.}
\label{fig:fig3}
\end{figure}\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (244).png} 
\caption{Learning rate alpha.}
\label{fig:fig3}
\end{figure}\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (245).png} 
\caption{Learning rate alpha.}
\label{fig:fig3}
\end{figure}\\ \newpage
\section{Online Learning}
Online learning is a form of learning when the system has a continuous stream of training data. It implements the stochastic gradient descent forever using the input stream of data and discarding it once the parameter updates have been done using it.\\
It is observed that such an online learning setting is \textbf{capable of learning the changing trends} of data streams.\\
Typical domains where online learning can be successfully implemented include, search engines (predict click through rate i.e. CTR), recommendation websites etc.\\
Many of the listed problems can be modeled as a standard learning problem with fixed dataset, but often such data streams are available in such abundance that there is little utility of storing the data in place of implementing an online training system.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (246).png} 
\caption{Online Learning example.}
\label{fig:fig3}
\end{figure}\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (247).png} 
\caption{Other Online Learning example.}
\label{fig:fig3}
\end{figure}\\\newpage
\section{Map Reduce and Parallelism}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (249).png} 
\caption{Map reduce.}
\label{fig:fig3}
\end{figure}\\
Map-Reduce is a technique used in large scale learning when a single system is not enough to train the models required. Under this training paradigm, all the \textbf{summation operations are parallelized over a set of slave systems by spliting the training data} (batch or entire set) across the systems which compute on smaller datasets and feed the results to the \textbf{master system that aggregates the results} from all the slaves and combines them together. This parallelized implementation boosts the speed of algorithm.\\
If the network latencies are not high, then one can expect a boost in speed by upto n times by using a pool of n systems. So, in practice when the systems are on a network speed boost is slightly less than n times.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (250).png} 
\caption{Map reducing process.}
\label{fig:fig3}
\end{figure}\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (252).png} 
\caption{Map reduce summation over the training set.}
\label{fig:fig3}
\end{figure}\\
\notte{\textbf{Algorithms that can be expressed as a summation over the training sets can be parallelized using map-reduce.}}
Besides a pool of computers, parallelization also works on multi-core machines with the added benifit of near-zero network latencies and hence faster.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (251).png} 
\caption{Map reduce for Multi-core machines.}
\label{fig:fig3}
\end{figure}\\
\chapter{Application example: Photo OCR}
\section{Introduction}
Optical character recognition (OCR) is a method that helps machines recognize texts. Traditional OCR uses patterns and correlation to differentiate words from other elements. However, these techniques don’t tend to produce results with high accuracy for complex text or in-motion streams. For these tasks, you’re better served using deep learning models. In that spirit, in this article we’ll explore three deep learning models for OCR.\\
OCR refers to optical character recognition. The author here introduces the purpose of photo OCR technology for three main purposes:
\begin{enumerate}
    \item Demonstrate how a complex machine learning system is combined.
    \item Introduce the concepts of the machine learning pipeline (ML pipeline) and decide how to allocate resources next.
    \item Tell you about many interesting ideas and concepts of machine learning by introducing the opportunity of photo OCR. One of them is how to apply machine learning to computer vision problems, and the second is about the concept of artificial data synthesis.
    \item The main solution of photo OCR technology is to let the computer read out the text information of the photo.
\end{enumerate}
OCR, or optical character recognition, is one of the earliest addressed computer vision tasks, since in some aspects it does not require deep learning. Therefore there were different OCR implementations even before the deep learning boom in 2012, and some even dated back to 1914 (!).\\
This makes many people think the OCR challenge is “solved”, it is no longer challenging. Another belief which comes from similar sources is that OCR does not require deep learning, or in other words, using deep learning for OCR is an overkill.\\
Anyone who practices computer vision, or machine learning in general, knows that there is no such thing as a solved task, and this case is not different. On the contrary, OCR yields very-good results only on very specific use cases, but in general, it is still considered as challenging.\\
Additionally, it’s true there are good solutions for certain OCR tasks that do not require deep learning. However, to really step forward towards better, more general solutions, deep learning will be mandatory.
\section{What is OCR?}
OCR takes images that include textual elements and attempts to recognize that text. The output is a text string, and accuracy is measured as the degree of similarity between the recognized text and the text a human would be able to read from the image.\\
OCR is used to recognize printed text in paper documents, handwritten characters, and text elements in the physical environment, such as license plate numbers, street signs, and street numbers.\\
Traditional OCR algorithms are based on pattern matching, pattern recognition, or image correlation. These techniques, in a standard use case such as a document scanner, can recognize words and sentences with a very high level of accuracy.\\
However, in more challenging cases that include multiple font styles, camera motion blurring the image, or unclear human handwriting, legacy algorithms fall short and are gradually being replaced by deep learning methods.
\subsubsection{why do I write about OCR?}
Like many of my works/write-ups, this too started off as project for client. I was requested to solve a specific OCR task.\\
During and after working on this task, I’ve reached some conclusions and insights which are worth sharing. Additionally, after intensively working on a task, it is hard to stop and throw it away, so I keep my research going, and hoping to achieve an even better and more generalized solution.
\subsubsection{What you’ll find here}
In this post I will explore some of the strategies, methods and logic used to address different OCR tasks, and will share some useful approaches. In the last part, we will tackle a real world problem with code. This should not be considered as an exhaustive review (unfortunately) since the depth, history and breadth of approaches are too wide for this kind of a blog-post.\\
However, as always, I will not spare you from references to articles, data sets, repositories and other relevant blog-posts.
\subsection{Types of OCR}
As hinted before, there are more than one meaning for OCR. In its most general meaning, it refers to extracting text from every possible image, be it a standard printed page from a book, or a random image with graffiti in it (“in the wild”). In between, you may find many other tasks, such as reading license plates, no-robot captchas, street signs etc.\\
Although each of these options has its own difficulties, clearly “in the wild” task is the hardest.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{LeftPrinted text. Righttext in the wild.png}
\caption{Left: Printed text. Right: text in the wild.}
\label{fig:fig3}
\end{figure}\\
Form these examples we can draw out some attributes of the OCR tasks:
\begin{enumerate}
    \item \textbf{Text density:} on a printed/written page, text is dense. However, given an image of a street with a single street sign, text is sparse.
    \item \textbf{Structure of text:} text on a page is structured, mostly in strict rows, while text in the wild may be sprinkled everywhere, in different rotations.
    \item \textbf{Fonts:} printed fonts are easier, since they are more structured then the noisy hand-written characters.
    \item \textbf{Character type:} text may come in different language which may be very different from each other. Additionally, structure of text may be different from numbers, such as house numbers etc.
    \item \textbf{Artifacts:} clearly, outdoor pictures are much noisier than the comfortable scanner.
    \item \textbf{Location:} some tasks include cropped/centred text, while in others, text may be located in random locations in the image.
\end{enumerate}
\section{Photo OCR}
The specific steps of photo OCR are as follows:
\begin{enumerate}
    \item Photo OCR (Optical Character Recognition) Problem
    \begin{enumerate}
        \item Given picture, detect location of text in the picture.
        \item Read text at that location.
    \end{enumerate}
    \item Photo OCR Pipeline
    \begin{enumerate}
        \item Text detection.
        \item Character segmentation
        \begin{enumerate}
            \item Splitting “ADD” for example.
        \end{enumerate}
        \item Character classification
        \begin{enumerate}
            \item First character “A”, second “D”, and so on.
        \end{enumerate}
    \end{enumerate}
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Photo OCR pipeline.png}
\caption{Photo OCR pipeline.}
\label{fig:fig3}
\end{figure}
\begin{enumerate}
    \item Text recognition – Text detection uses text recognition technology to find text information in a given picture.
    \item Character segmentation – Character segmentation performs character segmentation on the rectangular outlines of these text areas.
    \item Character classification – After character is divided into independent characters, we can try to run a classifier, input these recognizable characters, and then try to recognize the above characters.There is actually the last step-spell correction. If your character segmentation and classification system tells you that the word it recognizes is "C1eaning", then many spelling correction systems will tell you that this may be It is the spelling of the word "Cleaning". Your character classification algorithm just recognized the letter "l" as the number "1", but in undergraduate, this step is not considered.
\end{enumerate}
\section{Photo OCR pipeline}
Like a system above, we call it a machine learning pipeline. In machine learning problems, it is very important to organize this pipeline. The photo OCR pipeline is given below.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Photo OCR pipeline 1.png}
\caption{Photo OCR pipeline.}
\label{fig:fig3}
\end{figure}\\
In this way, a system is divided into several modules, and different people can complete different modules. Work in parallel to improve efficiency.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Screenshot (253).png}
\caption{Text detection vs Pedestrian detection.}
\label{fig:fig3}
\end{figure}\\
\section{Sliding window}
The author first draws from the example of pedestrian detection:
\begin{enumerate}
    \item Specify the pedestrian ratio, such as 82:36.
    \item Collect samples, pedestrians and no pedestrians.
    \item rain the algorithm, input samples, classify, put  y=1 (Indicates a positive sample), there are pedestrians,  y=0 (Indicates a negative sample, no pedestrians.)
\end{enumerate}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{pede.png}
\caption{Supervised Learning for pedestrian detection.}
\label{fig:fig3}
\end{figure}\\
Specific to pedestrian detection, suppose we obtain a new test sample, we take a small block of lines from the upper left corner of the image, and stipulate each step size, gradually move long, for the results of each rectangular box, pass us Trained classifier to determine whether there are pedestrians.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{slide.png}
\caption{Sliding detection.}
\label{fig:fig3}
\end{figure}\\\newpage
Of course, you can adjust the size and step size of the rectangular frame. The final result is as follows:
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{slide 1.png}
\caption{Sliding window detection.}
\label{fig:fig3}
\end{figure}\\\newpage
Similarly, the same applies to text recognition.\\
\textbf{First, train the classifier.}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{classifier.png}
\caption{Text detection classifier.}
\label{fig:fig3}
\end{figure}\\
Secondly, slide the form on the target picture to identify the text area
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{text.png}
\caption{Text detection.}
\label{fig:fig3}
\end{figure}\\
The white area represents the area where the text was found, and the black area represents the area where the text was not found. Different gray levels represent the probability value of the output result given by the classifier, so for example, some gray shades indicate that the classifier seems to find text in this area, but it is not very sure; and the brighter areas indicate the classifier prediction The probability of having text in this area is relatively large.
\subsubsection{Substitute into the expansion operator (expansion operator)}
The role of the expander is to take this picture and expand each white dot into a white area.
Ignore those weird areas, because we know that the area with text should not be very high, but rather wide. So we ignore those tall and thin white pieces. Then for the remaining ones, draw a rectangular window with white blocks that look more like normal text areas in scale. See picture above.
\subsubsection{Character segmentation}
Similarly, we still use a supervised learning algorithm to train a classifier with positive samples and some negative samples of whether there is a segmentation area between characters.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{slide 1d.png}
\caption{1D Sliding window for character segmentation.}
\label{fig:fig3}
\end{figure}\\
Use the same form sliding method (only the classifier used is different), scan the text area image output by the text detection system, the classifier tells us y=1, at that time, it means that we need to draw a line in the middle to separate the two characters, otherwise skip. If normal, the classifier will tell us where to split the image into independent characters.
\section{Synthetic data}
The machine learning algorithm model we often expect is low-bias, because in this way, we can improve the efficiency of the algorithm by adding a large number of data sets. The fact is that our existing data does not meet the requirements. We need to synthesize the data manually. There are two ways to synthesize the data:
\begin{enumerate}
    \item Create new data.
    \item Expand an existing labeled small data set to obtain new data.
\end{enumerate}
\subsection{Create new data}
Still taking photo OCR as an example
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{artificial data.png}
\caption{Artificial data synthesis for photo OCR.}
\label{fig:fig3}
\end{figure}\\
We know that modern computers usually have a large font library, we can also download many free font styles, so if you want to get more training samples, one of them One way is that you can collect different fonts of the same character, and then add a random background to these characters to create a training sample. When generating simulated data, you need to consider blurring, deforming, and rotating the simulated samples, because the samples created in this way are more realistic. If you generate some samples rashly, then the final trained algorithm may not work very well.\\
After such operations, you can get such a training set after synthesis
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{artificial data 1.png}
\caption{Creating the synthetic data.}
\label{fig:fig3}
\end{figure}\\\newpage
\subsection{Create new samples from existing samples}
We select a real sample, and then expand your training set by adding additional data.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{artificial data 2.png}
\caption{Synthesizing data by using distortions.}
\label{fig:fig3}
\end{figure}\\
Compared to the letter A in the picture, it comes from a real image (not a composite image). For convenience of description, I added some gray grids to the image. There are actually no such grids. All you have to do is take out this image and perform artificial distortion or artificial deformation. In this way, 16 new samples can be generated from one image A.\\
As another example, new noise can also be added during the speech recognition process:
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{artificial data 3.png}
\caption{Synthesizing data by using distortions: speech recognition.}
\label{fig:fig3}
\end{figure}\\
So in this way, you can suddenly expand a small labeled training set to get more training samples.
\notte{\textbf{\textcolor{red}{In this method, it is reasonable to consider what factors are introduced for deformation. For example, if some random noise is added, it usually does not work. Some Gaussian noise is randomly introduced as follows, which has no effect.}}}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{artificial data 4.png}
\caption{Synthesizing data by introducing distortions.}
\label{fig:fig3}
\end{figure}\\\newpage
\subsubsection{To sum up:}
There are generally the following precautions when getting more training data
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1.png}
\caption{Discussion on getting more data.}
\label{fig:fig3}
\end{figure}\\
\begin{enumerate}
    \item Optimization algorithm, the model is best to have low deviation.
    \item Think about how much it takes to get 10 times the amount of data we have?
    \item Crowd sourcing method-used to manually mark samples.
\end{enumerate}
\section{Ceiling analysis: what part of the pipeline to work on next}
\subsection{Upper bound analysis}
As we said before, for machine learning problems, we generally organize a machine learning pipeline to work in parallel from each module of the pipeline. But we don't know which module is worth our time to improve system efficiency. The upper limit analysis function mentioned in this section is to tell you which part of the pipeline is most worth the time.\\
We still take the photo OCR as an example:
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{2.png}
\caption{Ceiling analysis.}
\label{fig:fig3}
\end{figure}\\
\subsection{The main idea:}
Assume that the estimated accuracy of the entire system is 72$\%$. Then assume that the accuracy of each module is 100$\%$and the overall accuracy of the system is obtained. If the accuracy of the text detection module is assumed to be 100$\%$, then the accuracy of the entire system is 8$\%$. Calculate the accuracy in other cases accordingly.\\
We analyze the gain between each module step by step. If we have a perfect text detection module, the performance of the entire system will increase from 72$\%$ to 89$\%$, so the effect gain is 17$\%$. This means that if you spend time and effort on the basis of the existing system to improve the effect of the text detection module, the performance of the system may increase by 17$\%$. Relatively speaking, if we get the perfect character segmentation module, then the final system performance is only improved by 1$\%$.
So through this analysis, it is clear which module we should invest more time in.\\
\newline Take another face recognition as an example, and give the pipeline
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{3.png}
\caption{Another Ceiling analysis example.}
\label{fig:fig3}
\end{figure}\\
\textbf{Step by step}
\begin{enumerate}
    \item Preprocessing, remove background.
    \item Face detection—eye segmentation—nose segmentation—mouth segmentation.
    \item Logistic regression (We can use any other machine learning algorithms too, not only logistic regression.)
\end{enumerate}
According to the pipeline, do an upper limit analysis as follows:
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{4.png}
\caption{Another Ceiling analysis example.}
\label{fig:fig3}
\end{figure}\\\newpage
Through the upper limit analysis, it is found that the preprocessing of the back is not so important, we should spend more on face detection.
\notte{\textbf{\textcolor{red}{The problem of machine learning can best be organized into a pipeline form, according to the upper limit analysis of each module of the pipeline, and then find the most time-consuming module.}}}

\chapter{Information beyond this course}
\section{Backpropagation Derivation}
\textbf{The post delves into the mathematics of how backpropagation is defined. It has its roots in partial derivatives and is easily understandable.}
\subsection{Derivative of Sigmoid}
The sigmoid function, represented by $\sigma$ is defined as,
\begin{align}
    \sigma(x)  = {1 \over 1 + e^{-x}}  \label{23}
\end{align}
So, the derivative of \ref{23}, denoted by $\sigma^{\prime}$ can be derived using the quotient rule of differentiation, i.e., if f and g are functions, then,
\begin{align}
    \left({f \over g} \right)^{\prime} = {f^{\prime}g - g^{\prime}f \over g^2}  \label{24}
\end{align}
Since f is a constant (i.e. 1) in this case, \ref{24} reduces to,
\begin{align}
    \left({1 \over g} \right)^{\prime} = {- g^{\prime} \over g^2}  \label{25}
\end{align}
Also, by the chain rule of differentiation, if h(x) = f(g(x)), then,
\begin{align}
    h^{\prime}(x) = f^{\prime}(g(x)) \cdot g^{\prime}(x)  \label{26}
\end{align}
Applying \ref{25} and \ref{26} to \ref{23}, $\sigma^{\prime}(x)$ is given by,
\begin{align}
\sigma^{\prime}(x) &= -\frac{e^{-x}} {(1+e^{-x})^2} \cdot -1 \\
    &= \frac{e^{-x}} {(1+e^{-x})^2} \\
    &= {1 \over 1 + e^{-x}} \cdot \frac{1 + e^{-x} - 1} {1+e^{-x}} \\
    &= \sigma(x) \cdot \left(\frac{1 + e^{-x}} {1+e^{-x}} - \frac{1} {1+e^{-x}} \right) \\
\sigma^{\prime}(x) &= \sigma(x) \cdot (1 - \sigma(x)) \label{27}
\end{align}
\subsection{Mathematics of Backpropagation}
(* all the derivations are based scalar calculus and not the matrix calculus for simplicity of calculations)\\
In most of the cases of algorithms like logistic regression, linear regression, there is no hidden layer, which basically zeroes down to the fact that there is no concept of error propagation in backward direction because there is a direct dependence of model cost function on the single layer of model parameters.\\
Backpropagation tries to do the similar exercise using the partial derivatives of model output with respect to the individual parameters. It so happens that there is a trend that can be observed when such derivatives are calculated and backpropagation tries to exploit the patterns and hence minimizes the overall computation by reusing the terms already calculated.\\
Consider a simple neural network with a single path (following the notation from $\href{https://machinelearningmedium.com/2017/10/03/neural-networks-cost-function-and-back-propagation/}{Neural\ Networks:\ Cost\ Function\ and\ Backpropagation}$) as shown below,
\begin{figure}[!htb]
\centering
\includegraphics[width = 1.0\textwidth]{single-path-neural-network.png}
\caption{Single-Path Neural Network.}
\label{fig:fig3}
\end{figure}\\
where,
\begin{align}
a^{(1)} &= x^{(i)} \\ 
z^{(2)} &= \theta^{(1)} a^{(1)} \\
a^{(2)} &= \sigma(z^{(2)}) \\ 
z^{(3)} &= \theta^{(2)} a^{(2)} \\
a^{(3)} &= \sigma(z^{(3)}) \\ 
z^{(4)} &= \theta^{(3)} a^{(3)} \\
a^{(4)} &= g(z^{(4)}) = h(x^{(i)}) = \hat{y}^{(i)}  \label{28}
\end{align}
where g is a linear function defined as g(x)=x, and hence $g^{\prime}(x)$=1. $\sigma$ represents the sigmoid function.\\
For the simplicity of derivation, let the cost function, J be defined as,
\begin{align}
    J = {1 \over 2} (\hat{y}^{(i)} - y^{(i)})^2 \label{29}
\end{align}
where,
\begin{align}
    \delta^{(4)} = \hat{y}^{(i)} - y^{(i)} \label{30}
\end{align}
Now, in order to find the changes that should be made in the parameters (i.e. weights), partial derivatives of the cost function is calculated w.r.t. individual $\theta^{\prime}$s,
\begin{align}
\frac {\partial J} {\partial \theta^{(3)}} &= \frac {\partial} {\partial \theta^{(3)}} {1 \over 2} (\hat{y}^{(i)} - y^{(i)})^2 \\
    &= (\hat{y}^{(i)} - y^{(i)}) \frac {\partial} {\partial \theta^{(3)}} (g(\theta^{(3)} a^{(3)}) - y^{(i)}) \\
    &= (\hat{y}^{(i)} - y^{(i)}) \cdot a^{(3)} \\
    &= (\hat{y}^{(i)} - y^{(i)}) \cdot a^{(3)} \label{31} \\ 
\frac {\partial J} {\partial \theta^{(2)}} &= \frac {\partial} {\partial \theta^{(2)}} {1 \over 2} (\hat{y}^{(i)} - y^{(i)})^2 \\
    &= (\hat{y}^{(i)} - y^{(i)}) \frac {\partial} {\partial \theta^{(2)}} (g(\theta^{(3)} \sigma(\theta^{(2)} a^{(2)})) - y^{(i)}) \\
    &= (\hat{y}^{(i)} - y^{(i)}) \cdot \theta^{(3)} \sigma'(z^{(3)}) \cdot a^{(2)}\label{32}\\ 
\frac {\partial J} {\partial \theta^{(1)}} &= \frac {\partial} {\partial \theta^{(1)}} {1 \over 2} (\hat{y}^{(i)} - y^{(i)})^2 \\
    &= (\hat{y}^{(i)} - y^{(i)}) \frac {\partial} {\partial \theta^{(1)}} (g(\theta^{(3)} \sigma(\theta^{(2)} \sigma(\theta^{(1)} a^{(1)}))) - y^{(i)}) \\
    &= (\hat{y}^{(i)} - y^{(i)}) \cdot \theta^{(3)} \sigma'(z^{(3)}) \cdot \theta^{(2)} \sigma'(z^{(2)}) \cdot a^{(1)} \\
    &= (\hat{y}^{(i)} - y^{(i)}) \cdot \theta^{(3)} \sigma'(z^{(3)}) \cdot \theta^{(2)} \sigma'(z^{(2)}) \cdot x^{(i)} \label{33}
\end{align}
One can see a pattern emerging among the partial derivatives of the cost function with respect to the individual parameters matrices. The expressions in \ref{31}, \ref{32} and \ref{33} show that each term consists of the derivative of the network error, the weighted derivative of the node output with respect to the node input leading upto that node.\\
So, for this network the updates for the matrices are given by,
\begin{align}
\Delta \theta^{(1)} &= - \eta [(\hat{y}^{(i)} - y^{(i)}) \cdot \theta^{(3)} \sigma'(z^{(3)}) \cdot \theta^{(2)} \sigma'(z^{(2)}) \cdot x^{(i)}] \\
\Delta \theta^{(2)} &= - \eta [(\hat{y}^{(i)} - y^{(i)}) \cdot \theta^{(3)} \sigma'(z^{(3)}) \cdot a^{(2)}] \\
\Delta \theta^{(3)} &= - \eta [(\hat{y}^{(i)} - y^{(i)}) \cdot a^{(3)}]  \label{34}
\end{align}
\notte{\textbf{Forward propagation is a recursive algorithm takes an input, weighs it along the edges and then applies the activation function in a node and repeats this process until the output node. Similarly, backpropagation is a recursive algorithm performing the inverse of the forward propagation, i.e. it takes the error signal from the output layer, weighs it along the edges and performs derivative of activation in an encountered node until it reaches the input. This brings in the concept of backward error propagation.}}
\section{Error Signal}
Following the concept of backward error propagation, error signal is defined as the accumulated error at each layer. The recursive error signal at a layer l is defined as,
\begin{align}
    \delta^{(l)} = \frac {\partial J} {\partial z^{(l)}}  \label{35}
\end{align}
Intuitively, it can be understood as the measure of how the network error changes with respect to the change in input to unit l. \\
So, $\delta^{(4)}$ in \ref{30}, can be derived using \ref{35},
\begin{align}
\delta^{(4)} &= \frac {\partial J} {\partial z^{(4)}} = \frac {\partial} {\partial z^{(4)}} {1 \over 2} (\hat{y}^{(i)} - y^{(i)})^2 \\
&= (\hat{y}^{(i)} - y^{(i)}) \frac {\partial} {\partial z^{(4)}} (\hat{y}^{(i)} - y^{(i)}) \\
    &= (\hat{y}^{(i)} - y^{(i)}) \frac {\partial} {\partial z^{(4)}} (g(z^{(4)}) - y^{(i)}) \\ 
    &= (\hat{y}^{(i)} - y^{(i)}) \label{36}
\end{align}
Similary the error signal at previous layers can be derived and it can be seen how the error signal of the forward layers get transmitted to the backward layers
\begin{align}
\delta^{(3)} &= \frac {\partial J} {\partial z^{(3)}} \\
    &= \frac {\partial} {\partial z^{(3)}} {1 \over 2} (\hat{y}^{(i)} - y^{(i)})^2 \\
    &= \delta^{(4)} \frac {\partial} {\partial z^{(3)}} (g(\theta^{(3)} \sigma(z^{(3)}))) \\
    &= \delta^{(4)} \theta^{(3)} \sigma'(z^{(3)})  \label{37} \\ 
\delta^{(2)} &= \frac {\partial J} {\partial z^{(2)}} \\
    &= \frac {\partial} {\partial z^{(2)}} {1 \over 2} (\hat{y}^{(i)} - y^{(i)})^2 \\
    &= \delta^{(4)} \frac {\partial} {\partial z^{(2)}} (g(\theta^{(3)} \sigma(\theta^{(2)} \sigma(z^{(2)})))) \\
    &= \delta^{(4)} \theta^{(3)} \sigma'(z^{(3)}) \cdot \theta^{(2)} \sigma'(z^{(2)}) \\ 
    &= \delta^{(3)} \theta^{(2)} \sigma'(z^{(2)})  \label{38} \\
\end{align}
Using \ref{36}, \ref{37} and \ref{38}, \ref{34} can be written as,
\begin{align}
\Delta \theta^{(1)} &= - \eta \delta^{(2)} a^{(1)} \\
\Delta \theta^{(2)} &= - \eta \delta^{(3)} a^{(2)} \\
\Delta \theta^{(3)} &= - \eta \delta^{(4)} a^{(3)} \label{39}
\end{align}
which is nothing but the updates to individual parameter matrices based on partial derivatives of cost w.r.t. individual matrices.
\section{Activation Function}
Generally, the choice of activation function at the output layer is dependent on the type of cost function. This is mainly to simplify the process of differentiation. For example, as shown in the example above, if the cost function is mean-squared error then choice of linear function as activation for the output layer often helps simplify calculations. Similarly, the cross-entropy loss works well with sigmoid or softmax activation functions. But this is not a hard and fast rule. One is free to use any activation function with any cost function, although the equations for partial derivatives might not look as nice.\\
Similarly, the choice of activation function in hidden layers are plenty. Although sigmoid functions are widely used, they suffer from vanishing gradient as the depth increases, hence other activations like ReLUs are recommended for deeper neural networks.
\section{Decision Tree}
Introduction Decision Trees are a type of Supervised Machine Learning (that is you explain what the input is and what the corresponding output is in the training data) where the data is continuously split according to a certain parameter. The tree can be explained by two entities, namely decision nodes and leaves.The leaves are the decisions or the final outcomes. And the decision nodes are where the data is split. An example of a decision tree can be explained using above binary tree. Let’s say you want to predict whether a person is fit given their information like age, eating habit, and physical activity, etc. The decision nodes here are questions like ‘What’s the age?’, ‘Does he exercise?’, ‘Does he eat a lot of pizzas’? And the leaves, which are outcomes like either ‘fit’, or ‘unfit’. In this case this was a binary classification problem (a yes no type problem). There are two main types of Decision Trees:
\begin{enumerate}
    \item \textbf{Classification trees (Yes/No types)}\\
     What we’ve seen above is an example of classification tree, where the outcome was a variable like ‘fit’ or ‘unfit’. Here the decision variable is Categorical.
     \item \textbf{Regression trees (Continuous data types)}\\
     Here the decision or the outcome variable is Continuous, e.g. a number like 123.
\end{enumerate}
\section{Recommender Systems}
\subsection{Content based methods}
Unlike collaborative methods that only rely on the user-item interactions, content based approaches use additional information about users and/or items. If we consider the example of a movies recommender system, this additional information can be, for example, the age, the sex, the job or any other personal information for users as well as the category, the main actors, the duration or other characteristics for the movies (items).\\
Then, the idea of content based methods is to try to build a model, based on the available “features”, that explain the observed user-item interactions. Still considering users and movies, we will try, for example, to model the fact that young women tend to rate better some movies, that young men tend to rate better some other movies and so on. If we manage to get such model, then, making new predictions for a user is pretty easy: we just need to look at the profile (age, sex, …) of this user and, based on this information, to determine relevant movies to suggest.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Overview of the content based methods paradigm..png} 
\caption{Overview of the content based methods paradigm.}
\label{fig:fig3}
\end{figure}\\ 
Content based methods suffer far less from the cold start problem than collaborative approaches: new users or items can be described by their characteristics (content) and so relevant suggestions can be done for these new entities. Only new users or items with previously unseen features will logically suffer from this drawback, but once the system old enough, this has few to no chance to happen.\\
Later in this post, we will further discuss content based approaches and see that, depending on our problem, various classification or regression models can be used, ranging from very simple to much more complex models.
\subsection{Models, bias and variance}
Let’s focus a bit more on the main differences between the previously mentioned methods. More especially let’s see the implication that the modelling level has on the bias and the variance.\\
In memory based collaborative methods, no latent model is assumed. The algorithms directly works with the user-item interactions: for example, users are represented by their interactions with items and a nearest neighbours search on these representations is used to produce suggestions. As no latent model is assumed, these methods have theoretically a low bias but a high variance.\\
In model based collaborative methods, some latent interaction model is assumed. The model is trained to reconstruct user-item interactions values from its own representation of users and items. New suggestions can then be done based on this model. The users and items latent representations extracted by the model have a mathematical meaning that can be hard to interpret for a human being. As a (pretty free) model for user-item interactions is assumed, this methods has theoretically a higher bias but a lower variance than methods assuming no latent model.\\
Finally, in content based methods some latent interaction model is also assumed. However, here, the model is provided with content that define the representation of users and/or items: for example, users are represented by given features and we try to model for each item the kind of user profile that likes or not this item. Here, as for model based collaborative methods, a user-item interactions model is assumed. However, this model is more constrained (because representation of users and/or items are given) and, so, the method tends to have the highest bias but the lowest variance.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Summary of the different types of recommender systems algorithms..png} 
\caption{Summary of the different types of recommender systems algorithms.}
\label{fig:fig3}
\end{figure}\\ 
\section{Memory based collaborative approaches}
The main characteristics of user-user and item-item approaches it that they use only information from the user-item interaction matrix and they assume no model to produce new recommendations.
\subsection{User-user}
In order to make a new recommendation to a user, user-user method roughly tries to identify users with the most similar “interactions profile” (nearest neighbours) in order to suggest items that are the most popular among these neighbours (and that are “new” to our user). This method is said to be “user-centred” as it represent users based on their interactions with items and evaluate distances between users.\\
Assume that we want to make a recommendation for a given user. First, every user can be represented by its vector of interactions with the different items (“its line” in the interaction matrix). Then, we can compute some kind of “similarity” between our user of interest and every other users. That similarity measure is such that two users with similar interactions on the same items should be considered as being close. Once similarities to every users have been computed, we can keep the k-nearest-neighbours to our user and then suggest the most popular items among them (only looking at the items that our reference user has not interacted with yet).\\
Notice that, when computing similarity between users, the number of “common interactions” (how much items have already been considered by both users?) should be considered carefully! Indeed, most of the time, we want to avoid that someone that only have one interaction in common with our reference user could have a 100$\%$ match and be considered as being “closer” than someone having 100 common interactions and agreeing “only” on 98$\%$ of them. So, we consider that two users are similar if they have interacted with a lot of common items in the same way (similar rating, similar time hovering…).
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Illustration of the user-user method. The same colour code will be used in the remaining of the post..png} 
\caption{Illustration of the user-user method. The same colour code will be used in the remaining of the post.}
\label{fig:fig3}
\end{figure}\\ 
\subsection{Item-item}
To make a new recommendation to a user, the idea of item-item method is to find items similar to the ones the user already “positively” interacted with. Two items are considered to be similar if most of the users that have interacted with both of them did it in a similar way. This method is said to be “item-centred” as it represent items based on interactions users had with them and evaluate distances between those items.\\
Assume that we want to make a recommendation for a given user. First, we consider the item this user liked the most and represent it (as all the other items) by its vector of interaction with every users (“its column” in the interaction matrix). Then, we can compute similarities between the “best item” and all the other items. Once the similarities have been computed, we can then keep the k-nearest-neighbours to the selected “best item” that are new to our user of interest and recommend these items.\\
Notice that in order to get more relevant recommendations, we can do this job for more than only the user’s favourite item and consider the n preferred items instead. In this case, we can recommend items that are close to several of these preferred items.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Illustration of the item-item method..png} 
\caption{Illustration of the item-item method.}
\label{fig:fig3}
\end{figure}\\ 
\subsection{Comparing user-user and item-item}
The user-user method is based on the search of similar users in terms of interactions with items. As, in general, every user have only interacted with a few items, it makes the method pretty sensitive to any recorded interactions (high variance). On the other hand, as the final recommendation is only based on interactions recorded for users similar to our user of interest, we obtain more personalized results (low bias).\\
Conversely, the item-item method is based on the search of similar items in terms of user-item interactions. As, in general, a lot of users have interacted with an item, the neighbourhood search is far less sensitive to single interactions (lower variance). As a counterpart, interactions coming from every kind of users (even users very different from our reference user) are then considered in the recommendation, making the method less personalised (more biased). Thus, this approach is less personalized than the user-user approach but more robust.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Illustration of the difference between item-item and user-user methods..png} 
\caption{Illustration of the difference between item-item and user-user methods.}
\label{fig:fig3}
\end{figure}\\ 
\subsection{Complexity and side effect}
One of the biggest flaw of memory based collaborative filtering is that they do not scale easily: generating a new recommendation can be extremely time consuming for big systems. Indeed, for systems with millions of users and millions of items, the nearest neighbours search step can become intractable if not carefully designed (KNN algorithm has a complexity of O(ndk) with n the number of users, d the number of items and k the number of considered neighbours). In order to make computations more tractable for huge systems, we can both take advantage of the sparsity of the interaction matrix when designing our algorithm or use approximate nearest neighbours methods (ANN).\\
In most of the recommendation algorithms, it is necessary to be extremely careful to avoid a “rich-get-richer” effect for popular items and to avoid getting users stuck into what could be called an “information confinement area”. In other words, we do not want that our system tend to recommend more and more only popular items as well as we do not want that our users only receive recommendations for items extremely close to the one they already liked with no chance to get to know new items they might like too (as these items are not “close enough” to be suggested). If, as we mentioned, these problems can arise in most of the recommendation algorithms, it is especially true for memory based collaborative ones. Indeed, with the lack of model “to regularise”, this kind of phenomenon can be accentuated and observed more frequently.
\section{Model based collaborative approaches}
Model based collaborative approaches only rely on user-item interactions information and assume a latent model supposed to explain these interactions. For example, matrix factorisation algorithms consists in decomposing the huge and sparse user-item interaction matrix into a product of two smaller and dense matrices: a user-factor matrix (containing users representations) that multiplies a factor-item matrix (containing items representations).
\subsection{Matrix factorisation}
The main assumption behind matrix factorisation is that there exists a pretty low dimensional latent space of features in which we can represent both users and items and such that the interaction between a user and an item can be obtained by computing the dot product of corresponding dense vectors in that space.\\
For example, consider that we have a user-movie rating matrix. In order to model the interactions between users and movies, we can assume that:
\begin{enumerate}
    \item There exists some features describing (and telling apart) pretty well movies.
    \item These features can also be used to describe user preferences (high values for features the user likes, low values otherwise)
\end{enumerate}
However we don’t want to give explicitly these features to our model (as it could be done for content based approaches that we will describe later). Instead, we prefer to let the system discover these useful features by itself and make its own representations of both users and items. As they are learned and not given, extracted features taken individually have a mathematical meaning but no intuitive interpretation (and, so, are difficult, if not impossible, to understand as human). However, it is not unusual to ends up having structures emerging from that type of algorithm being extremely close to intuitive decomposition that human could think about. Indeed, the consequence of such factorisation is that close users in terms of preferences as well as close items in terms of characteristics ends up having close representations in the latent space.
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Illustration of the matrix factorization method..png} 
\caption{Illustration of the matrix factorization method.}
\label{fig:fig3}
\end{figure}\\ 
\subsection{Mathematics of matrix factorisation}
In this subsection, we will give a simple mathematical overview of matrix factorization. More especially, we describe a classical iterative approach based on gradient descent that makes possible to obtain factorisations for very large matrices without loading all the data at the same time in computer’s memory.\\
Let’s consider an interaction matrix M ($n\times m$) of ratings where only some items have been rated by each user (most of the interactions are set to None to express the lack of rating). We want to factorise that matrix such that
\begin{align*}
    M \approx X.Y^{\top}
\end{align*}
where X is the “user matrix” ($n\times l$) whose rows represent the n users and where Y is the “item matrix” ($m\times l$) whose rows represent the m items:
\begin{align*}
    user_i \equiv X_i
\end{align*}
\end{document}